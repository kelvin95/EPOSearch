Script started.
loading dataset mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.3377677 1.5641606], train_acc=[0.5543 0.4623]
2/100: train_loss=[1.0697734 1.320792 ], train_acc=[0.64335 0.54645]
4/100: train_loss=[0.8147271 0.9798833], train_acc=[0.735  0.6725]
6/100: train_loss=[0.70600337 0.86472875], train_acc=[0.7711 0.7077]
8/100: train_loss=[0.6755749 0.7576864], train_acc=[0.7795  0.75225]
10/100: train_loss=[0.5817202 0.7267621], train_acc=[0.8132  0.75055]
12/100: train_loss=[0.55988723 0.68776923], train_acc=[0.8201 0.7687]
14/100: train_loss=[0.527494  0.6353819], train_acc=[0.82885 0.78775]
16/100: train_loss=[0.54003227 0.612456  ], train_acc=[0.82285 0.79555]
18/100: train_loss=[0.5068503 0.5873713], train_acc=[0.8339 0.8026]
20/100: train_loss=[0.48829192 0.5653663 ], train_acc=[0.84375 0.81155]
22/100: train_loss=[0.45784035 0.53862834], train_acc=[0.84945 0.81825]
24/100: train_loss=[0.44827625 0.5330813 ], train_acc=[0.85405 0.82165]
26/100: train_loss=[0.4299915 0.5221746], train_acc=[0.86025 0.82515]
28/100: train_loss=[0.4173467 0.5524171], train_acc=[0.8642 0.816 ]
30/100: train_loss=[0.4169345 0.5156202], train_acc=[0.8649 0.8264]
32/100: train_loss=[0.4018819 0.5095525], train_acc=[0.86805 0.8339 ]
34/100: train_loss=[0.39768037 0.48368025], train_acc=[0.87125 0.8343 ]
36/100: train_loss=[0.39891765 0.46875864], train_acc=[0.86995 0.8427 ]
38/100: train_loss=[0.39219448 0.48539305], train_acc=[0.8705 0.8429]
40/100: train_loss=[0.3947443  0.48158884], train_acc=[0.8698 0.8373]
42/100: train_loss=[0.43107197 0.50205284], train_acc=[0.8576  0.83535]
44/100: train_loss=[0.37848765 0.44572687], train_acc=[0.8749 0.8538]
46/100: train_loss=[0.37396762 0.46452   ], train_acc=[0.87705 0.8466 ]
48/100: train_loss=[0.3636282  0.42428368], train_acc=[0.8786 0.8597]
50/100: train_loss=[0.35637936 0.4283008 ], train_acc=[0.88125 0.85825]
52/100: train_loss=[0.37165356 0.42688662], train_acc=[0.8766  0.85695]
54/100: train_loss=[0.3603458  0.44514573], train_acc=[0.87925 0.8519 ]
56/100: train_loss=[0.35435727 0.4106232 ], train_acc=[0.8834 0.8633]
58/100: train_loss=[0.35472903 0.41724178], train_acc=[0.88185 0.86095]
60/100: train_loss=[0.34486288 0.42534667], train_acc=[0.8845  0.85795]
62/100: train_loss=[0.33064404 0.42233506], train_acc=[0.8899 0.8604]
64/100: train_loss=[0.33039454 0.39256814], train_acc=[0.8894 0.8709]
66/100: train_loss=[0.33082777 0.40377116], train_acc=[0.8906  0.86675]
68/100: train_loss=[0.33784342 0.40204364], train_acc=[0.8881  0.86485]
70/100: train_loss=[0.3207708  0.38919494], train_acc=[0.89325 0.87215]
72/100: train_loss=[0.32869834 0.38968906], train_acc=[0.89195 0.8721 ]
74/100: train_loss=[0.31983918 0.38450152], train_acc=[0.89325 0.87155]
76/100: train_loss=[0.34785566 0.3980574 ], train_acc=[0.8875 0.8671]
78/100: train_loss=[0.31910434 0.3852013 ], train_acc=[0.895   0.87335]
80/100: train_loss=[0.33943182 0.39769986], train_acc=[0.8872  0.87145]
82/100: train_loss=[0.32759064 0.37795293], train_acc=[0.8904  0.87345]
84/100: train_loss=[0.31227547 0.38920742], train_acc=[0.89795 0.8707 ]
86/100: train_loss=[0.3107727  0.37444204], train_acc=[0.898  0.8763]
88/100: train_loss=[0.3205826  0.38249144], train_acc=[0.8955 0.8748]
90/100: train_loss=[0.30966005 0.37514353], train_acc=[0.89795 0.87425]
92/100: train_loss=[0.31034583 0.37697834], train_acc=[0.89895 0.87595]
94/100: train_loss=[0.32498088 0.37837484], train_acc=[0.89455 0.87335]
96/100: train_loss=[0.29908234 0.36634722], train_acc=[0.9014 0.8793]
98/100: train_loss=[0.29930508 0.37511533], train_acc=[0.9013  0.87565]
100/100: train_loss=[0.3012039 0.3679021], train_acc=[0.9005  0.87645]
**** Time taken for mnist_0 = 4140.223893404007
loading dataset mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.2544895 1.4893142], train_acc=[0.5734  0.49905]
2/100: train_loss=[0.9523538 1.1315401], train_acc=[0.68455 0.62185]
4/100: train_loss=[0.73673683 0.8576611 ], train_acc=[0.7593 0.7106]
6/100: train_loss=[0.65068835 0.7375674 ], train_acc=[0.7903  0.74885]
8/100: train_loss=[0.5828632  0.66920847], train_acc=[0.8109  0.77535]
10/100: train_loss=[0.5512659 0.6273213], train_acc=[0.82005 0.78475]
12/100: train_loss=[0.5240681 0.6325301], train_acc=[0.8282  0.78175]
14/100: train_loss=[0.48595062 0.54304606], train_acc=[0.84125 0.81645]
16/100: train_loss=[0.45617986 0.52877724], train_acc=[0.85205 0.8242 ]
18/100: train_loss=[0.45606652 0.5270703 ], train_acc=[0.84855 0.82285]
20/100: train_loss=[0.44904733 0.49739072], train_acc=[0.85305 0.83315]
22/100: train_loss=[0.40488917 0.5053551 ], train_acc=[0.86525 0.82965]
24/100: train_loss=[0.3893382  0.48695993], train_acc=[0.87035 0.8355 ]
26/100: train_loss=[0.38929537 0.45112485], train_acc=[0.8717 0.8478]
28/100: train_loss=[0.41342145 0.45619223], train_acc=[0.8619 0.8484]
30/100: train_loss=[0.3776071 0.4352166], train_acc=[0.87275 0.85155]
32/100: train_loss=[0.3789345  0.43992537], train_acc=[0.8754 0.8522]
34/100: train_loss=[0.37472612 0.43099287], train_acc=[0.8754 0.8554]
36/100: train_loss=[0.350865   0.40585887], train_acc=[0.88305 0.8635 ]
38/100: train_loss=[0.3459254  0.42423007], train_acc=[0.8847 0.8589]
40/100: train_loss=[0.36462834 0.4062889 ], train_acc=[0.8792 0.8625]
42/100: train_loss=[0.35234052 0.40789497], train_acc=[0.8843  0.86235]
44/100: train_loss=[0.33983418 0.40338096], train_acc=[0.8873 0.8663]
46/100: train_loss=[0.32308042 0.39935252], train_acc=[0.895   0.86435]
48/100: train_loss=[0.33416182 0.40818137], train_acc=[0.88965 0.8628 ]
50/100: train_loss=[0.32610515 0.37915465], train_acc=[0.8934  0.87115]
52/100: train_loss=[0.31815323 0.37469062], train_acc=[0.89505 0.87305]
54/100: train_loss=[0.32657012 0.38461512], train_acc=[0.89255 0.87195]
56/100: train_loss=[0.31541264 0.36119083], train_acc=[0.8965  0.87835]
58/100: train_loss=[0.32433566 0.3708384 ], train_acc=[0.8942 0.8761]
60/100: train_loss=[0.30689698 0.38745213], train_acc=[0.89915 0.8717 ]
62/100: train_loss=[0.33397195 0.37856025], train_acc=[0.89075 0.8725 ]
64/100: train_loss=[0.3251374 0.3738692], train_acc=[0.8928 0.8754]
66/100: train_loss=[0.36296648 0.38832223], train_acc=[0.88225 0.86975]
68/100: train_loss=[0.3159219  0.35435498], train_acc=[0.8958  0.88125]
70/100: train_loss=[0.33941072 0.3646087 ], train_acc=[0.88795 0.8775 ]
72/100: train_loss=[0.3304405  0.35187823], train_acc=[0.8923 0.8806]
74/100: train_loss=[0.3148183  0.35909048], train_acc=[0.89715 0.8789 ]
76/100: train_loss=[0.33837244 0.40524423], train_acc=[0.8898  0.86425]
78/100: train_loss=[0.2910597  0.34580922], train_acc=[0.9037 0.8858]
80/100: train_loss=[0.31204098 0.3570031 ], train_acc=[0.89695 0.88165]
82/100: train_loss=[0.29891825 0.34676048], train_acc=[0.90265 0.88275]
84/100: train_loss=[0.29752302 0.3984476 ], train_acc=[0.9025 0.8671]
86/100: train_loss=[0.2961188  0.33994406], train_acc=[0.9019 0.8863]
88/100: train_loss=[0.28950104 0.33545706], train_acc=[0.90445 0.8871 ]
90/100: train_loss=[0.29788956 0.3494397 ], train_acc=[0.90235 0.8837 ]
92/100: train_loss=[0.2857084  0.33223042], train_acc=[0.9065 0.8881]
94/100: train_loss=[0.32200238 0.3652837 ], train_acc=[0.89405 0.87715]
96/100: train_loss=[0.2950627  0.33497438], train_acc=[0.9027  0.88835]
98/100: train_loss=[0.28629157 0.3401142 ], train_acc=[0.9065 0.8866]
100/100: train_loss=[0.2871352  0.33032927], train_acc=[0.90645 0.88935]
**** Time taken for mnist_1 = 4213.859894037247
loading dataset mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[0.92510766 1.1493466 ], train_acc=[0.6933  0.60995]
2/100: train_loss=[0.74075717 0.8846804 ], train_acc=[0.75345 0.6986 ]
4/100: train_loss=[0.60246724 0.6959808 ], train_acc=[0.80105 0.7657 ]
6/100: train_loss=[0.51986253 0.65070343], train_acc=[0.8286 0.7831]
8/100: train_loss=[0.48629066 0.5763923 ], train_acc=[0.8401 0.8073]
10/100: train_loss=[0.44552776 0.5392343 ], train_acc=[0.8517  0.82075]
12/100: train_loss=[0.43819985 0.5333496 ], train_acc=[0.8557 0.8224]
14/100: train_loss=[0.3969549  0.48835418], train_acc=[0.8696 0.8377]
16/100: train_loss=[0.38481405 0.48699963], train_acc=[0.87475 0.8369 ]
18/100: train_loss=[0.38801295 0.48016068], train_acc=[0.87295 0.84265]
20/100: train_loss=[0.360143   0.46384507], train_acc=[0.8821 0.8475]
22/100: train_loss=[0.35193148 0.43790418], train_acc=[0.8853 0.8553]
24/100: train_loss=[0.34700075 0.44054207], train_acc=[0.88775 0.85545]
26/100: train_loss=[0.33664298 0.42430967], train_acc=[0.8899 0.8597]
28/100: train_loss=[0.34916502 0.42975086], train_acc=[0.88505 0.85805]
30/100: train_loss=[0.3197924 0.4093756], train_acc=[0.89635 0.86545]
32/100: train_loss=[0.32998097 0.4258109 ], train_acc=[0.8928 0.8573]
34/100: train_loss=[0.31867453 0.4093005 ], train_acc=[0.8964 0.8653]
36/100: train_loss=[0.3098494  0.38641384], train_acc=[0.899  0.8738]
38/100: train_loss=[0.3561721  0.38815984], train_acc=[0.8841  0.87325]
40/100: train_loss=[0.30950215 0.40490684], train_acc=[0.90045 0.86585]
42/100: train_loss=[0.30759552 0.39205462], train_acc=[0.89965 0.87005]
44/100: train_loss=[0.30317253 0.3995932 ], train_acc=[0.901   0.87035]
46/100: train_loss=[0.29284614 0.37480304], train_acc=[0.9056  0.87645]
48/100: train_loss=[0.30137432 0.397887  ], train_acc=[0.9039  0.86795]
50/100: train_loss=[0.32367972 0.40839994], train_acc=[0.89555 0.866  ]
52/100: train_loss=[0.2978658 0.3926427], train_acc=[0.90415 0.87175]
54/100: train_loss=[0.29345652 0.35879025], train_acc=[0.90465 0.88125]
56/100: train_loss=[0.3074119  0.37044913], train_acc=[0.903  0.8786]
58/100: train_loss=[0.3072731 0.3756667], train_acc=[0.90195 0.87655]
60/100: train_loss=[0.29429248 0.36182582], train_acc=[0.90355 0.8814 ]
62/100: train_loss=[0.3033655  0.36844596], train_acc=[0.90035 0.8792 ]
64/100: train_loss=[0.27993506 0.35634726], train_acc=[0.9085 0.8842]
66/100: train_loss=[0.28482163 0.3665368 ], train_acc=[0.90645 0.8787 ]
68/100: train_loss=[0.29487792 0.3757984 ], train_acc=[0.90445 0.87685]
70/100: train_loss=[0.28835422 0.3649088 ], train_acc=[0.90855 0.8796 ]
72/100: train_loss=[0.27935162 0.35825557], train_acc=[0.9102 0.8834]
74/100: train_loss=[0.27588317 0.36410263], train_acc=[0.91055 0.8836 ]
76/100: train_loss=[0.28584632 0.3514801 ], train_acc=[0.9075 0.8854]
78/100: train_loss=[0.27356467 0.34148088], train_acc=[0.90995 0.8894 ]
80/100: train_loss=[0.29569584 0.35162637], train_acc=[0.90485 0.88515]
82/100: train_loss=[0.27478537 0.34200138], train_acc=[0.9098  0.88815]
84/100: train_loss=[0.2868836  0.34228092], train_acc=[0.9083  0.88875]
86/100: train_loss=[0.2752988  0.34655526], train_acc=[0.9102  0.88605]
88/100: train_loss=[0.2702818  0.35649246], train_acc=[0.91355 0.8834 ]
90/100: train_loss=[0.28626776 0.3760386 ], train_acc=[0.90895 0.87925]
92/100: train_loss=[0.26865906 0.3378268 ], train_acc=[0.91295 0.8897 ]
94/100: train_loss=[0.30973902 0.36541787], train_acc=[0.90095 0.88135]
96/100: train_loss=[0.26862317 0.34622005], train_acc=[0.9134 0.8876]
98/100: train_loss=[0.31244177 0.40177286], train_acc=[0.8989 0.8668]
100/100: train_loss=[0.27948567 0.3373575 ], train_acc=[0.9097  0.88805]
**** Time taken for mnist_2 = 4218.4366245269775
loading dataset mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.6577191 1.9241298], train_acc=[0.4281  0.32695]
2/100: train_loss=[1.1032462 1.4596585], train_acc=[0.6285 0.4892]
4/100: train_loss=[0.76099074 1.0499985 ], train_acc=[0.7482  0.64605]
6/100: train_loss=[0.67432475 0.9038399 ], train_acc=[0.78105 0.6938 ]
8/100: train_loss=[0.6243442  0.77019423], train_acc=[0.79325 0.7396 ]
10/100: train_loss=[0.5577252 0.7100108], train_acc=[0.81765 0.76645]
12/100: train_loss=[0.5601549 0.655787 ], train_acc=[0.8158 0.7801]
14/100: train_loss=[0.51589715 0.6210613 ], train_acc=[0.83155 0.7913 ]
16/100: train_loss=[0.48354253 0.5915009 ], train_acc=[0.83995 0.80265]
18/100: train_loss=[0.47254282 0.5724766 ], train_acc=[0.8435  0.80965]
20/100: train_loss=[0.45969835 0.55141693], train_acc=[0.8482 0.815 ]
22/100: train_loss=[0.44908997 0.5862113 ], train_acc=[0.85425 0.8013 ]
24/100: train_loss=[0.48943552 0.570155  ], train_acc=[0.84105 0.80885]
26/100: train_loss=[0.42341006 0.50624824], train_acc=[0.8623 0.8302]
28/100: train_loss=[0.41523725 0.50184596], train_acc=[0.86465 0.83075]
30/100: train_loss=[0.43702507 0.48728243], train_acc=[0.85625 0.83765]
32/100: train_loss=[0.4116807  0.49149212], train_acc=[0.8639  0.83435]
34/100: train_loss=[0.39440712 0.47822118], train_acc=[0.8716 0.8436]
36/100: train_loss=[0.38231745 0.45023602], train_acc=[0.87415 0.85085]
38/100: train_loss=[0.38257614 0.4464667 ], train_acc=[0.8748  0.85075]
40/100: train_loss=[0.3731068  0.44135758], train_acc=[0.8784 0.8514]
42/100: train_loss=[0.36459246 0.48378754], train_acc=[0.88035 0.8379 ]
44/100: train_loss=[0.37153435 0.43088415], train_acc=[0.8798  0.85865]
46/100: train_loss=[0.36868498 0.44574666], train_acc=[0.8801  0.85115]
48/100: train_loss=[0.35632205 0.44568864], train_acc=[0.88325 0.8533 ]
50/100: train_loss=[0.3481351  0.42769426], train_acc=[0.8862 0.8584]
52/100: train_loss=[0.34246266 0.4334452 ], train_acc=[0.88945 0.8595 ]
54/100: train_loss=[0.3449259  0.40664726], train_acc=[0.88875 0.86475]
56/100: train_loss=[0.35126036 0.39976382], train_acc=[0.8857  0.86575]
58/100: train_loss=[0.3318939 0.428    ], train_acc=[0.89215 0.85885]
60/100: train_loss=[0.3275117 0.393982 ], train_acc=[0.89355 0.8673 ]
62/100: train_loss=[0.32990173 0.41358083], train_acc=[0.8935  0.86465]
64/100: train_loss=[0.33488932 0.38349056], train_acc=[0.8916 0.873 ]
66/100: train_loss=[0.31766865 0.38396367], train_acc=[0.89785 0.87325]
68/100: train_loss=[0.31602132 0.38022318], train_acc=[0.89925 0.87345]
70/100: train_loss=[0.31288996 0.37718585], train_acc=[0.8997 0.874 ]
72/100: train_loss=[0.3248304  0.39217657], train_acc=[0.8941 0.869 ]
74/100: train_loss=[0.32041517 0.37419748], train_acc=[0.8957 0.8762]
76/100: train_loss=[0.30987892 0.38287944], train_acc=[0.89755 0.87435]
78/100: train_loss=[0.3120412  0.40703425], train_acc=[0.8989 0.8645]
80/100: train_loss=[0.3085414  0.36865667], train_acc=[0.9013 0.878 ]
82/100: train_loss=[0.3090567  0.37097907], train_acc=[0.9012  0.87725]
84/100: train_loss=[0.29895753 0.358668  ], train_acc=[0.90385 0.88025]
86/100: train_loss=[0.29897523 0.36482733], train_acc=[0.9045 0.8784]
88/100: train_loss=[0.30622235 0.375302  ], train_acc=[0.902  0.8774]
90/100: train_loss=[0.2996318  0.36085343], train_acc=[0.9042  0.87905]
92/100: train_loss=[0.29677108 0.3688196 ], train_acc=[0.90335 0.8792 ]
94/100: train_loss=[0.29526156 0.3597227 ], train_acc=[0.90435 0.88075]
96/100: train_loss=[0.2957608  0.37001655], train_acc=[0.9035 0.8776]
98/100: train_loss=[0.29531536 0.3858124 ], train_acc=[0.9048  0.87285]
100/100: train_loss=[0.30504492 0.35766372], train_acc=[0.89985 0.88005]
**** Time taken for mnist_3 = 4319.5294234752655
loading dataset mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.3262929 1.5267206], train_acc=[0.5562 0.4745]
2/100: train_loss=[1.036126  1.2863225], train_acc=[0.65015 0.56565]
4/100: train_loss=[0.7803855 0.911338 ], train_acc=[0.74065 0.692  ]
6/100: train_loss=[0.6579035 0.8204076], train_acc=[0.7825  0.72025]
8/100: train_loss=[0.6460275  0.77456486], train_acc=[0.785  0.7422]
10/100: train_loss=[0.55796427 0.66757786], train_acc=[0.8173 0.7774]
12/100: train_loss=[0.5589706  0.66226614], train_acc=[0.8153 0.782 ]
14/100: train_loss=[0.5198775 0.5862988], train_acc=[0.82915 0.80625]
16/100: train_loss=[0.49204838 0.564195  ], train_acc=[0.8369 0.81  ]
18/100: train_loss=[0.48166275 0.5502278 ], train_acc=[0.83875 0.81855]
20/100: train_loss=[0.4756513  0.54633546], train_acc=[0.8429  0.81775]
22/100: train_loss=[0.45024893 0.528289  ], train_acc=[0.85245 0.82645]
24/100: train_loss=[0.44799733 0.50092465], train_acc=[0.85315 0.8323 ]
26/100: train_loss=[0.42914867 0.49331918], train_acc=[0.8589  0.83385]
28/100: train_loss=[0.429615 0.496174], train_acc=[0.8596  0.83485]
30/100: train_loss=[0.40536076 0.4687017 ], train_acc=[0.86695 0.8432 ]
32/100: train_loss=[0.40647513 0.4930641 ], train_acc=[0.8657  0.83675]
34/100: train_loss=[0.38767654 0.45544192], train_acc=[0.8723  0.84625]
36/100: train_loss=[0.37943017 0.44255564], train_acc=[0.87585 0.85225]
38/100: train_loss=[0.38269028 0.47118774], train_acc=[0.8739 0.8429]
40/100: train_loss=[0.37343508 0.45543507], train_acc=[0.87615 0.84745]
42/100: train_loss=[0.3904589  0.42951775], train_acc=[0.87295 0.8568 ]
44/100: train_loss=[0.36480036 0.42326277], train_acc=[0.88055 0.8593 ]
46/100: train_loss=[0.36893114 0.4257198 ], train_acc=[0.87955 0.85815]
48/100: train_loss=[0.3564731  0.42513546], train_acc=[0.8848  0.85815]
50/100: train_loss=[0.352355  0.4070962], train_acc=[0.8866  0.86475]
52/100: train_loss=[0.35746515 0.4152975 ], train_acc=[0.8843 0.8608]
54/100: train_loss=[0.34967867 0.41636935], train_acc=[0.88485 0.8602 ]
56/100: train_loss=[0.34034535 0.40821216], train_acc=[0.88975 0.86435]
58/100: train_loss=[0.36106142 0.41579056], train_acc=[0.88275 0.8607 ]
60/100: train_loss=[0.35499376 0.42244726], train_acc=[0.8834 0.86  ]
62/100: train_loss=[0.32984734 0.40460584], train_acc=[0.892   0.86615]
64/100: train_loss=[0.33260843 0.3881319 ], train_acc=[0.8915 0.8703]
66/100: train_loss=[0.32919535 0.39414534], train_acc=[0.89355 0.8676 ]
68/100: train_loss=[0.37095207 0.4386015 ], train_acc=[0.88265 0.85355]
70/100: train_loss=[0.33832142 0.3919504 ], train_acc=[0.8908 0.8702]
72/100: train_loss=[0.384486   0.47382748], train_acc=[0.8753 0.8418]
74/100: train_loss=[0.3465291  0.43364334], train_acc=[0.888  0.8539]
76/100: train_loss=[0.33981118 0.3875137 ], train_acc=[0.8897 0.8705]
78/100: train_loss=[0.3113923  0.37481302], train_acc=[0.8979 0.8738]
80/100: train_loss=[0.33082223 0.41548863], train_acc=[0.89085 0.86125]
82/100: train_loss=[0.3153199  0.37336168], train_acc=[0.89625 0.8744 ]
84/100: train_loss=[0.33757743 0.37891883], train_acc=[0.8911  0.87075]
86/100: train_loss=[0.31360656 0.37300405], train_acc=[0.8973 0.8751]
88/100: train_loss=[0.31162623 0.37168983], train_acc=[0.8987 0.8758]
90/100: train_loss=[0.32290468 0.3982029 ], train_acc=[0.8934  0.86705]
92/100: train_loss=[0.32014003 0.37515703], train_acc=[0.8952 0.8725]
94/100: train_loss=[0.32770482 0.37603697], train_acc=[0.8922 0.8714]
96/100: train_loss=[0.2975947  0.36262977], train_acc=[0.90345 0.8768 ]
98/100: train_loss=[0.3083693  0.36854327], train_acc=[0.8988 0.8768]
100/100: train_loss=[0.30338487 0.37434778], train_acc=[0.9011  0.87585]
**** Time taken for mnist_4 = 4290.533138036728
**** Time taken for mnist = 21182.69929742813
loading dataset fashion
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.1938242 1.2441943], train_acc=[0.55595 0.54195]
2/100: train_loss=[0.99425304 1.0510124 ], train_acc=[0.6325  0.60685]
4/100: train_loss=[0.89381635 0.95423716], train_acc=[0.6674 0.6465]
6/100: train_loss=[0.8268319  0.86246127], train_acc=[0.6922  0.68115]
8/100: train_loss=[0.80617136 0.8277443 ], train_acc=[0.698  0.6935]
10/100: train_loss=[0.7991874  0.79062295], train_acc=[0.70165 0.70815]
12/100: train_loss=[0.80234164 0.7990102 ], train_acc=[0.69615 0.70095]
14/100: train_loss=[0.72698647 0.7549755 ], train_acc=[0.7284 0.721 ]
16/100: train_loss=[0.7212781 0.7549989], train_acc=[0.7311 0.7231]
18/100: train_loss=[0.71228   0.7180245], train_acc=[0.73365 0.7347 ]
20/100: train_loss=[0.69371   0.7520541], train_acc=[0.7432  0.71885]
22/100: train_loss=[0.69128776 0.7290222 ], train_acc=[0.7375 0.7319]
24/100: train_loss=[0.68053925 0.68673944], train_acc=[0.7448 0.7472]
26/100: train_loss=[0.66823804 0.68601733], train_acc=[0.7488 0.7509]
28/100: train_loss=[0.6625753 0.6894147], train_acc=[0.7539 0.7461]
30/100: train_loss=[0.6848476  0.67495066], train_acc=[0.74365 0.75375]
32/100: train_loss=[0.6516574 0.6722874], train_acc=[0.7603  0.75795]
34/100: train_loss=[0.640259   0.65392816], train_acc=[0.7614  0.75875]
36/100: train_loss=[0.6671318 0.6821265], train_acc=[0.7506 0.7527]
38/100: train_loss=[0.62839985 0.643927  ], train_acc=[0.76685 0.7607 ]
40/100: train_loss=[0.6362973  0.65127635], train_acc=[0.7641  0.75695]
42/100: train_loss=[0.628589   0.63848233], train_acc=[0.7666  0.76525]
44/100: train_loss=[0.62875396 0.6524912 ], train_acc=[0.76785 0.758  ]
46/100: train_loss=[0.61569506 0.6549137 ], train_acc=[0.77115 0.7592 ]
48/100: train_loss=[0.6153183 0.6530834], train_acc=[0.7702 0.7577]
50/100: train_loss=[0.5933133  0.61817735], train_acc=[0.77975 0.7686 ]
52/100: train_loss=[0.6096301  0.62071043], train_acc=[0.77475 0.7675 ]
54/100: train_loss=[0.60262495 0.60315514], train_acc=[0.7762  0.77865]
56/100: train_loss=[0.6248099 0.6073654], train_acc=[0.7667 0.7741]
58/100: train_loss=[0.58812493 0.5994631 ], train_acc=[0.782   0.78095]
60/100: train_loss=[0.596445  0.6512054], train_acc=[0.7759 0.7581]
62/100: train_loss=[0.61478937 0.6075657 ], train_acc=[0.7712 0.7733]
64/100: train_loss=[0.62699074 0.64149773], train_acc=[0.7658  0.76635]
66/100: train_loss=[0.5793314 0.6155957], train_acc=[0.7861  0.76825]
68/100: train_loss=[0.56837523 0.5787596 ], train_acc=[0.78805 0.7849 ]
70/100: train_loss=[0.5784059 0.5746559], train_acc=[0.7871 0.7876]
72/100: train_loss=[0.5895049 0.6015896], train_acc=[0.7789 0.7774]
74/100: train_loss=[0.57552433 0.5734522 ], train_acc=[0.78475 0.78605]
76/100: train_loss=[0.618015   0.58102995], train_acc=[0.76955 0.7836 ]
78/100: train_loss=[0.5578252 0.5783856], train_acc=[0.79485 0.7845 ]
80/100: train_loss=[0.56417894 0.5687228 ], train_acc=[0.7906  0.79025]
82/100: train_loss=[0.5748909  0.58808404], train_acc=[0.7857 0.7832]
84/100: train_loss=[0.57412827 0.60982907], train_acc=[0.78575 0.7791 ]
86/100: train_loss=[0.62885034 0.6018712 ], train_acc=[0.7696  0.77415]
88/100: train_loss=[0.55768335 0.561914  ], train_acc=[0.7933 0.7958]
90/100: train_loss=[0.5464177 0.5538848], train_acc=[0.7968 0.7963]
92/100: train_loss=[0.556027   0.57021433], train_acc=[0.79015 0.78935]
94/100: train_loss=[0.55441564 0.561423  ], train_acc=[0.79475 0.79115]
96/100: train_loss=[0.5499923  0.55710137], train_acc=[0.79425 0.79365]
98/100: train_loss=[0.5350249  0.54850966], train_acc=[0.80165 0.7993 ]
100/100: train_loss=[0.5444723  0.55268127], train_acc=[0.79385 0.79505]
**** Time taken for fashion_0 = 3926.0060431957245
loading dataset fashion
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.1572008 1.2117234], train_acc=[0.57295 0.557  ]
2/100: train_loss=[0.9965375 1.0593405], train_acc=[0.6243  0.60825]
4/100: train_loss=[0.87591016 0.9332317 ], train_acc=[0.6743 0.6586]
6/100: train_loss=[0.84357756 0.8614123 ], train_acc=[0.68375 0.68655]
8/100: train_loss=[0.78240156 0.8272099 ], train_acc=[0.7068 0.6954]
10/100: train_loss=[0.763428   0.81687593], train_acc=[0.71645 0.70055]
12/100: train_loss=[0.7601419  0.77317375], train_acc=[0.7208 0.7181]
14/100: train_loss=[0.72925943 0.7400036 ], train_acc=[0.7268  0.72615]
16/100: train_loss=[0.7248146  0.71594805], train_acc=[0.7283  0.73495]
18/100: train_loss=[0.6882189 0.7094352], train_acc=[0.74795 0.73965]
20/100: train_loss=[0.6863784  0.71684754], train_acc=[0.7479 0.736 ]
22/100: train_loss=[0.66921645 0.6740813 ], train_acc=[0.75415 0.75065]
24/100: train_loss=[0.6515803  0.66467255], train_acc=[0.7599 0.7541]
26/100: train_loss=[0.64042276 0.6693036 ], train_acc=[0.7643 0.7552]
28/100: train_loss=[0.65175855 0.66454256], train_acc=[0.7595 0.7568]
30/100: train_loss=[0.6210311 0.6365181], train_acc=[0.7698  0.76585]
32/100: train_loss=[0.6641644  0.65170616], train_acc=[0.75425 0.7615 ]
34/100: train_loss=[0.609082   0.61897403], train_acc=[0.774 0.773]
36/100: train_loss=[0.6071571 0.6216291], train_acc=[0.77805 0.77255]
38/100: train_loss=[0.6038424 0.6252558], train_acc=[0.77745 0.7691 ]
40/100: train_loss=[0.5963302 0.6046922], train_acc=[0.7825 0.7784]
42/100: train_loss=[0.60165423 0.6176442 ], train_acc=[0.7775  0.77075]
44/100: train_loss=[0.5789113 0.6004778], train_acc=[0.78715 0.78115]
46/100: train_loss=[0.5862597 0.5948969], train_acc=[0.7829 0.781 ]
48/100: train_loss=[0.582593   0.58986855], train_acc=[0.7845 0.783 ]
50/100: train_loss=[0.6079699 0.5868775], train_acc=[0.77415 0.7826 ]
52/100: train_loss=[0.57810503 0.57826465], train_acc=[0.78845 0.7879 ]
54/100: train_loss=[0.5705907 0.5790589], train_acc=[0.79245 0.78815]
56/100: train_loss=[0.576761  0.5983701], train_acc=[0.7892  0.78155]
58/100: train_loss=[0.5745437 0.57201  ], train_acc=[0.78845 0.7906 ]
60/100: train_loss=[0.55383486 0.5702744 ], train_acc=[0.79625 0.7908 ]
62/100: train_loss=[0.5575953  0.59276944], train_acc=[0.79795 0.78405]
64/100: train_loss=[0.57361585 0.5887756 ], train_acc=[0.7886  0.79045]
66/100: train_loss=[0.55984616 0.5696725 ], train_acc=[0.7938 0.7949]
68/100: train_loss=[0.6221797 0.5909449], train_acc=[0.77155 0.7897 ]
70/100: train_loss=[0.58988374 0.62521225], train_acc=[0.7783  0.77655]
72/100: train_loss=[0.540163  0.5571789], train_acc=[0.8011 0.794 ]
74/100: train_loss=[0.5496231 0.5616976], train_acc=[0.79585 0.79745]
76/100: train_loss=[0.5393436 0.5541859], train_acc=[0.8021 0.7984]
78/100: train_loss=[0.54648376 0.5474624 ], train_acc=[0.7985 0.8012]
80/100: train_loss=[0.544501   0.56041634], train_acc=[0.7996  0.79585]
82/100: train_loss=[0.5396624 0.5533962], train_acc=[0.80105 0.80055]
84/100: train_loss=[0.52739245 0.55077934], train_acc=[0.80605 0.80115]
86/100: train_loss=[0.54921764 0.5516522 ], train_acc=[0.79835 0.79915]
88/100: train_loss=[0.5553166  0.59705555], train_acc=[0.79395 0.7852 ]
90/100: train_loss=[0.5286379  0.55826235], train_acc=[0.80495 0.79935]
92/100: train_loss=[0.527527  0.5375916], train_acc=[0.8078  0.80435]
94/100: train_loss=[0.5271364 0.5375218], train_acc=[0.80615 0.8066 ]
96/100: train_loss=[0.55525595 0.5530469 ], train_acc=[0.79775 0.80355]
98/100: train_loss=[0.52154964 0.5456334 ], train_acc=[0.8079  0.80435]
100/100: train_loss=[0.52351826 0.5464667 ], train_acc=[0.8065 0.8034]
**** Time taken for fashion_1 = 3743.3540666103363
loading dataset fashion
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.2203461 1.2968329], train_acc=[0.54895 0.52725]
2/100: train_loss=[1.0016007 1.0969253], train_acc=[0.625   0.59655]
4/100: train_loss=[0.85880446 0.93383867], train_acc=[0.67745 0.6574 ]
6/100: train_loss=[0.80260396 0.8714509 ], train_acc=[0.69975 0.68015]
8/100: train_loss=[0.7693343 0.8523927], train_acc=[0.7081 0.6874]
10/100: train_loss=[0.7479174  0.77979964], train_acc=[0.7181 0.71  ]
12/100: train_loss=[0.7355494  0.79061073], train_acc=[0.7231  0.70725]
14/100: train_loss=[0.6876182  0.72878706], train_acc=[0.74595 0.7291 ]
16/100: train_loss=[0.6896639 0.7237676], train_acc=[0.73885 0.7282 ]
18/100: train_loss=[0.65995836 0.72276866], train_acc=[0.7495  0.73175]
20/100: train_loss=[0.66302145 0.68626654], train_acc=[0.7495 0.746 ]
22/100: train_loss=[0.6305488 0.6940335], train_acc=[0.76415 0.74495]
24/100: train_loss=[0.6236622 0.6638063], train_acc=[0.76315 0.7535 ]
26/100: train_loss=[0.61247855 0.65548885], train_acc=[0.7692  0.75695]
28/100: train_loss=[0.60698724 0.6505114 ], train_acc=[0.77105 0.76335]
30/100: train_loss=[0.58336705 0.6448239 ], train_acc=[0.7799  0.76245]
32/100: train_loss=[0.61553746 0.6399267 ], train_acc=[0.76695 0.7648 ]
34/100: train_loss=[0.58065003 0.6119374 ], train_acc=[0.78485 0.77655]
36/100: train_loss=[0.57132244 0.6190981 ], train_acc=[0.782   0.77315]
38/100: train_loss=[0.5646929 0.6061833], train_acc=[0.7852 0.7795]
40/100: train_loss=[0.5677888  0.60821605], train_acc=[0.78695 0.77705]
42/100: train_loss=[0.5602783 0.6173911], train_acc=[0.78985 0.76975]
44/100: train_loss=[0.5631664 0.5923879], train_acc=[0.78785 0.78405]
46/100: train_loss=[0.5713956 0.609883 ], train_acc=[0.78515 0.77965]
48/100: train_loss=[0.54596525 0.58202124], train_acc=[0.7938 0.7894]
50/100: train_loss=[0.5574764 0.5807258], train_acc=[0.79105 0.7866 ]
52/100: train_loss=[0.5521064 0.5956198], train_acc=[0.79505 0.77875]
54/100: train_loss=[0.54423434 0.58704275], train_acc=[0.7972  0.78355]
56/100: train_loss=[0.5571315 0.5951313], train_acc=[0.7906 0.786 ]
58/100: train_loss=[0.54957217 0.5696358 ], train_acc=[0.7929  0.79455]
60/100: train_loss=[0.53765005 0.5866062 ], train_acc=[0.79815 0.78275]
62/100: train_loss=[0.52980256 0.5617406 ], train_acc=[0.80225 0.79465]
64/100: train_loss=[0.5346536 0.5976623], train_acc=[0.7991  0.78175]
66/100: train_loss=[0.52717435 0.56094056], train_acc=[0.8039  0.79695]
68/100: train_loss=[0.530341   0.56531006], train_acc=[0.80445 0.79405]
70/100: train_loss=[0.5330594 0.5960651], train_acc=[0.80075 0.7864 ]
72/100: train_loss=[0.533657   0.58030605], train_acc=[0.80025 0.7952 ]
74/100: train_loss=[0.5181116 0.5623535], train_acc=[0.80805 0.799  ]
76/100: train_loss=[0.53272593 0.5573107 ], train_acc=[0.80085 0.79735]
78/100: train_loss=[0.524102 0.562797], train_acc=[0.80595 0.79455]
80/100: train_loss=[0.51862353 0.5488781 ], train_acc=[0.80725 0.80105]
82/100: train_loss=[0.51994115 0.55167687], train_acc=[0.8073  0.79915]
84/100: train_loss=[0.51786613 0.5455592 ], train_acc=[0.80655 0.80325]
86/100: train_loss=[0.5280206 0.5647903], train_acc=[0.80215 0.7975 ]
88/100: train_loss=[0.5201559 0.5602254], train_acc=[0.8078 0.7951]
90/100: train_loss=[0.51725435 0.5694727 ], train_acc=[0.8099 0.7908]
92/100: train_loss=[0.5180465  0.55666226], train_acc=[0.80795 0.7956 ]
94/100: train_loss=[0.51282614 0.54492635], train_acc=[0.81075 0.8021 ]
96/100: train_loss=[0.51972014 0.5585489 ], train_acc=[0.80875 0.80275]
98/100: train_loss=[0.5168982 0.5502345], train_acc=[0.8098  0.80345]
100/100: train_loss=[0.5211824 0.5387101], train_acc=[0.8066 0.8051]
**** Time taken for fashion_2 = 3704.6354377269745
loading dataset fashion
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.103839 1.152665], train_acc=[0.5924  0.58235]
2/100: train_loss=[0.989236  1.0049337], train_acc=[0.6281  0.63165]
4/100: train_loss=[0.89373714 0.90145403], train_acc=[0.66965 0.66325]
6/100: train_loss=[0.87651974 0.8354179 ], train_acc=[0.6696  0.69175]
8/100: train_loss=[0.768899  0.7661975], train_acc=[0.71555 0.71975]
10/100: train_loss=[0.7406352 0.7716872], train_acc=[0.72485 0.71685]
12/100: train_loss=[0.7121117 0.7058624], train_acc=[0.73425 0.74255]
14/100: train_loss=[0.7153859  0.69356656], train_acc=[0.7315 0.7459]
16/100: train_loss=[0.66694736 0.6742942 ], train_acc=[0.7544 0.7519]
18/100: train_loss=[0.6635387 0.6563293], train_acc=[0.7553  0.76055]
20/100: train_loss=[0.64407927 0.6472952 ], train_acc=[0.76375 0.75915]
22/100: train_loss=[0.64927036 0.6345371 ], train_acc=[0.7572  0.76665]
24/100: train_loss=[0.62127984 0.6524994 ], train_acc=[0.77205 0.7548 ]
26/100: train_loss=[0.6396954  0.62294155], train_acc=[0.7641  0.76945]
28/100: train_loss=[0.6092629  0.62703395], train_acc=[0.77795 0.76565]
30/100: train_loss=[0.60338247 0.60882   ], train_acc=[0.7776  0.77625]
32/100: train_loss=[0.59863186 0.6059563 ], train_acc=[0.78005 0.7761 ]
34/100: train_loss=[0.5977522 0.6286584], train_acc=[0.7833 0.7686]
36/100: train_loss=[0.59463125 0.60229266], train_acc=[0.7837 0.7776]
38/100: train_loss=[0.59196657 0.607782  ], train_acc=[0.7855 0.7733]
40/100: train_loss=[0.5763301 0.5996338], train_acc=[0.78865 0.78315]
42/100: train_loss=[0.5822117  0.59424937], train_acc=[0.79135 0.7811 ]
44/100: train_loss=[0.57373744 0.5771193 ], train_acc=[0.7926  0.78965]
46/100: train_loss=[0.5659159  0.57789785], train_acc=[0.79455 0.78635]
48/100: train_loss=[0.5658239 0.5707548], train_acc=[0.79555 0.79145]
50/100: train_loss=[0.5835412 0.5806442], train_acc=[0.7885  0.78795]
52/100: train_loss=[0.57938176 0.57125926], train_acc=[0.7872 0.7911]
54/100: train_loss=[0.57139885 0.5761818 ], train_acc=[0.793  0.7881]
56/100: train_loss=[0.5582156 0.5710661], train_acc=[0.79705 0.7935 ]
58/100: train_loss=[0.56312615 0.5718318 ], train_acc=[0.79255 0.7922 ]
60/100: train_loss=[0.56719464 0.57292324], train_acc=[0.7936  0.79035]
62/100: train_loss=[0.5673455 0.5683897], train_acc=[0.79285 0.79115]
64/100: train_loss=[0.5766932  0.57561684], train_acc=[0.78675 0.78575]
66/100: train_loss=[0.5607391  0.56509286], train_acc=[0.79325 0.795  ]
68/100: train_loss=[0.5649907 0.5846447], train_acc=[0.79245 0.79025]
70/100: train_loss=[0.55919516 0.5628023 ], train_acc=[0.79865 0.79735]
72/100: train_loss=[0.55110353 0.56051666], train_acc=[0.79885 0.7974 ]
74/100: train_loss=[0.54898244 0.55888486], train_acc=[0.80095 0.7961 ]
76/100: train_loss=[0.55940884 0.5716344 ], train_acc=[0.7954  0.79155]
78/100: train_loss=[0.5490553  0.56201124], train_acc=[0.80115 0.7954 ]
80/100: train_loss=[0.5454782  0.56292164], train_acc=[0.80385 0.7985 ]
82/100: train_loss=[0.55347645 0.5514932 ], train_acc=[0.801  0.7995]
84/100: train_loss=[0.54081315 0.5527053 ], train_acc=[0.8029  0.79975]
86/100: train_loss=[0.5530521  0.56019104], train_acc=[0.79765 0.79635]
88/100: train_loss=[0.5397457  0.55733454], train_acc=[0.80425 0.7977 ]
90/100: train_loss=[0.53949916 0.56195545], train_acc=[0.80365 0.7984 ]
92/100: train_loss=[0.54334104 0.55494475], train_acc=[0.80475 0.79885]
94/100: train_loss=[0.53988975 0.56004614], train_acc=[0.80545 0.7989 ]
96/100: train_loss=[0.5389763  0.55592525], train_acc=[0.80455 0.7987 ]
98/100: train_loss=[0.54142326 0.5496121 ], train_acc=[0.80555 0.801  ]
100/100: train_loss=[0.53813064 0.5588076 ], train_acc=[0.8054  0.79915]
**** Time taken for fashion_3 = 3708.4881298542023
loading dataset fashion
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.230543  1.3205304], train_acc=[0.544   0.51505]
2/100: train_loss=[1.0265574 1.1089616], train_acc=[0.611   0.60035]
4/100: train_loss=[0.88267934 0.9461108 ], train_acc=[0.66775 0.65575]
6/100: train_loss=[0.8408405 0.8661094], train_acc=[0.68115 0.6837 ]
8/100: train_loss=[0.7776446  0.82779956], train_acc=[0.70765 0.6991 ]
10/100: train_loss=[0.7668279 0.7794019], train_acc=[0.7129 0.7189]
12/100: train_loss=[0.72286904 0.7832678 ], train_acc=[0.73   0.7096]
14/100: train_loss=[0.7211829  0.77961373], train_acc=[0.7292 0.714 ]
16/100: train_loss=[0.70455754 0.754496  ], train_acc=[0.73435 0.7233 ]
18/100: train_loss=[0.6768212 0.7157194], train_acc=[0.74905 0.7369 ]
20/100: train_loss=[0.69505817 0.7301537 ], train_acc=[0.7362  0.73325]
22/100: train_loss=[0.69484   0.7235357], train_acc=[0.74305 0.7452 ]
24/100: train_loss=[0.67542    0.71905965], train_acc=[0.74855 0.74145]
26/100: train_loss=[0.6419134 0.664577 ], train_acc=[0.7589  0.75885]
28/100: train_loss=[0.6440267 0.6598638], train_acc=[0.7582 0.7598]
30/100: train_loss=[0.6343361  0.67339164], train_acc=[0.764   0.75725]
32/100: train_loss=[0.6164597 0.6433231], train_acc=[0.77205 0.7689 ]
34/100: train_loss=[0.6215368 0.6370017], train_acc=[0.7732  0.77065]
36/100: train_loss=[0.6413809  0.67966986], train_acc=[0.7601  0.75245]
38/100: train_loss=[0.6051849 0.6531788], train_acc=[0.7753 0.7626]
40/100: train_loss=[0.6258799 0.6471512], train_acc=[0.76825 0.76635]
42/100: train_loss=[0.599624 0.623264], train_acc=[0.7729 0.7755]
44/100: train_loss=[0.5977153 0.630942 ], train_acc=[0.77865 0.77085]
46/100: train_loss=[0.60590154 0.61823004], train_acc=[0.77115 0.77395]
48/100: train_loss=[0.59904134 0.64325887], train_acc=[0.7787  0.76285]
50/100: train_loss=[0.5800277 0.6112377], train_acc=[0.7864  0.77665]
52/100: train_loss=[0.60444415 0.6199397 ], train_acc=[0.7757 0.776 ]
54/100: train_loss=[0.57867247 0.6595113 ], train_acc=[0.78425 0.7521 ]
56/100: train_loss=[0.61334175 0.6136123 ], train_acc=[0.76685 0.7793 ]
58/100: train_loss=[0.57500607 0.593611  ], train_acc=[0.78445 0.78825]
60/100: train_loss=[0.5757977  0.59442776], train_acc=[0.7883 0.7872]
62/100: train_loss=[0.5723216 0.5923136], train_acc=[0.79235 0.7851 ]
64/100: train_loss=[0.57790995 0.60188544], train_acc=[0.78265 0.7803 ]
66/100: train_loss=[0.56458265 0.5832758 ], train_acc=[0.79015 0.791  ]
68/100: train_loss=[0.6127867  0.59325415], train_acc=[0.77755 0.784  ]
70/100: train_loss=[0.55264544 0.5749964 ], train_acc=[0.79915 0.79395]
72/100: train_loss=[0.55317616 0.5872673 ], train_acc=[0.79635 0.7864 ]
74/100: train_loss=[0.55455434 0.5740707 ], train_acc=[0.7949 0.7926]
76/100: train_loss=[0.5899274 0.5760979], train_acc=[0.785   0.79335]
78/100: train_loss=[0.57006556 0.6049011 ], train_acc=[0.7899 0.7846]
80/100: train_loss=[0.56155723 0.5725472 ], train_acc=[0.7944  0.79285]
82/100: train_loss=[0.5519807 0.5731835], train_acc=[0.79975 0.79205]
84/100: train_loss=[0.55528075 0.56648195], train_acc=[0.79425 0.79265]
86/100: train_loss=[0.5602892 0.5737627], train_acc=[0.79365 0.78945]
88/100: train_loss=[0.5346866 0.5904102], train_acc=[0.80375 0.77875]
90/100: train_loss=[0.546291  0.5696157], train_acc=[0.7982  0.79135]
92/100: train_loss=[0.54088074 0.55367756], train_acc=[0.7988 0.799 ]
94/100: train_loss=[0.54665804 0.6004534 ], train_acc=[0.7967  0.78015]
96/100: train_loss=[0.54166096 0.5626287 ], train_acc=[0.8041 0.7944]
98/100: train_loss=[0.53813124 0.5652114 ], train_acc=[0.8035  0.79625]
100/100: train_loss=[0.54843515 0.57179457], train_acc=[0.803   0.79545]
**** Time taken for fashion_4 = 2876.828598022461
**** Time taken for fashion = 17959.357291936874
loading dataset fashion_and_mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.2868488 1.0072286], train_acc=[0.5705 0.6279]
2/100: train_loss=[1.0948713  0.89487886], train_acc=[0.6338  0.66565]
4/100: train_loss=[0.8910797 0.8004288], train_acc=[0.7042 0.7081]
6/100: train_loss=[0.74709564 0.7512312 ], train_acc=[0.752  0.7205]
8/100: train_loss=[0.7028753 0.7068925], train_acc=[0.76875 0.7388 ]
10/100: train_loss=[0.5971242  0.68698746], train_acc=[0.80565 0.74425]
12/100: train_loss=[0.5684352  0.67057425], train_acc=[0.81235 0.75055]
14/100: train_loss=[0.5361379 0.6509805], train_acc=[0.82205 0.75905]
16/100: train_loss=[0.50550014 0.64117134], train_acc=[0.83365 0.7611 ]
18/100: train_loss=[0.48359883 0.6170806 ], train_acc=[0.84285 0.76975]
20/100: train_loss=[0.47394535 0.6033348 ], train_acc=[0.8444  0.77445]
22/100: train_loss=[0.48262933 0.60621876], train_acc=[0.8418  0.77185]
24/100: train_loss=[0.48928553 0.6305814 ], train_acc=[0.83925 0.7703 ]
26/100: train_loss=[0.4469202 0.587358 ], train_acc=[0.85445 0.7792 ]
28/100: train_loss=[0.41519132 0.57171464], train_acc=[0.8656  0.78595]
30/100: train_loss=[0.40777057 0.5924052 ], train_acc=[0.8693 0.7757]
32/100: train_loss=[0.40790796 0.5725639 ], train_acc=[0.8687  0.78545]
34/100: train_loss=[0.39933228 0.5633552 ], train_acc=[0.871  0.7894]
36/100: train_loss=[0.42407593 0.5801435 ], train_acc=[0.8671  0.78325]
38/100: train_loss=[0.38720578 0.552789  ], train_acc=[0.87785 0.79145]
40/100: train_loss=[0.4363313 0.5596186], train_acc=[0.8587 0.7895]
42/100: train_loss=[0.36961   0.5490974], train_acc=[0.88155 0.7953 ]
44/100: train_loss=[0.38654593 0.55492806], train_acc=[0.87575 0.79215]
46/100: train_loss=[0.36705455 0.5744194 ], train_acc=[0.88225 0.78585]
48/100: train_loss=[0.3679107 0.5435038], train_acc=[0.88295 0.7971 ]
50/100: train_loss=[0.34922132 0.5364717 ], train_acc=[0.88945 0.79785]
52/100: train_loss=[0.37655243 0.54269224], train_acc=[0.87805 0.7969 ]
54/100: train_loss=[0.37706453 0.5586581 ], train_acc=[0.8751  0.78965]
56/100: train_loss=[0.34161618 0.53581613], train_acc=[0.8899  0.79985]
58/100: train_loss=[0.32868508 0.5270994 ], train_acc=[0.8962 0.8051]
60/100: train_loss=[0.33844686 0.5269777 ], train_acc=[0.893  0.8064]
62/100: train_loss=[0.33896402 0.5470816 ], train_acc=[0.89145 0.7991 ]
64/100: train_loss=[0.34185562 0.5250621 ], train_acc=[0.89015 0.8053 ]
66/100: train_loss=[0.35454068 0.553143  ], train_acc=[0.88485 0.795  ]
68/100: train_loss=[0.32218382 0.5242159 ], train_acc=[0.89765 0.8065 ]
70/100: train_loss=[0.3267071  0.52543914], train_acc=[0.89535 0.80555]
72/100: train_loss=[0.30983508 0.51005095], train_acc=[0.90105 0.81075]
74/100: train_loss=[0.3036737 0.5111284], train_acc=[0.9037  0.81145]
76/100: train_loss=[0.30942652 0.5143017 ], train_acc=[0.89905 0.8114 ]
78/100: train_loss=[0.30773592 0.5041224 ], train_acc=[0.90135 0.813  ]
80/100: train_loss=[0.32861125 0.5320322 ], train_acc=[0.89255 0.7992 ]
82/100: train_loss=[0.30223045 0.5030529 ], train_acc=[0.9036  0.81485]
84/100: train_loss=[0.31842908 0.5034952 ], train_acc=[0.8983 0.8147]
86/100: train_loss=[0.29563248 0.50527906], train_acc=[0.90515 0.8145 ]
88/100: train_loss=[0.2952859 0.516224 ], train_acc=[0.9069  0.80815]
90/100: train_loss=[0.30336517 0.50496924], train_acc=[0.90465 0.8137 ]
92/100: train_loss=[0.29184145 0.5259343 ], train_acc=[0.90585 0.80745]
94/100: train_loss=[0.28800622 0.5090652 ], train_acc=[0.9104 0.8128]
96/100: train_loss=[0.30251125 0.49766797], train_acc=[0.9019  0.81715]
98/100: train_loss=[0.28755465 0.49558395], train_acc=[0.9095 0.8146]
100/100: train_loss=[0.2775914 0.4920327], train_acc=[0.9125 0.8184]
**** Time taken for fashion_and_mnist_0 = 2397.4356524944305
loading dataset fashion_and_mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.1687721  0.96348673], train_acc=[0.6122 0.6488]
2/100: train_loss=[0.943676   0.85374576], train_acc=[0.6875 0.6807]
4/100: train_loss=[0.7028572 0.7452385], train_acc=[0.7764  0.72095]
6/100: train_loss=[0.6371522 0.7095128], train_acc=[0.79625 0.7378 ]
8/100: train_loss=[0.55116236 0.6701509 ], train_acc=[0.8211 0.7543]
10/100: train_loss=[0.5224098 0.6543363], train_acc=[0.83105 0.7601 ]
12/100: train_loss=[0.48359624 0.6499934 ], train_acc=[0.84365 0.7596 ]
14/100: train_loss=[0.4598067 0.6269571], train_acc=[0.85115 0.76995]
16/100: train_loss=[0.43640563 0.613876  ], train_acc=[0.86235 0.7704 ]
18/100: train_loss=[0.4345541 0.6081056], train_acc=[0.8603 0.7749]
20/100: train_loss=[0.39441285 0.6170212 ], train_acc=[0.87315 0.77   ]
22/100: train_loss=[0.37805846 0.5917431 ], train_acc=[0.8805  0.78315]
24/100: train_loss=[0.37774476 0.583322  ], train_acc=[0.87735 0.7842 ]
26/100: train_loss=[0.39675668 0.60309505], train_acc=[0.8736  0.77655]
28/100: train_loss=[0.3465262  0.63987726], train_acc=[0.88835 0.7641 ]
30/100: train_loss=[0.3335487  0.57913685], train_acc=[0.8939  0.78755]
32/100: train_loss=[0.33728454 0.5607348 ], train_acc=[0.89305 0.79305]
34/100: train_loss=[0.31272075 0.5489726 ], train_acc=[0.89845 0.7965 ]
36/100: train_loss=[0.3159241  0.54663616], train_acc=[0.8975  0.79875]
38/100: train_loss=[0.3020685 0.5457098], train_acc=[0.90475 0.7985 ]
40/100: train_loss=[0.2958179 0.5449002], train_acc=[0.9042 0.7972]
42/100: train_loss=[0.30575114 0.5504605 ], train_acc=[0.90115 0.79575]
44/100: train_loss=[0.28282857 0.5398956 ], train_acc=[0.9098 0.7991]
46/100: train_loss=[0.28601506 0.55146325], train_acc=[0.9064  0.79505]
48/100: train_loss=[0.29515353 0.5352963 ], train_acc=[0.905  0.8009]
50/100: train_loss=[0.2713884 0.5305496], train_acc=[0.913   0.80185]
52/100: train_loss=[0.26526192 0.5312752 ], train_acc=[0.91605 0.803  ]
54/100: train_loss=[0.26569277 0.53362906], train_acc=[0.9149  0.79985]
56/100: train_loss=[0.26910892 0.5245648 ], train_acc=[0.914   0.80695]
58/100: train_loss=[0.27067122 0.52235067], train_acc=[0.91375 0.80425]
60/100: train_loss=[0.2520009  0.51809025], train_acc=[0.91995 0.80725]
62/100: train_loss=[0.26206142 0.52412814], train_acc=[0.91685 0.80735]
64/100: train_loss=[0.26270828 0.52734923], train_acc=[0.91525 0.8035 ]
66/100: train_loss=[0.25663984 0.5140706 ], train_acc=[0.91775 0.8091 ]
68/100: train_loss=[0.2508836 0.5147577], train_acc=[0.91975 0.80885]
70/100: train_loss=[0.24401909 0.50860876], train_acc=[0.9228 0.8137]
72/100: train_loss=[0.24898186 0.5071376 ], train_acc=[0.9218 0.8132]
74/100: train_loss=[0.24744791 0.5133859 ], train_acc=[0.92135 0.81265]
76/100: train_loss=[0.23952214 0.509398  ], train_acc=[0.9258 0.8107]
78/100: train_loss=[0.23720795 0.50996244], train_acc=[0.926  0.8134]
80/100: train_loss=[0.23920311 0.5010846 ], train_acc=[0.92435 0.815  ]
82/100: train_loss=[0.23922437 0.5023349 ], train_acc=[0.92375 0.81605]
84/100: train_loss=[0.24109757 0.5221323 ], train_acc=[0.9236 0.8096]
86/100: train_loss=[0.23570223 0.49399096], train_acc=[0.926  0.8187]
88/100: train_loss=[0.23779935 0.5049064 ], train_acc=[0.9245 0.8167]
90/100: train_loss=[0.23158014 0.5023906 ], train_acc=[0.92675 0.8165 ]
92/100: train_loss=[0.23066138 0.5035443 ], train_acc=[0.92715 0.8136 ]
94/100: train_loss=[0.23612875 0.50190187], train_acc=[0.9243  0.81485]
96/100: train_loss=[0.24405563 0.5034668 ], train_acc=[0.92145 0.81485]
98/100: train_loss=[0.22743313 0.497835  ], train_acc=[0.9264  0.81845]
100/100: train_loss=[0.26875633 0.5111662 ], train_acc=[0.9144  0.81595]
**** Time taken for fashion_and_mnist_1 = 2470.986592531204
loading dataset fashion_and_mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.2998073 1.03601  ], train_acc=[0.56265 0.6271 ]
2/100: train_loss=[1.030384  0.9313077], train_acc=[0.6545 0.6645]
4/100: train_loss=[0.7996819  0.80526435], train_acc=[0.7337 0.7063]
6/100: train_loss=[0.6741626 0.7613914], train_acc=[0.7794 0.7153]
8/100: train_loss=[0.6039368 0.7042264], train_acc=[0.7995  0.73435]
10/100: train_loss=[0.55611855 0.6962409 ], train_acc=[0.8163  0.74065]
12/100: train_loss=[0.5195371 0.6764206], train_acc=[0.8274  0.74825]
14/100: train_loss=[0.4960597 0.6667446], train_acc=[0.83595 0.754  ]
16/100: train_loss=[0.45969594 0.62915826], train_acc=[0.84905 0.76655]
18/100: train_loss=[0.44610158 0.614756  ], train_acc=[0.8538  0.77005]
20/100: train_loss=[0.41677505 0.6135881 ], train_acc=[0.86375 0.7732 ]
22/100: train_loss=[0.40077677 0.60988635], train_acc=[0.869   0.77645]
24/100: train_loss=[0.39121613 0.5952702 ], train_acc=[0.8734 0.7775]
26/100: train_loss=[0.38170266 0.59225726], train_acc=[0.87385 0.7787 ]
28/100: train_loss=[0.37609646 0.58660084], train_acc=[0.87655 0.78315]
30/100: train_loss=[0.36227697 0.57526755], train_acc=[0.88275 0.7852 ]
32/100: train_loss=[0.39120984 0.6948112 ], train_acc=[0.8682 0.7537]
34/100: train_loss=[0.37035426 0.5644119 ], train_acc=[0.87605 0.79185]
36/100: train_loss=[0.36368215 0.554902  ], train_acc=[0.8795 0.7934]
38/100: train_loss=[0.34205216 0.58149207], train_acc=[0.88855 0.7834 ]
40/100: train_loss=[0.3427739  0.55407226], train_acc=[0.88585 0.79595]
42/100: train_loss=[0.31529647 0.54950243], train_acc=[0.89435 0.79565]
44/100: train_loss=[0.30702195 0.5407367 ], train_acc=[0.898   0.79945]
46/100: train_loss=[0.29443103 0.58019596], train_acc=[0.9031  0.78875]
48/100: train_loss=[0.29967186 0.57169306], train_acc=[0.9021 0.7895]
50/100: train_loss=[0.2899838 0.5319179], train_acc=[0.9042 0.8041]
52/100: train_loss=[0.283046   0.53054124], train_acc=[0.90625 0.8034 ]
54/100: train_loss=[0.2796843 0.5677112], train_acc=[0.90815 0.78475]
56/100: train_loss=[0.27528685 0.5180788 ], train_acc=[0.91025 0.8108 ]
58/100: train_loss=[0.2792552 0.5280527], train_acc=[0.90595 0.80415]
60/100: train_loss=[0.27049533 0.51268405], train_acc=[0.9106 0.8118]
62/100: train_loss=[0.2735991  0.53281385], train_acc=[0.90805 0.8059 ]
64/100: train_loss=[0.26479575 0.5207255 ], train_acc=[0.9118  0.81155]
66/100: train_loss=[0.2707884 0.5195498], train_acc=[0.91035 0.8098 ]
68/100: train_loss=[0.2975433 0.532005 ], train_acc=[0.90295 0.80315]
70/100: train_loss=[0.26595083 0.5383806 ], train_acc=[0.91215 0.8006 ]
72/100: train_loss=[0.26647386 0.5190029 ], train_acc=[0.91205 0.8104 ]
74/100: train_loss=[0.2576174 0.526225 ], train_acc=[0.91535 0.80575]
76/100: train_loss=[0.25694254 0.5116439 ], train_acc=[0.9165  0.81175]
78/100: train_loss=[0.27509487 0.49902272], train_acc=[0.9091 0.8152]
80/100: train_loss=[0.25531486 0.52211446], train_acc=[0.9163  0.80765]
82/100: train_loss=[0.2609011 0.5033681], train_acc=[0.9142 0.8173]
84/100: train_loss=[0.24561355 0.48867136], train_acc=[0.91965 0.8197 ]
86/100: train_loss=[0.25577047 0.508183  ], train_acc=[0.9164 0.8106]
88/100: train_loss=[0.24677919 0.49397367], train_acc=[0.9198  0.81685]
90/100: train_loss=[0.26280418 0.49556342], train_acc=[0.9151  0.81815]
92/100: train_loss=[0.24957372 0.5262028 ], train_acc=[0.9181  0.80245]
94/100: train_loss=[0.27503914 0.5070395 ], train_acc=[0.9119  0.81415]
96/100: train_loss=[0.25625557 0.50364333], train_acc=[0.91605 0.8148 ]
98/100: train_loss=[0.23494022 0.486225  ], train_acc=[0.9241 0.8221]
100/100: train_loss=[0.2525109  0.48621166], train_acc=[0.91955 0.82315]
**** Time taken for fashion_and_mnist_2 = 2401.1692504882812
loading dataset fashion_and_mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.2870125 1.1036671], train_acc=[0.56505 0.60175]
2/100: train_loss=[0.9645884 0.8841131], train_acc=[0.67125 0.67605]
4/100: train_loss=[0.76623267 0.81525433], train_acc=[0.7403  0.70545]
6/100: train_loss=[0.65914947 0.7376472 ], train_acc=[0.78115 0.73185]
8/100: train_loss=[0.5761184 0.7146499], train_acc=[0.8078  0.74045]
10/100: train_loss=[0.5384811 0.6753256], train_acc=[0.8225 0.7502]
12/100: train_loss=[0.5059823 0.6563951], train_acc=[0.83515 0.75985]
14/100: train_loss=[0.4769858  0.65642464], train_acc=[0.84575 0.76205]
16/100: train_loss=[0.44698784 0.64783627], train_acc=[0.8542 0.7609]
18/100: train_loss=[0.41217855 0.6142582 ], train_acc=[0.8668 0.7737]
20/100: train_loss=[0.42701593 0.6186718 ], train_acc=[0.86065 0.77205]
22/100: train_loss=[0.40215462 0.6112834 ], train_acc=[0.8684 0.7715]
24/100: train_loss=[0.36983934 0.59726846], train_acc=[0.8815  0.77835]
26/100: train_loss=[0.3902628  0.61311615], train_acc=[0.8725 0.7754]
28/100: train_loss=[0.3758901 0.6314087], train_acc=[0.8767 0.7658]
30/100: train_loss=[0.35762084 0.57687175], train_acc=[0.8845 0.7872]
32/100: train_loss=[0.3438421 0.577596 ], train_acc=[0.89015 0.7851 ]
34/100: train_loss=[0.35030612 0.5622226 ], train_acc=[0.8858  0.79305]
36/100: train_loss=[0.39176157 0.5616749 ], train_acc=[0.8742  0.79135]
38/100: train_loss=[0.32369664 0.55470824], train_acc=[0.89575 0.7962 ]
40/100: train_loss=[0.3281137 0.5714834], train_acc=[0.8939 0.7867]
42/100: train_loss=[0.3246829 0.5373896], train_acc=[0.8939 0.8047]
44/100: train_loss=[0.3106071 0.5319964], train_acc=[0.90245 0.8059 ]
46/100: train_loss=[0.32199374 0.5370194 ], train_acc=[0.89575 0.806  ]
48/100: train_loss=[0.29783136 0.54121083], train_acc=[0.90495 0.80055]
50/100: train_loss=[0.295593  0.5336842], train_acc=[0.9054 0.8041]
52/100: train_loss=[0.30506825 0.554374  ], train_acc=[0.90185 0.80065]
54/100: train_loss=[0.308223   0.53228694], train_acc=[0.90095 0.80735]
56/100: train_loss=[0.2873784 0.5166808], train_acc=[0.90645 0.814  ]
58/100: train_loss=[0.28710774 0.5251799 ], train_acc=[0.90905 0.81005]
60/100: train_loss=[0.28125137 0.5126699 ], train_acc=[0.90975 0.8137 ]
62/100: train_loss=[0.28302476 0.50480527], train_acc=[0.9098  0.81765]
64/100: train_loss=[0.30109763 0.5790826 ], train_acc=[0.90365 0.7858 ]
66/100: train_loss=[0.2866043  0.49944186], train_acc=[0.9068  0.81965]
68/100: train_loss=[0.32032457 0.50703216], train_acc=[0.8939 0.8161]
70/100: train_loss=[0.2769798  0.51006144], train_acc=[0.9122  0.81425]
72/100: train_loss=[0.27736342 0.53998387], train_acc=[0.9112 0.8074]
74/100: train_loss=[0.26614246 0.49770004], train_acc=[0.914   0.82185]
76/100: train_loss=[0.26379317 0.4981268 ], train_acc=[0.91575 0.82155]
78/100: train_loss=[0.279718 0.494195], train_acc=[0.91135 0.82125]
80/100: train_loss=[0.28133672 0.4894626 ], train_acc=[0.90885 0.82325]
82/100: train_loss=[0.2859385 0.5045473], train_acc=[0.9058  0.82145]
84/100: train_loss=[0.25853696 0.48633122], train_acc=[0.917   0.82605]
86/100: train_loss=[0.26479343 0.49881238], train_acc=[0.91295 0.8212 ]
88/100: train_loss=[0.2554344 0.4838526], train_acc=[0.91925 0.82475]
90/100: train_loss=[0.25476974 0.49432644], train_acc=[0.9187 0.8188]
92/100: train_loss=[0.2584923  0.49161783], train_acc=[0.91615 0.823  ]
94/100: train_loss=[0.27353877 0.49499652], train_acc=[0.9113 0.8216]
96/100: train_loss=[0.26768413 0.48087955], train_acc=[0.91365 0.82605]
98/100: train_loss=[0.27051646 0.48580605], train_acc=[0.91345 0.8262 ]
100/100: train_loss=[0.31503168 0.52827847], train_acc=[0.9016 0.8128]
**** Time taken for fashion_and_mnist_3 = 2000.6797459125519
loading dataset fashion_and_mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.3115495 1.0268183], train_acc=[0.5603 0.6297]
2/100: train_loss=[1.0427933  0.89605945], train_acc=[0.64975 0.67455]
4/100: train_loss=[0.8676806 0.7879566], train_acc=[0.70915 0.71685]
6/100: train_loss=[0.72569346 0.74195987], train_acc=[0.75965 0.7246 ]
8/100: train_loss=[0.67345643 0.71999043], train_acc=[0.77665 0.73295]
10/100: train_loss=[0.6112932  0.71295756], train_acc=[0.79675 0.73935]
12/100: train_loss=[0.5562573  0.66221285], train_acc=[0.8162  0.75835]
14/100: train_loss=[0.51860327 0.65136474], train_acc=[0.82955 0.7612 ]
16/100: train_loss=[0.49220148 0.64104664], train_acc=[0.83675 0.7663 ]
18/100: train_loss=[0.4743579 0.6300241], train_acc=[0.8431 0.7661]
20/100: train_loss=[0.4530322 0.6122482], train_acc=[0.8506 0.7715]
22/100: train_loss=[0.42485124 0.60028005], train_acc=[0.8596 0.7775]
24/100: train_loss=[0.41613042 0.5944508 ], train_acc=[0.8639  0.78065]
26/100: train_loss=[0.39079216 0.59707534], train_acc=[0.87355 0.78325]
28/100: train_loss=[0.38211948 0.5846095 ], train_acc=[0.87735 0.7834 ]
30/100: train_loss=[0.4094077  0.59418535], train_acc=[0.8654 0.7814]
32/100: train_loss=[0.35760397 0.57859063], train_acc=[0.88445 0.7863 ]
34/100: train_loss=[0.34222054 0.5615347 ], train_acc=[0.88825 0.79135]
36/100: train_loss=[0.3390133 0.5749957], train_acc=[0.89025 0.78545]
38/100: train_loss=[0.3592284 0.5518983], train_acc=[0.8818 0.8002]
40/100: train_loss=[0.3268467 0.5525972], train_acc=[0.89305 0.79395]
42/100: train_loss=[0.3232419 0.5514705], train_acc=[0.8964  0.79735]
44/100: train_loss=[0.30988097 0.56528085], train_acc=[0.8992 0.7872]
46/100: train_loss=[0.30063692 0.53927326], train_acc=[0.90305 0.80035]
48/100: train_loss=[0.30678138 0.53049034], train_acc=[0.90135 0.8053 ]
50/100: train_loss=[0.29222944 0.52615744], train_acc=[0.90495 0.8078 ]
52/100: train_loss=[0.2940087 0.5410341], train_acc=[0.9054 0.8011]
54/100: train_loss=[0.28237513 0.5187716 ], train_acc=[0.90905 0.80865]
56/100: train_loss=[0.28683475 0.52459544], train_acc=[0.90725 0.8074 ]
58/100: train_loss=[0.2798258  0.52689457], train_acc=[0.9109 0.8057]
60/100: train_loss=[0.28483617 0.52081156], train_acc=[0.9098 0.8072]
62/100: train_loss=[0.27659976 0.5242955 ], train_acc=[0.9112  0.80495]
64/100: train_loss=[0.26863813 0.5254793 ], train_acc=[0.91375 0.80435]
66/100: train_loss=[0.27140877 0.5152511 ], train_acc=[0.9121  0.80935]
68/100: train_loss=[0.27766025 0.51471657], train_acc=[0.9124 0.8112]
70/100: train_loss=[0.26309416 0.5098429 ], train_acc=[0.91625 0.8157 ]
72/100: train_loss=[0.25962952 0.50518805], train_acc=[0.9175 0.8136]
74/100: train_loss=[0.26159778 0.5055129 ], train_acc=[0.91635 0.8148 ]
76/100: train_loss=[0.25847507 0.52220726], train_acc=[0.91835 0.80805]
78/100: train_loss=[0.25237045 0.4989487 ], train_acc=[0.9198  0.81515]
80/100: train_loss=[0.2621863 0.5022014], train_acc=[0.9156 0.8127]
82/100: train_loss=[0.25409922 0.49483994], train_acc=[0.9201 0.8162]
84/100: train_loss=[0.26044896 0.50565815], train_acc=[0.9162  0.81485]
86/100: train_loss=[0.25627655 0.49263   ], train_acc=[0.91875 0.81725]
88/100: train_loss=[0.29933208 0.5002763 ], train_acc=[0.90355 0.817  ]
90/100: train_loss=[0.25715837 0.49255532], train_acc=[0.91935 0.81735]
92/100: train_loss=[0.2498827 0.5022116], train_acc=[0.91965 0.81665]
94/100: train_loss=[0.26476988 0.49545628], train_acc=[0.9157 0.8163]
96/100: train_loss=[0.28601024 0.49813652], train_acc=[0.91005 0.81725]
98/100: train_loss=[0.25966787 0.4964731 ], train_acc=[0.91745 0.81945]
100/100: train_loss=[0.2511005  0.48791936], train_acc=[0.9196  0.82135]
**** Time taken for fashion_and_mnist_4 = 1993.4016299247742
**** Time taken for fashion_and_mnist = 11263.714896202087
