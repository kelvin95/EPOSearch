Script started.
loading dataset mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.309245  1.4654752], train_acc=[0.5631  0.49025]
2/100: train_loss=[1.0725336 1.2069714], train_acc=[0.64745 0.58395]
4/100: train_loss=[0.8119323 0.9081707], train_acc=[0.7336 0.6905]
6/100: train_loss=[0.67153686 0.79412204], train_acc=[0.7784  0.72875]
8/100: train_loss=[0.60289526 0.6963989 ], train_acc=[0.80025 0.76485]
10/100: train_loss=[0.5551516 0.6630561], train_acc=[0.8161  0.77625]
12/100: train_loss=[0.5245383 0.6379331], train_acc=[0.82605 0.78645]
14/100: train_loss=[0.5389235 0.5861818], train_acc=[0.8219  0.80005]
16/100: train_loss=[0.47707063 0.55044866], train_acc=[0.8433 0.8154]
18/100: train_loss=[0.4662762 0.5360129], train_acc=[0.84495 0.8207 ]
20/100: train_loss=[0.45480385 0.5327768 ], train_acc=[0.85025 0.8226 ]
22/100: train_loss=[0.45698375 0.53891855], train_acc=[0.8486 0.8211]
24/100: train_loss=[0.44549254 0.50612485], train_acc=[0.85335 0.8306 ]
26/100: train_loss=[0.4281876 0.5357683], train_acc=[0.86015 0.81975]
28/100: train_loss=[0.40624088 0.48670846], train_acc=[0.8654  0.83505]
30/100: train_loss=[0.39215103 0.480118  ], train_acc=[0.87085 0.8408 ]
32/100: train_loss=[0.39621872 0.49459153], train_acc=[0.8708  0.83665]
34/100: train_loss=[0.37670013 0.4672901 ], train_acc=[0.87615 0.8416 ]
36/100: train_loss=[0.38443854 0.4534568 ], train_acc=[0.8732  0.84895]
38/100: train_loss=[0.38909972 0.43484536], train_acc=[0.8697 0.8554]
40/100: train_loss=[0.3582938  0.44650975], train_acc=[0.88285 0.8537 ]
42/100: train_loss=[0.3548294  0.42148775], train_acc=[0.88335 0.85915]
44/100: train_loss=[0.35102195 0.42100757], train_acc=[0.882   0.86005]
46/100: train_loss=[0.34882057 0.45313776], train_acc=[0.88575 0.8499 ]
48/100: train_loss=[0.35343525 0.4095637 ], train_acc=[0.8833 0.8652]
50/100: train_loss=[0.33765578 0.4055905 ], train_acc=[0.88855 0.867  ]
52/100: train_loss=[0.3515709  0.41755697], train_acc=[0.88435 0.8593 ]
54/100: train_loss=[0.33549282 0.3943507 ], train_acc=[0.88935 0.86895]
56/100: train_loss=[0.36138624 0.3928769 ], train_acc=[0.88125 0.8706 ]
58/100: train_loss=[0.33981818 0.40914908], train_acc=[0.88775 0.86395]
60/100: train_loss=[0.3419507 0.3920622], train_acc=[0.8887 0.8697]
62/100: train_loss=[0.31989232 0.37634295], train_acc=[0.8956 0.8736]
64/100: train_loss=[0.31942153 0.39046624], train_acc=[0.89555 0.8687 ]
66/100: train_loss=[0.3106576  0.37163278], train_acc=[0.898   0.87595]
68/100: train_loss=[0.35815716 0.48182154], train_acc=[0.8839 0.8417]
70/100: train_loss=[0.32574478 0.375924  ], train_acc=[0.8921  0.87415]
72/100: train_loss=[0.30545446 0.36830285], train_acc=[0.90015 0.8772 ]
74/100: train_loss=[0.30162913 0.3643524 ], train_acc=[0.90055 0.87925]
76/100: train_loss=[0.30493343 0.3632378 ], train_acc=[0.9008  0.87865]
78/100: train_loss=[0.3139617  0.36137158], train_acc=[0.89605 0.8791 ]
80/100: train_loss=[0.30592936 0.37581685], train_acc=[0.8979  0.87635]
82/100: train_loss=[0.32248583 0.36801878], train_acc=[0.8947  0.87715]
84/100: train_loss=[0.30543843 0.3677284 ], train_acc=[0.9009  0.87725]
86/100: train_loss=[0.29619643 0.37129784], train_acc=[0.90245 0.87715]
88/100: train_loss=[0.30730906 0.37650615], train_acc=[0.90095 0.8766 ]
90/100: train_loss=[0.28968847 0.3566593 ], train_acc=[0.90485 0.8802 ]
92/100: train_loss=[0.30102277 0.36966556], train_acc=[0.90155 0.8776 ]
94/100: train_loss=[0.29143238 0.35214993], train_acc=[0.9044  0.88395]
96/100: train_loss=[0.30275202 0.35774538], train_acc=[0.90005 0.8802 ]
98/100: train_loss=[0.29625198 0.3440697 ], train_acc=[0.90225 0.8885 ]
100/100: train_loss=[0.30152318 0.39511606], train_acc=[0.9008 0.869 ]
**** Time taken for mnist_0 = 5819.8070278167725
loading dataset mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.456699  1.6971706], train_acc=[0.50355 0.40605]
2/100: train_loss=[1.1717949 1.363133 ], train_acc=[0.6041  0.52745]
4/100: train_loss=[0.82841516 0.99165946], train_acc=[0.7286 0.6603]
6/100: train_loss=[0.6894879  0.83325523], train_acc=[0.7771 0.7161]
8/100: train_loss=[0.62250787 0.76853365], train_acc=[0.79635 0.7373 ]
10/100: train_loss=[0.58962667 0.70403206], train_acc=[0.80775 0.7598 ]
12/100: train_loss=[0.5629486 0.655399 ], train_acc=[0.8173  0.77755]
14/100: train_loss=[0.5295771 0.6258835], train_acc=[0.8271 0.7877]
16/100: train_loss=[0.52701193 0.602118  ], train_acc=[0.82625 0.79745]
18/100: train_loss=[0.48851043 0.5853137 ], train_acc=[0.8412  0.80165]
20/100: train_loss=[0.4658807 0.561717 ], train_acc=[0.84815 0.81045]
22/100: train_loss=[0.45658726 0.55408686], train_acc=[0.8527  0.81145]
24/100: train_loss=[0.4404353 0.5353294], train_acc=[0.8574  0.81775]
26/100: train_loss=[0.43025765 0.5058284 ], train_acc=[0.8627  0.82845]
28/100: train_loss=[0.41359267 0.4966    ], train_acc=[0.8666 0.8333]
30/100: train_loss=[0.40765107 0.49888018], train_acc=[0.8679  0.83145]
32/100: train_loss=[0.44708544 0.49779183], train_acc=[0.851   0.83245]
34/100: train_loss=[0.39922816 0.46583393], train_acc=[0.86975 0.8431 ]
36/100: train_loss=[0.38692954 0.45481652], train_acc=[0.8753 0.8456]
38/100: train_loss=[0.38797674 0.49795622], train_acc=[0.8759  0.83475]
40/100: train_loss=[0.42119122 0.4630174 ], train_acc=[0.86225 0.84505]
42/100: train_loss=[0.36739686 0.44803086], train_acc=[0.8808  0.84865]
44/100: train_loss=[0.37319568 0.46288112], train_acc=[0.87975 0.84815]
46/100: train_loss=[0.3844116  0.45505485], train_acc=[0.8755  0.84575]
48/100: train_loss=[0.35270116 0.43414038], train_acc=[0.88655 0.85505]
50/100: train_loss=[0.36066288 0.42641082], train_acc=[0.8854 0.8593]
52/100: train_loss=[0.34181643 0.40464   ], train_acc=[0.89025 0.86485]
54/100: train_loss=[0.35393485 0.44055042], train_acc=[0.88575 0.85445]
56/100: train_loss=[0.3648449 0.5219675], train_acc=[0.88115 0.8231 ]
58/100: train_loss=[0.33566904 0.40085295], train_acc=[0.89105 0.866  ]
60/100: train_loss=[0.3394084  0.39679766], train_acc=[0.8893  0.86775]
62/100: train_loss=[0.3395152  0.40827876], train_acc=[0.89105 0.86365]
64/100: train_loss=[0.37444043 0.4015257 ], train_acc=[0.88245 0.8674 ]
66/100: train_loss=[0.31656504 0.39870963], train_acc=[0.8984 0.8675]
68/100: train_loss=[0.3419289 0.4041575], train_acc=[0.8886 0.8653]
70/100: train_loss=[0.31801185 0.37480062], train_acc=[0.89865 0.8723 ]
72/100: train_loss=[0.35283214 0.3975627 ], train_acc=[0.8854 0.8675]
74/100: train_loss=[0.31880566 0.37173098], train_acc=[0.89765 0.87665]
76/100: train_loss=[0.30162182 0.3676867 ], train_acc=[0.9011 0.8784]
78/100: train_loss=[0.36512372 0.3975074 ], train_acc=[0.88215 0.8672 ]
80/100: train_loss=[0.29450083 0.38166785], train_acc=[0.9041 0.872 ]
82/100: train_loss=[0.3012285  0.41016337], train_acc=[0.9026 0.8653]
84/100: train_loss=[0.29671845 0.39628524], train_acc=[0.90275 0.8679 ]
86/100: train_loss=[0.29021734 0.37458327], train_acc=[0.90575 0.87645]
88/100: train_loss=[0.28797165 0.3546211 ], train_acc=[0.9074  0.88175]
90/100: train_loss=[0.28845152 0.37437135], train_acc=[0.9062 0.8762]
92/100: train_loss=[0.29599318 0.34758082], train_acc=[0.90415 0.8861 ]
94/100: train_loss=[0.28764895 0.3629109 ], train_acc=[0.9073 0.8814]
96/100: train_loss=[0.2843212 0.366259 ], train_acc=[0.90575 0.87825]
98/100: train_loss=[0.2879678 0.4323298], train_acc=[0.90595 0.86405]
100/100: train_loss=[0.29928946 0.38817766], train_acc=[0.90505 0.8738 ]
**** Time taken for mnist_1 = 4278.897674798965
loading dataset mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.0504192 1.3493854], train_acc=[0.6512  0.53435]
2/100: train_loss=[0.886241  1.1291671], train_acc=[0.7079 0.6077]
4/100: train_loss=[0.7608836  0.92336655], train_acc=[0.75225 0.6883 ]
6/100: train_loss=[0.64217097 0.77653205], train_acc=[0.7881 0.7384]
8/100: train_loss=[0.5680397  0.69458055], train_acc=[0.8155  0.76615]
10/100: train_loss=[0.514122   0.63652086], train_acc=[0.8322  0.78685]
12/100: train_loss=[0.47764805 0.5983818 ], train_acc=[0.8456  0.80255]
14/100: train_loss=[0.4580387  0.59416175], train_acc=[0.85125 0.80275]
16/100: train_loss=[0.44251195 0.5461344 ], train_acc=[0.8571 0.8205]
18/100: train_loss=[0.45348403 0.52372724], train_acc=[0.8552  0.82515]
20/100: train_loss=[0.40316594 0.54073626], train_acc=[0.8684 0.8209]
22/100: train_loss=[0.40345418 0.49633935], train_acc=[0.8702  0.83575]
24/100: train_loss=[0.39190188 0.48995104], train_acc=[0.8734 0.8378]
26/100: train_loss=[0.41488445 0.58523446], train_acc=[0.86695 0.8115 ]
28/100: train_loss=[0.38878542 0.46857348], train_acc=[0.8741 0.8458]
30/100: train_loss=[0.3641723  0.45780233], train_acc=[0.8838 0.8483]
32/100: train_loss=[0.3637348  0.46518472], train_acc=[0.881  0.8429]
34/100: train_loss=[0.4292111  0.48564786], train_acc=[0.86325 0.8404 ]
36/100: train_loss=[0.35411835 0.44975623], train_acc=[0.8833 0.8542]
38/100: train_loss=[0.34702736 0.4534658 ], train_acc=[0.8874 0.8496]
40/100: train_loss=[0.3404982 0.4220358], train_acc=[0.89005 0.85835]
42/100: train_loss=[0.35244235 0.42030117], train_acc=[0.8848 0.8627]
44/100: train_loss=[0.34580687 0.40514183], train_acc=[0.88635 0.8663 ]
46/100: train_loss=[0.3366287  0.46287706], train_acc=[0.8881  0.84325]
48/100: train_loss=[0.37087727 0.41562948], train_acc=[0.8803  0.86475]
50/100: train_loss=[0.33732376 0.4078858 ], train_acc=[0.8904  0.86445]
52/100: train_loss=[0.34648603 0.41141385], train_acc=[0.8882 0.8655]
54/100: train_loss=[0.33275157 0.42372307], train_acc=[0.8894  0.86075]
56/100: train_loss=[0.3312906 0.3806509], train_acc=[0.89115 0.87455]
58/100: train_loss=[0.34825787 0.4252671 ], train_acc=[0.8857 0.8575]
60/100: train_loss=[0.3037619  0.39055488], train_acc=[0.90215 0.8723 ]
62/100: train_loss=[0.30388826 0.37174258], train_acc=[0.90055 0.87595]
64/100: train_loss=[0.33361754 0.38659748], train_acc=[0.8908  0.87255]
66/100: train_loss=[0.3542018  0.38592765], train_acc=[0.8857 0.8712]
68/100: train_loss=[0.295087   0.36318526], train_acc=[0.9035  0.87915]
70/100: train_loss=[0.30927184 0.3744611 ], train_acc=[0.89855 0.87435]
72/100: train_loss=[0.30516487 0.3753636 ], train_acc=[0.90005 0.87555]
74/100: train_loss=[0.29999194 0.3565118 ], train_acc=[0.90275 0.8829 ]
76/100: train_loss=[0.3296174  0.37007403], train_acc=[0.8921 0.8771]
78/100: train_loss=[0.29578424 0.36760834], train_acc=[0.90435 0.8785 ]
80/100: train_loss=[0.29976553 0.35943258], train_acc=[0.9023 0.8809]
82/100: train_loss=[0.30515903 0.3880604 ], train_acc=[0.89945 0.8709 ]
84/100: train_loss=[0.2891123  0.35341284], train_acc=[0.9061  0.88305]
86/100: train_loss=[0.28535828 0.35134983], train_acc=[0.90645 0.884  ]
88/100: train_loss=[0.2915841 0.3476009], train_acc=[0.9037 0.8846]
90/100: train_loss=[0.29894903 0.35163012], train_acc=[0.90195 0.88405]
92/100: train_loss=[0.30009925 0.3680485 ], train_acc=[0.90315 0.8792 ]
94/100: train_loss=[0.29548216 0.36422402], train_acc=[0.90425 0.87945]
96/100: train_loss=[0.30099535 0.35216287], train_acc=[0.9    0.8844]
98/100: train_loss=[0.28693312 0.34290838], train_acc=[0.9055 0.8872]
100/100: train_loss=[0.2946173 0.3643446], train_acc=[0.9033 0.8804]
**** Time taken for mnist_2 = 3887.1515548229218
loading dataset mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.1715508 1.3650854], train_acc=[0.61575 0.5373 ]
2/100: train_loss=[0.93638873 1.1245561 ], train_acc=[0.6919  0.61515]
4/100: train_loss=[0.7425687 0.910344 ], train_acc=[0.7568 0.6957]
6/100: train_loss=[0.6642476  0.78301847], train_acc=[0.78535 0.73655]
8/100: train_loss=[0.60317814 0.705558  ], train_acc=[0.80375 0.76525]
10/100: train_loss=[0.6045625  0.69147253], train_acc=[0.80795 0.77035]
12/100: train_loss=[0.5436643 0.6442545], train_acc=[0.8255  0.78675]
14/100: train_loss=[0.5561847  0.61241114], train_acc=[0.81855 0.7964 ]
16/100: train_loss=[0.5095611 0.5781254], train_acc=[0.8351 0.8065]
18/100: train_loss=[0.49649176 0.5498474 ], train_acc=[0.8396  0.81805]
20/100: train_loss=[0.4849309 0.5293512], train_acc=[0.84335 0.82215]
22/100: train_loss=[0.48025775 0.52101386], train_acc=[0.8458  0.82635]
24/100: train_loss=[0.4532255 0.5029696], train_acc=[0.8555 0.8346]
26/100: train_loss=[0.45413798 0.4973416 ], train_acc=[0.85365 0.83475]
28/100: train_loss=[0.46180815 0.51171136], train_acc=[0.8499  0.83085]
30/100: train_loss=[0.4416672  0.47840014], train_acc=[0.8566  0.84155]
32/100: train_loss=[0.43596274 0.47467646], train_acc=[0.8597  0.84265]
34/100: train_loss=[0.42159253 0.45927218], train_acc=[0.86315 0.84735]
36/100: train_loss=[0.40114135 0.4610798 ], train_acc=[0.8689 0.8469]
38/100: train_loss=[0.43083346 0.47511804], train_acc=[0.8602 0.8425]
40/100: train_loss=[0.39906055 0.4416306 ], train_acc=[0.8684 0.8561]
42/100: train_loss=[0.38834277 0.4442734 ], train_acc=[0.8749 0.8534]
44/100: train_loss=[0.41163057 0.44664484], train_acc=[0.86715 0.8517 ]
46/100: train_loss=[0.40739605 0.44199285], train_acc=[0.86795 0.85515]
48/100: train_loss=[0.38846886 0.42338428], train_acc=[0.8757  0.86065]
50/100: train_loss=[0.365471   0.41360083], train_acc=[0.87955 0.86425]
52/100: train_loss=[0.35734022 0.4088323 ], train_acc=[0.8827 0.8659]
54/100: train_loss=[0.3763848  0.41947186], train_acc=[0.87685 0.86285]
56/100: train_loss=[0.3586399  0.40041795], train_acc=[0.88045 0.87085]
58/100: train_loss=[0.38269207 0.41078565], train_acc=[0.8744  0.86435]
60/100: train_loss=[0.3626386  0.41121104], train_acc=[0.8808 0.8665]
62/100: train_loss=[0.37717932 0.4324015 ], train_acc=[0.87665 0.85755]
64/100: train_loss=[0.3494796  0.40808064], train_acc=[0.88555 0.866  ]
66/100: train_loss=[0.35779303 0.40816012], train_acc=[0.88395 0.86515]
68/100: train_loss=[0.33853823 0.4256678 ], train_acc=[0.8882 0.8604]
70/100: train_loss=[0.33197367 0.40948227], train_acc=[0.89205 0.8639 ]
72/100: train_loss=[0.3474806  0.39658505], train_acc=[0.8878  0.86835]
74/100: train_loss=[0.33493266 0.40630987], train_acc=[0.8901  0.86775]
76/100: train_loss=[0.32809895 0.3740301 ], train_acc=[0.89375 0.8753 ]
78/100: train_loss=[0.3191427 0.3728598], train_acc=[0.8957  0.87745]
80/100: train_loss=[0.33215004 0.39653224], train_acc=[0.8907 0.8683]
82/100: train_loss=[0.34315485 0.3843959 ], train_acc=[0.88765 0.8718 ]
84/100: train_loss=[0.31555903 0.39790925], train_acc=[0.8961 0.8686]
86/100: train_loss=[0.3139846 0.3930798], train_acc=[0.8988 0.8683]
88/100: train_loss=[0.32264146 0.35334763], train_acc=[0.8954 0.8817]
90/100: train_loss=[0.32753524 0.36221337], train_acc=[0.8937  0.87875]
92/100: train_loss=[0.31045327 0.34686077], train_acc=[0.899   0.88365]
94/100: train_loss=[0.30809316 0.3626634 ], train_acc=[0.90005 0.8791 ]
96/100: train_loss=[0.3079492  0.34652117], train_acc=[0.901   0.88385]
98/100: train_loss=[0.3055335  0.35049462], train_acc=[0.9006  0.88265]
100/100: train_loss=[0.30947155 0.3580771 ], train_acc=[0.8998  0.87985]
**** Time taken for mnist_3 = 3824.0738039016724
loading dataset mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.1145507 1.4104099], train_acc=[0.63315 0.5153 ]
2/100: train_loss=[0.8561474 1.0324371], train_acc=[0.71815 0.6494 ]
4/100: train_loss=[0.66056633 0.7835665 ], train_acc=[0.78515 0.7419 ]
6/100: train_loss=[0.59187406 0.69811094], train_acc=[0.80885 0.77065]
8/100: train_loss=[0.528293  0.6095067], train_acc=[0.82965 0.80055]
10/100: train_loss=[0.5076085 0.6380382], train_acc=[0.83695 0.7937 ]
12/100: train_loss=[0.4690202  0.53968966], train_acc=[0.8485  0.82255]
14/100: train_loss=[0.45895725 0.52241176], train_acc=[0.85055 0.8292 ]
16/100: train_loss=[0.45762864 0.53157645], train_acc=[0.85195 0.82155]
18/100: train_loss=[0.41625634 0.48613518], train_acc=[0.8652 0.8391]
20/100: train_loss=[0.42312074 0.47518334], train_acc=[0.8628  0.84085]
22/100: train_loss=[0.39818457 0.46232074], train_acc=[0.87065 0.84645]
24/100: train_loss=[0.44011787 0.45155576], train_acc=[0.8561  0.85125]
26/100: train_loss=[0.40988603 0.4818773 ], train_acc=[0.86765 0.8397 ]
28/100: train_loss=[0.37403136 0.4524774 ], train_acc=[0.8785  0.84965]
30/100: train_loss=[0.37247422 0.43324   ], train_acc=[0.8787  0.85525]
32/100: train_loss=[0.38380584 0.4379931 ], train_acc=[0.8744 0.8558]
34/100: train_loss=[0.3552969  0.40851068], train_acc=[0.88475 0.8663 ]
36/100: train_loss=[0.35942397 0.42467418], train_acc=[0.88355 0.85955]
38/100: train_loss=[0.3996082  0.42433468], train_acc=[0.87115 0.85985]
40/100: train_loss=[0.3515916  0.40638366], train_acc=[0.88645 0.8658 ]
42/100: train_loss=[0.35127735 0.41537347], train_acc=[0.8865  0.86615]
44/100: train_loss=[0.33461794 0.38719198], train_acc=[0.88995 0.8703 ]
46/100: train_loss=[0.33863792 0.38832808], train_acc=[0.88835 0.8717 ]
48/100: train_loss=[0.3631832  0.44672993], train_acc=[0.88    0.85285]
50/100: train_loss=[0.3245806  0.38163117], train_acc=[0.8936 0.8731]
52/100: train_loss=[0.3290064 0.3986523], train_acc=[0.89245 0.86585]
54/100: train_loss=[0.32343027 0.3748863 ], train_acc=[0.8949 0.8749]
56/100: train_loss=[0.31930867 0.379068  ], train_acc=[0.8957  0.87615]
58/100: train_loss=[0.3395469  0.37879914], train_acc=[0.89075 0.87455]
60/100: train_loss=[0.32927996 0.369292  ], train_acc=[0.8912  0.87625]
62/100: train_loss=[0.32124245 0.38220417], train_acc=[0.897  0.8736]
64/100: train_loss=[0.31282446 0.36240807], train_acc=[0.89705 0.8792 ]
66/100: train_loss=[0.3038422  0.37244335], train_acc=[0.9009  0.87535]
68/100: train_loss=[0.3124363  0.35823637], train_acc=[0.89835 0.8807 ]
70/100: train_loss=[0.3006576  0.36131623], train_acc=[0.90215 0.8814 ]
72/100: train_loss=[0.32239592 0.36356542], train_acc=[0.89425 0.8791 ]
74/100: train_loss=[0.311585  0.3750819], train_acc=[0.89885 0.8758 ]
76/100: train_loss=[0.31603226 0.37164035], train_acc=[0.89655 0.87805]
78/100: train_loss=[0.3017046  0.35145465], train_acc=[0.90005 0.8827 ]
80/100: train_loss=[0.30891395 0.37533784], train_acc=[0.8994  0.87635]
82/100: train_loss=[0.30364823 0.3587276 ], train_acc=[0.8993  0.87995]
84/100: train_loss=[0.29693478 0.37933215], train_acc=[0.90295 0.8745 ]
86/100: train_loss=[0.307012   0.35076788], train_acc=[0.89925 0.88375]
88/100: train_loss=[0.28877103 0.34610575], train_acc=[0.90445 0.8866 ]
90/100: train_loss=[0.29148558 0.3459803 ], train_acc=[0.90555 0.885  ]
92/100: train_loss=[0.2938851  0.36153525], train_acc=[0.9044  0.88075]
94/100: train_loss=[0.3058849 0.3510998], train_acc=[0.9006 0.8839]
96/100: train_loss=[0.29006866 0.344437  ], train_acc=[0.90485 0.88665]
98/100: train_loss=[0.29228815 0.34248072], train_acc=[0.90415 0.8868 ]
100/100: train_loss=[0.28862324 0.3446747 ], train_acc=[0.9055 0.8859]
**** Time taken for mnist_4 = 3826.1720509529114
**** Time taken for mnist = 21636.20051908493
loading dataset fashion
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.1432061 1.3089912], train_acc=[0.56245 0.51895]
2/100: train_loss=[1.0462991 1.1597952], train_acc=[0.60215 0.5735 ]
4/100: train_loss=[0.92345005 1.0078928 ], train_acc=[0.6579  0.62685]
6/100: train_loss=[0.8649861  0.93778914], train_acc=[0.67615 0.64995]
8/100: train_loss=[0.834027   0.88578975], train_acc=[0.6903  0.67745]
10/100: train_loss=[0.78935313 0.8555416 ], train_acc=[0.6994  0.69095]
12/100: train_loss=[0.761056  0.8083137], train_acc=[0.7191 0.7004]
14/100: train_loss=[0.74659497 0.7751198 ], train_acc=[0.72545 0.71545]
16/100: train_loss=[0.7238792 0.7667222], train_acc=[0.73595 0.7189 ]
18/100: train_loss=[0.70087737 0.73715514], train_acc=[0.73805 0.7294 ]
20/100: train_loss=[0.69915235 0.7324485 ], train_acc=[0.74055 0.73645]
22/100: train_loss=[0.6757311  0.71125394], train_acc=[0.7487  0.74555]
24/100: train_loss=[0.6934462  0.71916306], train_acc=[0.75165 0.73635]
26/100: train_loss=[0.6763936 0.6939779], train_acc=[0.7483 0.7465]
28/100: train_loss=[0.67273146 0.6818351 ], train_acc=[0.7499  0.75405]
30/100: train_loss=[0.63499147 0.6574772 ], train_acc=[0.7662 0.7616]
32/100: train_loss=[0.63849837 0.662203  ], train_acc=[0.76255 0.7567 ]
34/100: train_loss=[0.6167594  0.65473324], train_acc=[0.7753 0.7617]
36/100: train_loss=[0.63212913 0.73649824], train_acc=[0.7659 0.7308]
38/100: train_loss=[0.602421  0.6361517], train_acc=[0.78105 0.76645]
40/100: train_loss=[0.6134719  0.62792075], train_acc=[0.7716 0.7699]
42/100: train_loss=[0.6013033 0.6356463], train_acc=[0.7787 0.7635]
44/100: train_loss=[0.5847071 0.6247853], train_acc=[0.78435 0.77245]
46/100: train_loss=[0.58573306 0.61186796], train_acc=[0.78105 0.77815]
48/100: train_loss=[0.57571036 0.6084099 ], train_acc=[0.7875  0.77955]
50/100: train_loss=[0.59285945 0.6048061 ], train_acc=[0.78145 0.7821 ]
52/100: train_loss=[0.5670381  0.62012756], train_acc=[0.7915 0.7755]
54/100: train_loss=[0.5668114  0.62043315], train_acc=[0.7924 0.78  ]
56/100: train_loss=[0.5635346 0.6041796], train_acc=[0.7942 0.7788]
58/100: train_loss=[0.572672  0.5975227], train_acc=[0.79265 0.78195]
60/100: train_loss=[0.57445467 0.60725105], train_acc=[0.79065 0.7792 ]
62/100: train_loss=[0.56296307 0.60063374], train_acc=[0.79245 0.77855]
64/100: train_loss=[0.5583276  0.58720475], train_acc=[0.79585 0.78605]
66/100: train_loss=[0.5852279 0.6567228], train_acc=[0.7856  0.76045]
68/100: train_loss=[0.57875717 0.59838796], train_acc=[0.7834 0.7852]
70/100: train_loss=[0.55703807 0.5931231 ], train_acc=[0.79535 0.78655]
72/100: train_loss=[0.56911343 0.59971833], train_acc=[0.79095 0.782  ]
74/100: train_loss=[0.54424    0.57587516], train_acc=[0.801  0.7918]
76/100: train_loss=[0.55386245 0.6058599 ], train_acc=[0.79625 0.77905]
78/100: train_loss=[0.5394571 0.5802401], train_acc=[0.80275 0.79295]
80/100: train_loss=[0.5297805  0.57465714], train_acc=[0.80385 0.79635]
82/100: train_loss=[0.5371683 0.5799683], train_acc=[0.803   0.79115]
84/100: train_loss=[0.5351497  0.57944167], train_acc=[0.7999  0.78885]
86/100: train_loss=[0.54547757 0.58397925], train_acc=[0.79825 0.7852 ]
88/100: train_loss=[0.53187746 0.5698486 ], train_acc=[0.805  0.7919]
90/100: train_loss=[0.5265477  0.56523424], train_acc=[0.80625 0.79845]
92/100: train_loss=[0.535725  0.5805325], train_acc=[0.8037 0.7913]
94/100: train_loss=[0.5336269 0.566329 ], train_acc=[0.8027 0.7957]
96/100: train_loss=[0.5364166 0.5729322], train_acc=[0.80405 0.7899 ]
98/100: train_loss=[0.528441  0.5725081], train_acc=[0.8046 0.7917]
100/100: train_loss=[0.52858883 0.56999046], train_acc=[0.8074 0.7949]
**** Time taken for fashion_0 = 3824.2164034843445
loading dataset fashion
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.1292309 1.2625374], train_acc=[0.58855 0.5382 ]
2/100: train_loss=[1.0045192 1.1087571], train_acc=[0.63525 0.60425]
4/100: train_loss=[0.88770825 0.9747644 ], train_acc=[0.67275 0.6451 ]
6/100: train_loss=[0.8459052 0.9110194], train_acc=[0.69015 0.6671 ]
8/100: train_loss=[0.80042773 0.8631682 ], train_acc=[0.70355 0.68975]
10/100: train_loss=[0.805882   0.82620645], train_acc=[0.70205 0.69635]
12/100: train_loss=[0.753067  0.8391184], train_acc=[0.72335 0.69765]
14/100: train_loss=[0.7666722 0.7872055], train_acc=[0.7145  0.70585]
16/100: train_loss=[0.7295611  0.77019066], train_acc=[0.73335 0.72305]
18/100: train_loss=[0.70079166 0.7477921 ], train_acc=[0.7436 0.7265]
20/100: train_loss=[0.69407576 0.7186401 ], train_acc=[0.745   0.73285]
22/100: train_loss=[0.67795324 0.70678556], train_acc=[0.75385 0.7424 ]
24/100: train_loss=[0.66667444 0.69377416], train_acc=[0.75615 0.7432 ]
26/100: train_loss=[0.67968434 0.7038336 ], train_acc=[0.74645 0.7387 ]
28/100: train_loss=[0.6524345 0.6751008], train_acc=[0.7625 0.7496]
30/100: train_loss=[0.64729893 0.6628087 ], train_acc=[0.7615 0.7582]
32/100: train_loss=[0.6534486  0.67064977], train_acc=[0.76125 0.75415]
34/100: train_loss=[0.6386662  0.65624946], train_acc=[0.76715 0.76255]
36/100: train_loss=[0.627995   0.64607114], train_acc=[0.7688  0.76455]
38/100: train_loss=[0.6257821  0.64678586], train_acc=[0.77025 0.76265]
40/100: train_loss=[0.6222346 0.653284 ], train_acc=[0.7692  0.76205]
42/100: train_loss=[0.6173413 0.6363586], train_acc=[0.77445 0.76685]
44/100: train_loss=[0.63551617 0.6535165 ], train_acc=[0.7684 0.7596]
46/100: train_loss=[0.615518  0.6234639], train_acc=[0.7756  0.77335]
48/100: train_loss=[0.637758   0.61898184], train_acc=[0.75885 0.77205]
50/100: train_loss=[0.58811367 0.61628234], train_acc=[0.78505 0.7752 ]
52/100: train_loss=[0.6008382 0.6150182], train_acc=[0.7804  0.77575]
54/100: train_loss=[0.5868759 0.6068976], train_acc=[0.7839  0.78005]
56/100: train_loss=[0.57873005 0.59747416], train_acc=[0.78675 0.7814 ]
58/100: train_loss=[0.5867973 0.6026141], train_acc=[0.78145 0.7783 ]
60/100: train_loss=[0.57111883 0.606337  ], train_acc=[0.7925  0.77925]
62/100: train_loss=[0.56784326 0.59069103], train_acc=[0.7919 0.7815]
64/100: train_loss=[0.6002158 0.6169925], train_acc=[0.77685 0.77235]
66/100: train_loss=[0.56367147 0.59131813], train_acc=[0.79575 0.78255]
68/100: train_loss=[0.55871266 0.5906587 ], train_acc=[0.7953  0.78305]
70/100: train_loss=[0.5775165 0.5955856], train_acc=[0.789  0.7809]
72/100: train_loss=[0.56056494 0.5866535 ], train_acc=[0.7968  0.78335]
74/100: train_loss=[0.5648121 0.5945818], train_acc=[0.7946  0.78185]
76/100: train_loss=[0.55978125 0.5746248 ], train_acc=[0.79695 0.78895]
78/100: train_loss=[0.5465422  0.56516457], train_acc=[0.80005 0.79005]
80/100: train_loss=[0.54971313 0.5767815 ], train_acc=[0.8014  0.78995]
82/100: train_loss=[0.5470536 0.5744941], train_acc=[0.80195 0.7884 ]
84/100: train_loss=[0.56296945 0.5659286 ], train_acc=[0.79665 0.7925 ]
86/100: train_loss=[0.5586657 0.56231  ], train_acc=[0.79595 0.79305]
88/100: train_loss=[0.5880183 0.5963345], train_acc=[0.7841 0.7796]
90/100: train_loss=[0.5423709 0.5740085], train_acc=[0.7997 0.789 ]
92/100: train_loss=[0.5484191  0.55848205], train_acc=[0.7974  0.79345]
94/100: train_loss=[0.5373292 0.5727475], train_acc=[0.8021  0.78865]
96/100: train_loss=[0.53923386 0.5668134 ], train_acc=[0.80415 0.79245]
98/100: train_loss=[0.5402906 0.5645489], train_acc=[0.8036  0.79185]
100/100: train_loss=[0.5477707 0.554381 ], train_acc=[0.795   0.79755]
**** Time taken for fashion_1 = 3825.262870311737
loading dataset fashion
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.1282116 1.1917796], train_acc=[0.57745 0.56385]
2/100: train_loss=[0.9990341 1.0589044], train_acc=[0.62815 0.6138 ]
4/100: train_loss=[0.8977679  0.94405967], train_acc=[0.66365 0.6599 ]
6/100: train_loss=[0.8436824 0.8831257], train_acc=[0.68265 0.6777 ]
8/100: train_loss=[0.79468024 0.8320435 ], train_acc=[0.7032  0.69405]
10/100: train_loss=[0.77819216 0.7824308 ], train_acc=[0.70695 0.7156 ]
12/100: train_loss=[0.7406388  0.75859606], train_acc=[0.72165 0.7199 ]
14/100: train_loss=[0.7333687 0.7486194], train_acc=[0.7257  0.73025]
16/100: train_loss=[0.712551  0.7336525], train_acc=[0.7356  0.72665]
18/100: train_loss=[0.68092215 0.70462304], train_acc=[0.74715 0.7388 ]
20/100: train_loss=[0.6733024 0.6973414], train_acc=[0.74965 0.7434 ]
22/100: train_loss=[0.6714131 0.6702872], train_acc=[0.752  0.7531]
24/100: train_loss=[0.66105866 0.67821926], train_acc=[0.75295 0.7569 ]
26/100: train_loss=[0.6444511 0.6594697], train_acc=[0.7616 0.756 ]
28/100: train_loss=[0.63018185 0.6399825 ], train_acc=[0.7693 0.7663]
30/100: train_loss=[0.684252  0.6786516], train_acc=[0.74785 0.7473 ]
32/100: train_loss=[0.62404025 0.6361459 ], train_acc=[0.7705 0.761 ]
34/100: train_loss=[0.64829284 0.6227899 ], train_acc=[0.7649  0.76775]
36/100: train_loss=[0.62960196 0.6426058 ], train_acc=[0.76615 0.75775]
38/100: train_loss=[0.62034667 0.6164481 ], train_acc=[0.77365 0.77225]
40/100: train_loss=[0.5995453  0.62004405], train_acc=[0.77885 0.77455]
42/100: train_loss=[0.61320406 0.6048861 ], train_acc=[0.7773  0.77435]
44/100: train_loss=[0.5939083  0.59157026], train_acc=[0.7788 0.782 ]
46/100: train_loss=[0.5883598 0.5848915], train_acc=[0.78245 0.78495]
48/100: train_loss=[0.5859631 0.5961581], train_acc=[0.7835 0.7828]
50/100: train_loss=[0.59592867 0.59871656], train_acc=[0.78125 0.7763 ]
52/100: train_loss=[0.5884054 0.5757415], train_acc=[0.7825  0.78825]
54/100: train_loss=[0.5868855 0.5979597], train_acc=[0.77965 0.78005]
56/100: train_loss=[0.61170727 0.61725503], train_acc=[0.7719  0.76915]
58/100: train_loss=[0.5790045 0.5880526], train_acc=[0.78575 0.7892 ]
60/100: train_loss=[0.61418164 0.58261806], train_acc=[0.7746  0.78565]
62/100: train_loss=[0.5667845  0.56803066], train_acc=[0.7918  0.78985]
64/100: train_loss=[0.5726221  0.56710386], train_acc=[0.7874 0.7932]
66/100: train_loss=[0.56594765 0.55472827], train_acc=[0.79115 0.7986 ]
68/100: train_loss=[0.5610833 0.5639344], train_acc=[0.79185 0.78955]
70/100: train_loss=[0.5743433  0.55645615], train_acc=[0.7874  0.79705]
72/100: train_loss=[0.5640026  0.56937516], train_acc=[0.78725 0.7923 ]
74/100: train_loss=[0.55559075 0.55580807], train_acc=[0.794  0.7946]
76/100: train_loss=[0.5531633  0.55937165], train_acc=[0.79435 0.7969 ]
78/100: train_loss=[0.54693586 0.5463913 ], train_acc=[0.7962 0.7998]
80/100: train_loss=[0.55128926 0.59071666], train_acc=[0.79555 0.7853 ]
82/100: train_loss=[0.5521394  0.56094694], train_acc=[0.7929  0.79615]
84/100: train_loss=[0.54249567 0.54460406], train_acc=[0.7981 0.8029]
86/100: train_loss=[0.5418559 0.5456105], train_acc=[0.79935 0.80185]
88/100: train_loss=[0.5396401 0.5475151], train_acc=[0.80035 0.8002 ]
90/100: train_loss=[0.5401649 0.5410064], train_acc=[0.79835 0.8008 ]
92/100: train_loss=[0.5458684 0.5603286], train_acc=[0.79585 0.79505]
94/100: train_loss=[0.5486976  0.53584707], train_acc=[0.79555 0.80665]
96/100: train_loss=[0.53582555 0.55046815], train_acc=[0.7985  0.79565]
98/100: train_loss=[0.55518085 0.55871296], train_acc=[0.79435 0.79395]
100/100: train_loss=[0.5357511  0.54235274], train_acc=[0.80095 0.7989 ]
**** Time taken for fashion_2 = 3446.5599417686462
loading dataset fashion
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.0908024 1.1393838], train_acc=[0.59685 0.5785 ]
2/100: train_loss=[0.9591436 1.0080607], train_acc=[0.6473 0.6252]
4/100: train_loss=[0.8507001 0.8836135], train_acc=[0.68085 0.67525]
6/100: train_loss=[0.85583913 0.8288007 ], train_acc=[0.679   0.69505]
8/100: train_loss=[0.7538561  0.78285295], train_acc=[0.7195 0.7099]
10/100: train_loss=[0.73462665 0.7638466 ], train_acc=[0.72685 0.72   ]
12/100: train_loss=[0.7132971 0.7343158], train_acc=[0.7362  0.72835]
14/100: train_loss=[0.6899197 0.7114126], train_acc=[0.7453  0.73505]
16/100: train_loss=[0.67046577 0.7021251 ], train_acc=[0.7516  0.73965]
18/100: train_loss=[0.6728763  0.69415724], train_acc=[0.7503  0.74365]
20/100: train_loss=[0.6601411  0.69671863], train_acc=[0.75345 0.7437 ]
22/100: train_loss=[0.6495207 0.671977 ], train_acc=[0.7611  0.75415]
24/100: train_loss=[0.63647926 0.64943755], train_acc=[0.76375 0.7611 ]
26/100: train_loss=[0.6252949 0.6694742], train_acc=[0.76865 0.74795]
28/100: train_loss=[0.6152759 0.6279795], train_acc=[0.77155 0.76845]
30/100: train_loss=[0.6354034 0.6310426], train_acc=[0.76475 0.7693 ]
32/100: train_loss=[0.59576255 0.622739  ], train_acc=[0.7778  0.77265]
34/100: train_loss=[0.5925844 0.6177163], train_acc=[0.78025 0.7718 ]
36/100: train_loss=[0.6447889 0.6297048], train_acc=[0.76195 0.76835]
38/100: train_loss=[0.58338964 0.6159311 ], train_acc=[0.783  0.7705]
40/100: train_loss=[0.58101404 0.6054211 ], train_acc=[0.78505 0.77655]
42/100: train_loss=[0.5875986 0.6102405], train_acc=[0.7849 0.7789]
44/100: train_loss=[0.5757303 0.5844606], train_acc=[0.7849 0.7868]
46/100: train_loss=[0.5790926 0.5905942], train_acc=[0.7837 0.7853]
48/100: train_loss=[0.5878934 0.5969295], train_acc=[0.7834  0.77905]
50/100: train_loss=[0.56209815 0.59057945], train_acc=[0.7885  0.78945]
52/100: train_loss=[0.5686748 0.5756711], train_acc=[0.78875 0.7928 ]
54/100: train_loss=[0.5628782 0.5744716], train_acc=[0.79125 0.79035]
56/100: train_loss=[0.55818236 0.581096  ], train_acc=[0.7922 0.7892]
58/100: train_loss=[0.56318134 0.5716705 ], train_acc=[0.7872 0.793 ]
60/100: train_loss=[0.56283563 0.57728195], train_acc=[0.79325 0.7903 ]
62/100: train_loss=[0.5472967 0.5694169], train_acc=[0.79715 0.794  ]
64/100: train_loss=[0.6049188  0.59357554], train_acc=[0.78065 0.78385]
66/100: train_loss=[0.5436889 0.5639446], train_acc=[0.7975 0.7963]
68/100: train_loss=[0.5410983 0.5577105], train_acc=[0.79755 0.79645]
70/100: train_loss=[0.55221325 0.5737792 ], train_acc=[0.79435 0.79405]
72/100: train_loss=[0.5458608  0.56032836], train_acc=[0.80035 0.79555]
74/100: train_loss=[0.5490255  0.55584073], train_acc=[0.7963 0.797 ]
76/100: train_loss=[0.5470052 0.5817446], train_acc=[0.7941  0.78825]
78/100: train_loss=[0.5413167  0.55657834], train_acc=[0.797   0.79585]
80/100: train_loss=[0.544955   0.56685156], train_acc=[0.7979 0.7924]
82/100: train_loss=[0.53315794 0.5486956 ], train_acc=[0.79985 0.7994 ]
84/100: train_loss=[0.5559772  0.57420945], train_acc=[0.79565 0.7966 ]
86/100: train_loss=[0.5359845  0.55520433], train_acc=[0.8014 0.7987]
88/100: train_loss=[0.5403348 0.5578437], train_acc=[0.79625 0.7999 ]
90/100: train_loss=[0.54524535 0.5438826 ], train_acc=[0.7968  0.80145]
92/100: train_loss=[0.53755295 0.5467232 ], train_acc=[0.80005 0.8027 ]
94/100: train_loss=[0.52628195 0.5474572 ], train_acc=[0.80455 0.7982 ]
96/100: train_loss=[0.5514622 0.5604654], train_acc=[0.79785 0.79435]
98/100: train_loss=[0.5325358  0.54574573], train_acc=[0.80435 0.80585]
100/100: train_loss=[0.53127867 0.55005   ], train_acc=[0.8029  0.80115]
**** Time taken for fashion_3 = 3384.0448253154755
loading dataset fashion
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.0483983 1.1571909], train_acc=[0.609  0.5741]
2/100: train_loss=[0.92977464 1.014632  ], train_acc=[0.6525 0.6278]
4/100: train_loss=[0.83776337 0.90372133], train_acc=[0.6878  0.66995]
6/100: train_loss=[0.79304206 0.8649496 ], train_acc=[0.6988 0.6823]
8/100: train_loss=[0.78863984 0.8073102 ], train_acc=[0.6986 0.7002]
10/100: train_loss=[0.7729675 0.813581 ], train_acc=[0.70665 0.701  ]
12/100: train_loss=[0.74197614 0.756409  ], train_acc=[0.72065 0.72305]
14/100: train_loss=[0.6906858 0.7434564], train_acc=[0.74485 0.7305 ]
16/100: train_loss=[0.6770924 0.7372204], train_acc=[0.7442  0.73595]
18/100: train_loss=[0.6757606 0.7286541], train_acc=[0.74845 0.73505]
20/100: train_loss=[0.65811765 0.6639249 ], train_acc=[0.75345 0.7596 ]
22/100: train_loss=[0.6531616 0.6591071], train_acc=[0.75705 0.7634 ]
24/100: train_loss=[0.63513076 0.6582378 ], train_acc=[0.7611 0.7628]
26/100: train_loss=[0.62467366 0.64732903], train_acc=[0.7658 0.7666]
28/100: train_loss=[0.6209762  0.63459164], train_acc=[0.7676 0.7724]
30/100: train_loss=[0.6082541 0.6623855], train_acc=[0.77285 0.7606 ]
32/100: train_loss=[0.6165887  0.64260614], train_acc=[0.774  0.7698]
34/100: train_loss=[0.59914494 0.6204139 ], train_acc=[0.776  0.7708]
36/100: train_loss=[0.5936503  0.61197793], train_acc=[0.7785  0.77835]
38/100: train_loss=[0.57464355 0.61552066], train_acc=[0.789   0.77535]
40/100: train_loss=[0.6036459  0.61798006], train_acc=[0.7796 0.7751]
42/100: train_loss=[0.5963546 0.6057273], train_acc=[0.7831 0.7802]
44/100: train_loss=[0.56917363 0.59320736], train_acc=[0.7919  0.78395]
46/100: train_loss=[0.5690107  0.60421324], train_acc=[0.789  0.7802]
48/100: train_loss=[0.5625982 0.5949251], train_acc=[0.7934 0.78  ]
50/100: train_loss=[0.56444585 0.646065  ], train_acc=[0.79095 0.77305]
52/100: train_loss=[0.56819975 0.59370524], train_acc=[0.78965 0.7858 ]
54/100: train_loss=[0.5610794 0.5943292], train_acc=[0.79125 0.78425]
56/100: train_loss=[0.5526356 0.5776603], train_acc=[0.79525 0.78935]
58/100: train_loss=[0.5523221 0.5749396], train_acc=[0.79665 0.79275]
60/100: train_loss=[0.55600035 0.5747044 ], train_acc=[0.7949 0.7908]
62/100: train_loss=[0.54939514 0.57306415], train_acc=[0.79725 0.79265]
64/100: train_loss=[0.55484605 0.57977414], train_acc=[0.79265 0.7862 ]
66/100: train_loss=[0.5479964 0.5655125], train_acc=[0.79945 0.7948 ]
68/100: train_loss=[0.54280657 0.5668784 ], train_acc=[0.799  0.7922]
70/100: train_loss=[0.54191524 0.5625967 ], train_acc=[0.8009 0.7968]
72/100: train_loss=[0.54845667 0.5711246 ], train_acc=[0.7973  0.79285]
74/100: train_loss=[0.5393715  0.56088376], train_acc=[0.80175 0.79415]
76/100: train_loss=[0.5511881 0.5659939], train_acc=[0.7969 0.7973]
78/100: train_loss=[0.5499464 0.5638411], train_acc=[0.7992 0.797 ]
80/100: train_loss=[0.54112005 0.5691928 ], train_acc=[0.80095 0.79265]
82/100: train_loss=[0.54597753 0.5718008 ], train_acc=[0.80015 0.7907 ]
84/100: train_loss=[0.53683835 0.5584766 ], train_acc=[0.80475 0.7992 ]
86/100: train_loss=[0.54333574 0.5657946 ], train_acc=[0.80135 0.79285]
88/100: train_loss=[0.53490084 0.56439114], train_acc=[0.8049  0.79515]
90/100: train_loss=[0.5522309 0.570011 ], train_acc=[0.798   0.79265]
92/100: train_loss=[0.5400398 0.551386 ], train_acc=[0.80385 0.79985]
94/100: train_loss=[0.5386593 0.5599964], train_acc=[0.80545 0.79825]
96/100: train_loss=[0.52932745 0.5533184 ], train_acc=[0.80565 0.8005 ]
98/100: train_loss=[0.5341298  0.55236024], train_acc=[0.8065 0.8004]
100/100: train_loss=[0.53528327 0.5584569 ], train_acc=[0.80525 0.8    ]
**** Time taken for fashion_4 = 2512.863750934601
**** Time taken for fashion = 16992.995247125626
loading dataset fashion_and_mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.3282639 1.0205646], train_acc=[0.54365 0.62775]
2/100: train_loss=[1.0680203  0.88485056], train_acc=[0.6342 0.681 ]
4/100: train_loss=[0.8682055 0.8180173], train_acc=[0.7036 0.7016]
6/100: train_loss=[0.7552173 0.7636688], train_acc=[0.7487 0.7219]
8/100: train_loss=[0.6748133 0.7236202], train_acc=[0.77195 0.73435]
10/100: train_loss=[0.6359434 0.6973643], train_acc=[0.79135 0.74365]
12/100: train_loss=[0.5744995  0.67540306], train_acc=[0.81065 0.75115]
14/100: train_loss=[0.5792146 0.6727582], train_acc=[0.81105 0.7497 ]
16/100: train_loss=[0.5282957  0.65825033], train_acc=[0.828  0.7528]
18/100: train_loss=[0.508506   0.63434356], train_acc=[0.83405 0.7653 ]
20/100: train_loss=[0.4684245 0.6366572], train_acc=[0.8491 0.7642]
22/100: train_loss=[0.46260902 0.61689115], train_acc=[0.8494  0.77055]
24/100: train_loss=[0.45634887 0.6273369 ], train_acc=[0.8526  0.76745]
26/100: train_loss=[0.44351646 0.6086845 ], train_acc=[0.85665 0.77325]
28/100: train_loss=[0.42675897 0.60311073], train_acc=[0.8633  0.77415]
30/100: train_loss=[0.4309826 0.60555  ], train_acc=[0.86135 0.7748 ]
32/100: train_loss=[0.41747025 0.6026216 ], train_acc=[0.86595 0.7766 ]
34/100: train_loss=[0.42628393 0.58691233], train_acc=[0.8647  0.78145]
36/100: train_loss=[0.41023588 0.58579427], train_acc=[0.86865 0.7833 ]
38/100: train_loss=[0.38595057 0.5791211 ], train_acc=[0.87725 0.7863 ]
40/100: train_loss=[0.4109661 0.586995 ], train_acc=[0.86735 0.7816 ]
42/100: train_loss=[0.38061816 0.6070574 ], train_acc=[0.87875 0.76885]
44/100: train_loss=[0.37227556 0.58002824], train_acc=[0.88035 0.78385]
46/100: train_loss=[0.36404145 0.56747   ], train_acc=[0.8834  0.78895]
48/100: train_loss=[0.36717442 0.56518596], train_acc=[0.8822  0.79175]
50/100: train_loss=[0.3615894  0.56094354], train_acc=[0.8858 0.7946]
52/100: train_loss=[0.34518826 0.5611119 ], train_acc=[0.89105 0.79215]
54/100: train_loss=[0.35183507 0.56361794], train_acc=[0.8881 0.7931]
56/100: train_loss=[0.35316172 0.5565655 ], train_acc=[0.8849 0.7961]
58/100: train_loss=[0.3726789 0.5697094], train_acc=[0.87855 0.7932 ]
60/100: train_loss=[0.3466226 0.5645121], train_acc=[0.8905 0.7937]
62/100: train_loss=[0.33007705 0.5457196 ], train_acc=[0.8967  0.80065]
64/100: train_loss=[0.3230891 0.5437861], train_acc=[0.89885 0.80095]
66/100: train_loss=[0.34473416 0.5989079 ], train_acc=[0.8892 0.7829]
68/100: train_loss=[0.33074543 0.54811084], train_acc=[0.89415 0.7974 ]
70/100: train_loss=[0.32632723 0.55404276], train_acc=[0.89685 0.7965 ]
72/100: train_loss=[0.30721104 0.5440112 ], train_acc=[0.9028  0.80025]
74/100: train_loss=[0.3232189 0.5408196], train_acc=[0.89745 0.8017 ]
76/100: train_loss=[0.30519375 0.5493035 ], train_acc=[0.90445 0.7986 ]
78/100: train_loss=[0.3506444  0.55925727], train_acc=[0.8875 0.8   ]
80/100: train_loss=[0.3010161  0.54051805], train_acc=[0.90425 0.80255]
82/100: train_loss=[0.30985162 0.5297224 ], train_acc=[0.89975 0.804  ]
84/100: train_loss=[0.33380616 0.54350555], train_acc=[0.8942 0.8009]
86/100: train_loss=[0.28741032 0.58292985], train_acc=[0.9064 0.7826]
88/100: train_loss=[0.31975433 0.5294558 ], train_acc=[0.8976  0.80565]
90/100: train_loss=[0.31773368 0.5431015 ], train_acc=[0.89555 0.80305]
92/100: train_loss=[0.31631088 0.53206575], train_acc=[0.8979 0.8056]
94/100: train_loss=[0.32393396 0.5268379 ], train_acc=[0.8955  0.80765]
96/100: train_loss=[0.31137246 0.5220245 ], train_acc=[0.89995 0.8095 ]
98/100: train_loss=[0.29031837 0.52061677], train_acc=[0.90405 0.81035]
100/100: train_loss=[0.2716611  0.51782805], train_acc=[0.91225 0.8124 ]
**** Time taken for fashion_and_mnist_0 = 2146.451067686081
loading dataset fashion_and_mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.3675343 1.0500466], train_acc=[0.53515 0.604  ]
2/100: train_loss=[1.0739695 0.9583852], train_acc=[0.6384  0.63705]
4/100: train_loss=[0.7855515 0.8254903], train_acc=[0.7373 0.691 ]
6/100: train_loss=[0.6649859 0.7690338], train_acc=[0.78255 0.70965]
8/100: train_loss=[0.5870237 0.7565323], train_acc=[0.80375 0.7129 ]
10/100: train_loss=[0.53726447 0.70039606], train_acc=[0.82345 0.74005]
12/100: train_loss=[0.50152606 0.67147535], train_acc=[0.8327  0.74575]
14/100: train_loss=[0.49661285 0.6542328 ], train_acc=[0.83685 0.75665]
16/100: train_loss=[0.43126678 0.6725404 ], train_acc=[0.8599 0.7465]
18/100: train_loss=[0.41731215 0.6276101 ], train_acc=[0.86545 0.7642 ]
20/100: train_loss=[0.39349523 0.62008256], train_acc=[0.87075 0.77135]
22/100: train_loss=[0.39603725 0.6335368 ], train_acc=[0.86855 0.76295]
24/100: train_loss=[0.3893211  0.59661937], train_acc=[0.8716 0.7784]
26/100: train_loss=[0.358219   0.59243476], train_acc=[0.88455 0.7785 ]
28/100: train_loss=[0.35592052 0.59096605], train_acc=[0.88335 0.78075]
30/100: train_loss=[0.33792    0.57813585], train_acc=[0.88985 0.7874 ]
32/100: train_loss=[0.33879876 0.6078239 ], train_acc=[0.88795 0.77175]
34/100: train_loss=[0.35779485 0.56628263], train_acc=[0.88415 0.7889 ]
36/100: train_loss=[0.33765292 0.59972364], train_acc=[0.88985 0.77645]
38/100: train_loss=[0.3237546  0.57362014], train_acc=[0.8963  0.79185]
40/100: train_loss=[0.3282609  0.56038177], train_acc=[0.8938 0.7957]
42/100: train_loss=[0.315571  0.5503629], train_acc=[0.89775 0.7971 ]
44/100: train_loss=[0.34178987 0.59634817], train_acc=[0.88945 0.774  ]
46/100: train_loss=[0.31091648 0.5384142 ], train_acc=[0.9011  0.80285]
48/100: train_loss=[0.3203781  0.52769583], train_acc=[0.8977 0.8054]
50/100: train_loss=[0.3345646  0.53452015], train_acc=[0.8906 0.8056]
52/100: train_loss=[0.28927952 0.58390826], train_acc=[0.90805 0.77515]
54/100: train_loss=[0.29924867 0.5250252 ], train_acc=[0.90385 0.80965]
56/100: train_loss=[0.2953059  0.52828646], train_acc=[0.90375 0.80575]
58/100: train_loss=[0.29497203 0.53446615], train_acc=[0.90515 0.80095]
60/100: train_loss=[0.28305975 0.5396555 ], train_acc=[0.91015 0.79865]
62/100: train_loss=[0.27942237 0.5194841 ], train_acc=[0.91155 0.80935]
64/100: train_loss=[0.26967996 0.5132483 ], train_acc=[0.91405 0.81345]
66/100: train_loss=[0.27489188 0.5038032 ], train_acc=[0.9127 0.8164]
68/100: train_loss=[0.26599726 0.50721955], train_acc=[0.9153 0.8141]
70/100: train_loss=[0.27683538 0.50834125], train_acc=[0.91185 0.81245]
72/100: train_loss=[0.26121557 0.5106326 ], train_acc=[0.91755 0.8101 ]
74/100: train_loss=[0.2671711  0.49834058], train_acc=[0.9133 0.8186]
76/100: train_loss=[0.27659863 0.5172269 ], train_acc=[0.9118  0.81405]
78/100: train_loss=[0.27163962 0.525185  ], train_acc=[0.9136 0.8071]
80/100: train_loss=[0.26157063 0.4974134 ], train_acc=[0.91515 0.81635]
82/100: train_loss=[0.25690722 0.49968272], train_acc=[0.91665 0.81385]
84/100: train_loss=[0.26326916 0.50839967], train_acc=[0.9147 0.809 ]
86/100: train_loss=[0.25192744 0.48468208], train_acc=[0.9199  0.81915]
88/100: train_loss=[0.29469416 0.5003194 ], train_acc=[0.9058 0.816 ]
90/100: train_loss=[0.26050895 0.4847366 ], train_acc=[0.9179 0.8219]
92/100: train_loss=[0.24685428 0.49083218], train_acc=[0.9222  0.81995]
94/100: train_loss=[0.26995274 0.5000212 ], train_acc=[0.91335 0.8128 ]
96/100: train_loss=[0.27186048 0.4801184 ], train_acc=[0.9148  0.82565]
98/100: train_loss=[0.23945329 0.4769702 ], train_acc=[0.9232  0.82695]
100/100: train_loss=[0.2583834  0.53047335], train_acc=[0.9177 0.8024]
**** Time taken for fashion_and_mnist_1 = 2141.4457111358643
loading dataset fashion_and_mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.3845868 1.0450916], train_acc=[0.52545 0.61605]
2/100: train_loss=[1.1524291 0.9251841], train_acc=[0.61055 0.66135]
4/100: train_loss=[0.93285227 0.8123254 ], train_acc=[0.68965 0.69935]
6/100: train_loss=[0.8248436  0.76996547], train_acc=[0.7213 0.7109]
8/100: train_loss=[0.74589366 0.7659632 ], train_acc=[0.7532 0.7164]
10/100: train_loss=[0.6753744  0.73240817], train_acc=[0.7781  0.72755]
12/100: train_loss=[0.63967586 0.7025592 ], train_acc=[0.7899 0.7416]
14/100: train_loss=[0.60534793 0.7137284 ], train_acc=[0.80395 0.73235]
16/100: train_loss=[0.6844357 0.6957695], train_acc=[0.76935 0.73405]
18/100: train_loss=[0.5607823 0.6742976], train_acc=[0.8185  0.74815]
20/100: train_loss=[0.53258634 0.663549  ], train_acc=[0.83125 0.75615]
22/100: train_loss=[0.51818    0.64046043], train_acc=[0.8322 0.7635]
24/100: train_loss=[0.49809545 0.65120167], train_acc=[0.8387  0.75665]
26/100: train_loss=[0.49965206 0.6402151 ], train_acc=[0.83675 0.76215]
28/100: train_loss=[0.45137444 0.631569  ], train_acc=[0.85585 0.7606 ]
30/100: train_loss=[0.44840163 0.6137677 ], train_acc=[0.85545 0.7729 ]
32/100: train_loss=[0.44246495 0.61579067], train_acc=[0.85715 0.7736 ]
34/100: train_loss=[0.49092624 0.60295427], train_acc=[0.84085 0.77595]
36/100: train_loss=[0.42170084 0.61020225], train_acc=[0.86645 0.77105]
38/100: train_loss=[0.41624475 0.5978749 ], train_acc=[0.8683  0.78065]
40/100: train_loss=[0.40659854 0.5895014 ], train_acc=[0.8715  0.78345]
42/100: train_loss=[0.39950567 0.5880106 ], train_acc=[0.87245 0.77905]
44/100: train_loss=[0.4043042  0.57937086], train_acc=[0.87225 0.7852 ]
46/100: train_loss=[0.39324552 0.576664  ], train_acc=[0.8745 0.7871]
48/100: train_loss=[0.42519635 0.6001996 ], train_acc=[0.86075 0.78055]
50/100: train_loss=[0.36872315 0.564137  ], train_acc=[0.8824 0.7934]
52/100: train_loss=[0.376991  0.5652841], train_acc=[0.8795 0.7917]
54/100: train_loss=[0.36805737 0.56076294], train_acc=[0.88165 0.7928 ]
56/100: train_loss=[0.3610082  0.55860275], train_acc=[0.8842 0.7943]
58/100: train_loss=[0.34499502 0.55729103], train_acc=[0.8897 0.7957]
60/100: train_loss=[0.3676515 0.5649762], train_acc=[0.88385 0.79415]
62/100: train_loss=[0.3598302 0.550485 ], train_acc=[0.88635 0.7961 ]
64/100: train_loss=[0.34582582 0.5512707 ], train_acc=[0.88965 0.7992 ]
66/100: train_loss=[0.35464665 0.54174507], train_acc=[0.88485 0.80175]
68/100: train_loss=[0.3313026 0.5458305], train_acc=[0.8954 0.7994]
70/100: train_loss=[0.35127738 0.5709972 ], train_acc=[0.88735 0.78635]
72/100: train_loss=[0.3213889  0.53885865], train_acc=[0.89695 0.8001 ]
74/100: train_loss=[0.31836572 0.54841626], train_acc=[0.89725 0.798  ]
76/100: train_loss=[0.34632277 0.5385871 ], train_acc=[0.8871  0.80215]
78/100: train_loss=[0.33100313 0.57250386], train_acc=[0.8941  0.78735]
80/100: train_loss=[0.33605194 0.6145904 ], train_acc=[0.89105 0.76755]
82/100: train_loss=[0.33506346 0.5344731 ], train_acc=[0.89105 0.8015 ]
84/100: train_loss=[0.31325358 0.53699255], train_acc=[0.89855 0.80115]
86/100: train_loss=[0.3205251  0.52593195], train_acc=[0.89865 0.8068 ]
88/100: train_loss=[0.30364364 0.532773  ], train_acc=[0.90255 0.80435]
90/100: train_loss=[0.30897328 0.5223073 ], train_acc=[0.90125 0.8068 ]
92/100: train_loss=[0.3308699  0.54926497], train_acc=[0.8937  0.79945]
94/100: train_loss=[0.32042786 0.5250442 ], train_acc=[0.89595 0.8037 ]
96/100: train_loss=[0.30641368 0.5195044 ], train_acc=[0.90115 0.80705]
98/100: train_loss=[0.30592608 0.51924384], train_acc=[0.90165 0.81205]
100/100: train_loss=[0.32003495 0.52194005], train_acc=[0.8961  0.80535]
**** Time taken for fashion_and_mnist_2 = 1900.5548706054688
loading dataset fashion_and_mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.4605565 1.1207125], train_acc=[0.51405 0.5895 ]
2/100: train_loss=[1.1057464 0.9451978], train_acc=[0.62825 0.6577 ]
4/100: train_loss=[0.8375543 0.8406693], train_acc=[0.7217  0.69045]
6/100: train_loss=[0.74928665 0.8277843 ], train_acc=[0.7526 0.6977]
8/100: train_loss=[0.6385101  0.76666194], train_acc=[0.7915  0.71325]
10/100: train_loss=[0.6006319  0.74332315], train_acc=[0.8029 0.7264]
12/100: train_loss=[0.5667011 0.7200485], train_acc=[0.8159 0.7274]
14/100: train_loss=[0.52613556 0.6918279 ], train_acc=[0.82925 0.7467 ]
16/100: train_loss=[0.5461827 0.7471794], train_acc=[0.81845 0.7223 ]
18/100: train_loss=[0.47827497 0.66275835], train_acc=[0.84305 0.7551 ]
20/100: train_loss=[0.45937002 0.655522  ], train_acc=[0.85075 0.75815]
22/100: train_loss=[0.44205153 0.6545275 ], train_acc=[0.8553  0.76305]
24/100: train_loss=[0.43030405 0.6416633 ], train_acc=[0.8602  0.76645]
26/100: train_loss=[0.40924895 0.6244007 ], train_acc=[0.86725 0.7687 ]
28/100: train_loss=[0.4295343 0.621795 ], train_acc=[0.85985 0.76955]
30/100: train_loss=[0.39978874 0.61431617], train_acc=[0.8717 0.7712]
32/100: train_loss=[0.3768928 0.6235549], train_acc=[0.8778 0.7675]
34/100: train_loss=[0.3892281  0.60670775], train_acc=[0.8739 0.7757]
36/100: train_loss=[0.38146105 0.6035818 ], train_acc=[0.87385 0.7755 ]
38/100: train_loss=[0.3880787  0.59452647], train_acc=[0.87525 0.77785]
40/100: train_loss=[0.37886223 0.59083706], train_acc=[0.87715 0.7782 ]
42/100: train_loss=[0.3702341  0.59418166], train_acc=[0.87775 0.77885]
44/100: train_loss=[0.3579513  0.58370805], train_acc=[0.8839 0.7839]
46/100: train_loss=[0.38113987 0.57302666], train_acc=[0.8768 0.7854]
48/100: train_loss=[0.33314383 0.567156  ], train_acc=[0.8915  0.79025]
50/100: train_loss=[0.33121789 0.56971014], train_acc=[0.8914 0.7872]
52/100: train_loss=[0.31740585 0.56019455], train_acc=[0.89875 0.7937 ]
54/100: train_loss=[0.3332061  0.56286424], train_acc=[0.89225 0.79125]
56/100: train_loss=[0.32530588 0.5553254 ], train_acc=[0.8933  0.79435]
58/100: train_loss=[0.32430616 0.56022125], train_acc=[0.89465 0.7905 ]
60/100: train_loss=[0.3027036 0.619565 ], train_acc=[0.902   0.76885]
62/100: train_loss=[0.30489016 0.56148374], train_acc=[0.90145 0.7915 ]
64/100: train_loss=[0.28996524 0.55436194], train_acc=[0.90565 0.79535]
66/100: train_loss=[0.31658962 0.5468684 ], train_acc=[0.8976  0.79405]
68/100: train_loss=[0.29397196 0.5787541 ], train_acc=[0.9059 0.7779]
70/100: train_loss=[0.3021387 0.5392717], train_acc=[0.90085 0.7972 ]
72/100: train_loss=[0.28137147 0.5398298 ], train_acc=[0.9089  0.79755]
74/100: train_loss=[0.27594396 0.5302079 ], train_acc=[0.91295 0.8012 ]
76/100: train_loss=[0.2797347 0.5404876], train_acc=[0.9101  0.79745]
78/100: train_loss=[0.27423537 0.5329163 ], train_acc=[0.9124  0.79985]
80/100: train_loss=[0.2829072 0.5332065], train_acc=[0.90885 0.80035]
82/100: train_loss=[0.3118675  0.53967893], train_acc=[0.89735 0.79695]
84/100: train_loss=[0.2944249  0.53298706], train_acc=[0.9039  0.80185]
86/100: train_loss=[0.3350705  0.64538336], train_acc=[0.8901 0.7657]
88/100: train_loss=[0.28095073 0.5424344 ], train_acc=[0.91125 0.8006 ]
90/100: train_loss=[0.27154705 0.54259384], train_acc=[0.9111 0.8001]
92/100: train_loss=[0.2797833  0.54483366], train_acc=[0.9113  0.79275]
94/100: train_loss=[0.2879952 0.5471102], train_acc=[0.90645 0.7998 ]
96/100: train_loss=[0.30782703 0.5286411 ], train_acc=[0.8995 0.8024]
98/100: train_loss=[0.3061637 0.527618 ], train_acc=[0.8996  0.80615]
100/100: train_loss=[0.26995346 0.5211504 ], train_acc=[0.91305 0.80705]
**** Time taken for fashion_and_mnist_3 = 1524.9401981830597
loading dataset fashion_and_mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.3475788 1.0699254], train_acc=[0.538   0.60315]
2/100: train_loss=[1.0779259 0.9084625], train_acc=[0.63185 0.66775]
4/100: train_loss=[0.84702843 0.79991025], train_acc=[0.71425 0.70695]
6/100: train_loss=[0.69271845 0.75523186], train_acc=[0.76755 0.7192 ]
8/100: train_loss=[0.6303348  0.71874726], train_acc=[0.7918 0.7287]
10/100: train_loss=[0.57787716 0.6967273 ], train_acc=[0.81105 0.7372 ]
12/100: train_loss=[0.5496181  0.66565496], train_acc=[0.8211  0.75185]
14/100: train_loss=[0.5234213  0.68138474], train_acc=[0.8329 0.7402]
16/100: train_loss=[0.47055778 0.6474153 ], train_acc=[0.84875 0.7586 ]
18/100: train_loss=[0.47389483 0.63344646], train_acc=[0.84785 0.7666 ]
20/100: train_loss=[0.4533013 0.6252346], train_acc=[0.85395 0.76735]
22/100: train_loss=[0.41410404 0.6123198 ], train_acc=[0.86535 0.77215]
24/100: train_loss=[0.42216396 0.5985024 ], train_acc=[0.86135 0.77625]
26/100: train_loss=[0.39881063 0.60967344], train_acc=[0.8721 0.7701]
28/100: train_loss=[0.45334598 0.58791167], train_acc=[0.85385 0.78275]
30/100: train_loss=[0.3773262  0.58224666], train_acc=[0.87735 0.783  ]
32/100: train_loss=[0.36472562 0.57084167], train_acc=[0.88365 0.78835]
34/100: train_loss=[0.35247824 0.57420564], train_acc=[0.8872  0.78745]
36/100: train_loss=[0.3529468 0.577615 ], train_acc=[0.88795 0.7799 ]
38/100: train_loss=[0.34855402 0.54771787], train_acc=[0.8864  0.79605]
40/100: train_loss=[0.32821864 0.5420852 ], train_acc=[0.8969  0.80035]
42/100: train_loss=[0.3222017 0.5487374], train_acc=[0.8975 0.7981]
44/100: train_loss=[0.323846  0.5398121], train_acc=[0.8971  0.79925]
46/100: train_loss=[0.3235715 0.5395618], train_acc=[0.89675 0.79965]
48/100: train_loss=[0.31049326 0.5384159 ], train_acc=[0.90105 0.7985 ]
50/100: train_loss=[0.30316165 0.5254316 ], train_acc=[0.9043 0.8037]
52/100: train_loss=[0.3140659 0.5257461], train_acc=[0.8976  0.80455]
54/100: train_loss=[0.30482015 0.5300618 ], train_acc=[0.90255 0.8014 ]
56/100: train_loss=[0.3018907  0.52232677], train_acc=[0.9043  0.80635]
58/100: train_loss=[0.34484693 0.54101557], train_acc=[0.8883 0.7984]
60/100: train_loss=[0.30242655 0.5214886 ], train_acc=[0.9043 0.8047]
62/100: train_loss=[0.3078044  0.51689637], train_acc=[0.90015 0.8062 ]
64/100: train_loss=[0.29168892 0.51473314], train_acc=[0.9065 0.8081]
66/100: train_loss=[0.2847632 0.517441 ], train_acc=[0.91035 0.80865]
68/100: train_loss=[0.29198176 0.5103886 ], train_acc=[0.90535 0.8095 ]
70/100: train_loss=[0.28565386 0.49864155], train_acc=[0.909   0.81395]
72/100: train_loss=[0.33372885 0.50690836], train_acc=[0.8888  0.81125]
74/100: train_loss=[0.31172442 0.52404785], train_acc=[0.89865 0.8038 ]
76/100: train_loss=[0.27670848 0.49960855], train_acc=[0.9108 0.8154]
78/100: train_loss=[0.27896523 0.5059158 ], train_acc=[0.91155 0.811  ]
80/100: train_loss=[0.28781018 0.5101866 ], train_acc=[0.907  0.8105]
82/100: train_loss=[0.2790758 0.5099853], train_acc=[0.91075 0.81245]
84/100: train_loss=[0.27873594 0.5019155 ], train_acc=[0.91   0.8145]
86/100: train_loss=[0.2772087  0.48899105], train_acc=[0.9097 0.8195]
88/100: train_loss=[0.28645602 0.5010597 ], train_acc=[0.9081  0.81345]
90/100: train_loss=[0.27993214 0.4892501 ], train_acc=[0.90965 0.81855]
92/100: train_loss=[0.26796558 0.49687177], train_acc=[0.91405 0.81555]
94/100: train_loss=[0.27907336 0.48902807], train_acc=[0.9125 0.8187]
96/100: train_loss=[0.2701105  0.48689204], train_acc=[0.91155 0.8199 ]
98/100: train_loss=[0.26207843 0.49298233], train_acc=[0.9172  0.81685]
100/100: train_loss=[0.26797992 0.49430963], train_acc=[0.9138 0.8147]
**** Time taken for fashion_and_mnist_4 = 1522.9308423995972
**** Time taken for fashion_and_mnist = 9238.31781244278
