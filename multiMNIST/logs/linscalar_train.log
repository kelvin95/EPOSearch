Preference Vector = [1.57079632e-04 9.99999988e-01]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[5.66864  1.301414], train_acc=[0.0922 0.5543]
2/100: train_loss=[6.700744  0.9998449], train_acc=[0.0972  0.65625]
4/100: train_loss=[7.4963837  0.71837115], train_acc=[0.10185 0.7566 ]
6/100: train_loss=[7.67794   0.6146225], train_acc=[0.10225 0.7933 ]
8/100: train_loss=[7.8308587 0.5650256], train_acc=[0.10235 0.8098 ]
10/100: train_loss=[7.8248215 0.5195392], train_acc=[0.10045 0.82535]
12/100: train_loss=[7.6972528 0.5070512], train_acc=[0.0994  0.83035]
14/100: train_loss=[7.5825405 0.5200069], train_acc=[0.09805 0.82755]
16/100: train_loss=[7.517732   0.45894918], train_acc=[0.09875 0.84525]
18/100: train_loss=[7.4381213  0.45329285], train_acc=[0.09755 0.84875]
20/100: train_loss=[7.4012895  0.45340982], train_acc=[0.0985 0.8476]
22/100: train_loss=[7.328295 0.422281], train_acc=[0.0995 0.8568]
24/100: train_loss=[7.2048845  0.41559586], train_acc=[0.09925 0.8575 ]
26/100: train_loss=[7.0388     0.42292124], train_acc=[0.1035  0.85795]
28/100: train_loss=[6.9633822 0.4008917], train_acc=[0.1021 0.8645]
30/100: train_loss=[7.0785236  0.40459055], train_acc=[0.09845 0.86365]
32/100: train_loss=[6.8386054  0.41208988], train_acc=[0.1046 0.86  ]
34/100: train_loss=[6.901647   0.39437473], train_acc=[0.10135 0.86765]
36/100: train_loss=[6.7231154  0.37911832], train_acc=[0.1015  0.87225]
38/100: train_loss=[6.749708  0.3839018], train_acc=[0.10055 0.87115]
40/100: train_loss=[6.474674   0.38630712], train_acc=[0.10425 0.87135]
42/100: train_loss=[6.5435553  0.37534484], train_acc=[0.1025  0.87475]
44/100: train_loss=[6.46737    0.37626755], train_acc=[0.10335 0.8747 ]
46/100: train_loss=[6.4337993  0.38604507], train_acc=[0.10495 0.87045]
48/100: train_loss=[6.580868   0.38326693], train_acc=[0.10055 0.8734 ]
50/100: train_loss=[6.4667826  0.35702142], train_acc=[0.1032  0.87985]
52/100: train_loss=[6.3674936  0.35951295], train_acc=[0.10335 0.87845]
54/100: train_loss=[6.3276644  0.36131364], train_acc=[0.1062  0.87985]
56/100: train_loss=[6.3218822  0.35386094], train_acc=[0.1052  0.88215]
58/100: train_loss=[6.2377877  0.36568397], train_acc=[0.1042 0.8781]
60/100: train_loss=[6.303358   0.36184388], train_acc=[0.1035 0.8797]
62/100: train_loss=[6.305315   0.34865126], train_acc=[0.1052  0.88335]
64/100: train_loss=[6.2214603  0.34834272], train_acc=[0.1047  0.88565]
66/100: train_loss=[6.2688556  0.34864083], train_acc=[0.10465 0.88365]
68/100: train_loss=[6.118319   0.35885543], train_acc=[0.1054  0.88055]
70/100: train_loss=[6.2139764  0.33349743], train_acc=[0.10575 0.88995]
72/100: train_loss=[6.137461  0.3325529], train_acc=[0.10395 0.88995]
74/100: train_loss=[6.164342  0.3380714], train_acc=[0.1033  0.88665]
76/100: train_loss=[6.1723866  0.34702095], train_acc=[0.10465 0.88355]
78/100: train_loss=[6.0513926  0.33853945], train_acc=[0.10655 0.8886 ]
80/100: train_loss=[6.00314   0.3338854], train_acc=[0.10545 0.8888 ]
82/100: train_loss=[6.083734  0.3384883], train_acc=[0.10515 0.8854 ]
84/100: train_loss=[6.0567503  0.34068397], train_acc=[0.10415 0.88515]
86/100: train_loss=[6.0145826  0.34237403], train_acc=[0.10305 0.88375]
88/100: train_loss=[6.044458  0.3303063], train_acc=[0.1042 0.8903]
90/100: train_loss=[6.0558     0.34627998], train_acc=[0.10345 0.88455]
92/100: train_loss=[6.0036254 0.3293765], train_acc=[0.1028  0.88955]
94/100: train_loss=[5.928922   0.33370116], train_acc=[0.1033  0.88795]
96/100: train_loss=[6.0436316 0.3351688], train_acc=[0.1013  0.88835]
98/100: train_loss=[5.860847 0.328513], train_acc=[0.10205 0.8897 ]
100/100: train_loss=[6.020119  0.3316233], train_acc=[0.09975 0.88935]
**** Time taken for mnist_0 = 1144.2047600746155
Preference Vector = [0.38275599 0.92384947]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.9034556 1.4199425], train_acc=[0.3521  0.52515]
2/100: train_loss=[1.4449753 1.0540059], train_acc=[0.52315 0.64725]
4/100: train_loss=[1.014233  0.8061901], train_acc=[0.66545 0.7315 ]
6/100: train_loss=[0.802887   0.69279444], train_acc=[0.7377 0.7683]
8/100: train_loss=[0.6915823  0.61544883], train_acc=[0.77465 0.79305]
10/100: train_loss=[0.6290369 0.5710925], train_acc=[0.7938  0.81105]
12/100: train_loss=[0.586553   0.54887563], train_acc=[0.8062 0.8187]
14/100: train_loss=[0.5455462  0.49073547], train_acc=[0.8202 0.8382]
16/100: train_loss=[0.5199426 0.4823184], train_acc=[0.82865 0.83935]
18/100: train_loss=[0.5027748  0.45645383], train_acc=[0.835  0.8472]
20/100: train_loss=[0.48031232 0.46180585], train_acc=[0.8408  0.84705]
22/100: train_loss=[0.46843392 0.44803417], train_acc=[0.84535 0.85295]
24/100: train_loss=[0.4564187 0.4334241], train_acc=[0.84915 0.8554 ]
26/100: train_loss=[0.44676447 0.4301412 ], train_acc=[0.8517 0.8555]
28/100: train_loss=[0.44006696 0.43928793], train_acc=[0.85455 0.85475]
30/100: train_loss=[0.434056   0.41363764], train_acc=[0.8562  0.86395]
32/100: train_loss=[0.4224426  0.41888934], train_acc=[0.8608 0.8591]
34/100: train_loss=[0.41786587 0.39585936], train_acc=[0.8618  0.86865]
36/100: train_loss=[0.40959448 0.40684974], train_acc=[0.8643 0.8652]
38/100: train_loss=[0.3998156  0.39229953], train_acc=[0.86735 0.8713 ]
40/100: train_loss=[0.3951432  0.39261383], train_acc=[0.8688  0.87145]
42/100: train_loss=[0.39196813 0.40542117], train_acc=[0.8704  0.86585]
44/100: train_loss=[0.38460758 0.3719097 ], train_acc=[0.87245 0.87685]
46/100: train_loss=[0.3893234  0.36639845], train_acc=[0.87   0.8783]
48/100: train_loss=[0.37883958 0.37190044], train_acc=[0.87485 0.87775]
50/100: train_loss=[0.3776607  0.36536294], train_acc=[0.87345 0.87725]
52/100: train_loss=[0.37055945 0.36453944], train_acc=[0.87645 0.878  ]
54/100: train_loss=[0.36540934 0.35824928], train_acc=[0.8782 0.8812]
56/100: train_loss=[0.36445487 0.36035725], train_acc=[0.87905 0.87845]
58/100: train_loss=[0.36322287 0.3520155 ], train_acc=[0.87745 0.88345]
60/100: train_loss=[0.36009425 0.34689835], train_acc=[0.87885 0.88485]
62/100: train_loss=[0.35504237 0.36670247], train_acc=[0.88035 0.87885]
64/100: train_loss=[0.35549626 0.35081178], train_acc=[0.8803  0.88315]
66/100: train_loss=[0.3532028  0.35265645], train_acc=[0.8801 0.8819]
68/100: train_loss=[0.35026866 0.35231635], train_acc=[0.8815 0.8838]
70/100: train_loss=[0.3531154  0.34601575], train_acc=[0.88195 0.88605]
72/100: train_loss=[0.34900376 0.33913377], train_acc=[0.8827  0.88595]
74/100: train_loss=[0.34374642 0.3496357 ], train_acc=[0.88575 0.88245]
76/100: train_loss=[0.34143358 0.341266  ], train_acc=[0.8877  0.88735]
78/100: train_loss=[0.34076804 0.34948283], train_acc=[0.88705 0.8864 ]
80/100: train_loss=[0.3402416  0.34461665], train_acc=[0.8878  0.88615]
82/100: train_loss=[0.33978373 0.34301844], train_acc=[0.88835 0.8852 ]
84/100: train_loss=[0.3363628  0.33513513], train_acc=[0.88895 0.8884 ]
86/100: train_loss=[0.33753762 0.34175712], train_acc=[0.88915 0.88535]
88/100: train_loss=[0.33741546 0.3478233 ], train_acc=[0.8879 0.8843]
90/100: train_loss=[0.33235064 0.34608227], train_acc=[0.8904  0.88585]
92/100: train_loss=[0.3297257  0.33643737], train_acc=[0.8904 0.8877]
94/100: train_loss=[0.33327368 0.33350664], train_acc=[0.88825 0.88805]
96/100: train_loss=[0.34241906 0.3362296 ], train_acc=[0.8871  0.88915]
98/100: train_loss=[0.34284365 0.33443895], train_acc=[0.8863 0.8908]
100/100: train_loss=[0.3288544  0.32939935], train_acc=[0.89085 0.89035]
**** Time taken for mnist_1 = 1125.8243803977966
Preference Vector = [0.70710678 0.70710678]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.5348853 1.9011737], train_acc=[0.47805 0.3399 ]
2/100: train_loss=[1.0976533 1.3063886], train_acc=[0.63435 0.55365]
4/100: train_loss=[0.80709636 0.8963716 ], train_acc=[0.73465 0.6989 ]
6/100: train_loss=[0.6539819 0.7573933], train_acc=[0.7848 0.7416]
8/100: train_loss=[0.5760028 0.6656359], train_acc=[0.81135 0.77575]
10/100: train_loss=[0.51405025 0.6262618 ], train_acc=[0.83355 0.78415]
12/100: train_loss=[0.48193833 0.6054677 ], train_acc=[0.84365 0.7948 ]
14/100: train_loss=[0.4762667  0.55871457], train_acc=[0.84455 0.8094 ]
16/100: train_loss=[0.45421255 0.5386601 ], train_acc=[0.8544  0.81755]
18/100: train_loss=[0.44313407 0.5481424 ], train_acc=[0.85605 0.81425]
20/100: train_loss=[0.43152484 0.53967136], train_acc=[0.85895 0.8161 ]
22/100: train_loss=[0.42026913 0.50293446], train_acc=[0.862  0.8316]
24/100: train_loss=[0.40031067 0.4793401 ], train_acc=[0.8703 0.8366]
26/100: train_loss=[0.39214316 0.4709074 ], train_acc=[0.87185 0.84215]
28/100: train_loss=[0.39112613 0.46828684], train_acc=[0.873   0.84275]
30/100: train_loss=[0.40944135 0.4647077 ], train_acc=[0.8646 0.8443]
32/100: train_loss=[0.37414733 0.4792829 ], train_acc=[0.8787 0.8416]
34/100: train_loss=[0.3751808  0.44967812], train_acc=[0.8775 0.8497]
36/100: train_loss=[0.365494   0.43144652], train_acc=[0.8809 0.8551]
38/100: train_loss=[0.3608473 0.424091 ], train_acc=[0.8817  0.85895]
40/100: train_loss=[0.37794766 0.4395589 ], train_acc=[0.8764  0.85165]
42/100: train_loss=[0.36264768 0.4147626 ], train_acc=[0.88   0.8609]
44/100: train_loss=[0.3498118 0.4151685], train_acc=[0.88615 0.86055]
46/100: train_loss=[0.35038874 0.41320556], train_acc=[0.88565 0.86275]
48/100: train_loss=[0.34449714 0.41293326], train_acc=[0.8888  0.86255]
50/100: train_loss=[0.3530296 0.4080809], train_acc=[0.88635 0.8613 ]
52/100: train_loss=[0.34913278 0.4084591 ], train_acc=[0.8843  0.86525]
54/100: train_loss=[0.3559328  0.39934257], train_acc=[0.88325 0.8674 ]
56/100: train_loss=[0.33544767 0.393568  ], train_acc=[0.89255 0.86915]
58/100: train_loss=[0.33256164 0.3936721 ], train_acc=[0.89205 0.8678 ]
60/100: train_loss=[0.35111707 0.39646158], train_acc=[0.88415 0.86765]
62/100: train_loss=[0.3350495  0.39584178], train_acc=[0.89055 0.87085]
64/100: train_loss=[0.33247498 0.38045236], train_acc=[0.89335 0.87405]
66/100: train_loss=[0.32904038 0.38291818], train_acc=[0.892  0.8732]
68/100: train_loss=[0.3226788 0.4025144], train_acc=[0.8963  0.86635]
70/100: train_loss=[0.31997284 0.4140361 ], train_acc=[0.89645 0.86175]
72/100: train_loss=[0.31847972 0.38694337], train_acc=[0.8955 0.8706]
74/100: train_loss=[0.32808477 0.37247008], train_acc=[0.8923 0.8756]
76/100: train_loss=[0.318832   0.38483045], train_acc=[0.89665 0.87195]
78/100: train_loss=[0.33864403 0.38724837], train_acc=[0.8911  0.86945]
80/100: train_loss=[0.31226513 0.37164736], train_acc=[0.89875 0.8768 ]
82/100: train_loss=[0.31266433 0.3670936 ], train_acc=[0.8984  0.87795]
84/100: train_loss=[0.3150286 0.361904 ], train_acc=[0.8977 0.8796]
86/100: train_loss=[0.3179131  0.36773393], train_acc=[0.89595 0.87755]
88/100: train_loss=[0.31690937 0.36879736], train_acc=[0.89625 0.87665]
90/100: train_loss=[0.31573957 0.36685446], train_acc=[0.89665 0.8759 ]
92/100: train_loss=[0.31505856 0.36140436], train_acc=[0.89845 0.88125]
94/100: train_loss=[0.31064022 0.38729128], train_acc=[0.9002  0.87285]
96/100: train_loss=[0.31421667 0.37085587], train_acc=[0.89755 0.8779 ]
98/100: train_loss=[0.31678104 0.3559472 ], train_acc=[0.89685 0.88205]
100/100: train_loss=[0.3132797  0.35721338], train_acc=[0.90035 0.8814 ]
**** Time taken for mnist_2 = 1121.1280632019043
Preference Vector = [0.92384947 0.38275599]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.2144655 1.9619018], train_acc=[0.5961 0.3134]
2/100: train_loss=[0.89011365 1.5253621 ], train_acc=[0.70375 0.4754 ]
4/100: train_loss=[0.62765145 1.1242838 ], train_acc=[0.7928 0.6177]
6/100: train_loss=[0.56492627 0.93424374], train_acc=[0.814 0.685]
8/100: train_loss=[0.49357244 0.8385282 ], train_acc=[0.83965 0.7149 ]
10/100: train_loss=[0.44756213 0.7573219 ], train_acc=[0.85445 0.7484 ]
12/100: train_loss=[0.43927678 0.7143463 ], train_acc=[0.8564 0.7614]
14/100: train_loss=[0.42182344 0.6733721 ], train_acc=[0.86295 0.77365]
16/100: train_loss=[0.39822644 0.6470979 ], train_acc=[0.87035 0.7831 ]
18/100: train_loss=[0.41742128 0.62824756], train_acc=[0.8615 0.7882]
20/100: train_loss=[0.38123727 0.6043556 ], train_acc=[0.8751  0.79885]
22/100: train_loss=[0.37934732 0.59084165], train_acc=[0.8755 0.8008]
24/100: train_loss=[0.36937737 0.5759721 ], train_acc=[0.8811 0.807 ]
26/100: train_loss=[0.36279428 0.564409  ], train_acc=[0.8833  0.80975]
28/100: train_loss=[0.34860924 0.5548225 ], train_acc=[0.886  0.8171]
30/100: train_loss=[0.3446852  0.54113966], train_acc=[0.8886  0.81865]
32/100: train_loss=[0.3296453  0.53262544], train_acc=[0.89375 0.82185]
34/100: train_loss=[0.33262742 0.52250767], train_acc=[0.89245 0.8263 ]
36/100: train_loss=[0.33156106 0.52032083], train_acc=[0.89205 0.82695]
38/100: train_loss=[0.3383885  0.51673704], train_acc=[0.89105 0.8266 ]
40/100: train_loss=[0.32179922 0.495478  ], train_acc=[0.894   0.83485]
42/100: train_loss=[0.3102642  0.49147812], train_acc=[0.89995 0.8337 ]
44/100: train_loss=[0.3107034  0.49025998], train_acc=[0.89835 0.83545]
46/100: train_loss=[0.31637526 0.48004568], train_acc=[0.89575 0.83915]
48/100: train_loss=[0.32527813 0.47346744], train_acc=[0.89325 0.84245]
50/100: train_loss=[0.30901083 0.47380906], train_acc=[0.8995 0.8432]
52/100: train_loss=[0.3188352 0.4818007], train_acc=[0.896   0.83935]
54/100: train_loss=[0.31980154 0.46199867], train_acc=[0.895   0.84715]
56/100: train_loss=[0.29564536 0.46338883], train_acc=[0.90305 0.8448 ]
58/100: train_loss=[0.2939706  0.46150336], train_acc=[0.90255 0.84705]
60/100: train_loss=[0.3017853  0.44725773], train_acc=[0.9003  0.85135]
62/100: train_loss=[0.30159315 0.44743204], train_acc=[0.90085 0.8504 ]
64/100: train_loss=[0.29341307 0.4416186 ], train_acc=[0.9026  0.85285]
66/100: train_loss=[0.2938386  0.44642496], train_acc=[0.9021 0.8515]
68/100: train_loss=[0.2919057  0.43658078], train_acc=[0.90375 0.85425]
70/100: train_loss=[0.29221508 0.43452218], train_acc=[0.9039 0.8557]
72/100: train_loss=[0.29209393 0.4412811 ], train_acc=[0.9038 0.8544]
74/100: train_loss=[0.30062774 0.44261563], train_acc=[0.9022  0.85215]
76/100: train_loss=[0.28960595 0.43130746], train_acc=[0.90545 0.85755]
78/100: train_loss=[0.29926422 0.43419772], train_acc=[0.90115 0.8533 ]
80/100: train_loss=[0.28348714 0.4231637 ], train_acc=[0.90765 0.8583 ]
82/100: train_loss=[0.28659508 0.42107686], train_acc=[0.90555 0.8604 ]
84/100: train_loss=[0.299617   0.42575982], train_acc=[0.9005 0.859 ]
86/100: train_loss=[0.28475314 0.4179316 ], train_acc=[0.9066  0.86175]
88/100: train_loss=[0.2843311  0.43051106], train_acc=[0.90605 0.8578 ]
90/100: train_loss=[0.2979038 0.4221944], train_acc=[0.90335 0.8596 ]
92/100: train_loss=[0.2910329 0.4140372], train_acc=[0.90435 0.86215]
94/100: train_loss=[0.28316855 0.40779728], train_acc=[0.90735 0.86275]
96/100: train_loss=[0.28228703 0.41145137], train_acc=[0.907  0.8634]
98/100: train_loss=[0.27689356 0.41303167], train_acc=[0.90805 0.863  ]
100/100: train_loss=[0.28293034 0.40688425], train_acc=[0.9076  0.86375]
**** Time taken for mnist_3 = 1118.0884835720062
Preference Vector = [9.99999988e-01 1.57079632e-04]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[0.8852369 7.746482 ], train_acc=[0.71245 0.1013 ]
2/100: train_loss=[0.69523084 8.938354  ], train_acc=[0.7766 0.1019]
4/100: train_loss=[0.5699798 9.64169  ], train_acc=[0.8128 0.1046]
6/100: train_loss=[ 0.4948073 10.251864 ], train_acc=[0.8378  0.10565]
8/100: train_loss=[ 0.49648127 10.056798  ], train_acc=[0.83665 0.1066 ]
10/100: train_loss=[ 0.43759573 10.43863   ], train_acc=[0.8563  0.10755]
12/100: train_loss=[ 0.41097173 10.129622  ], train_acc=[0.8658  0.10965]
14/100: train_loss=[ 0.41817424 10.178387  ], train_acc=[0.86305 0.1092 ]
16/100: train_loss=[0.39211947 9.848946  ], train_acc=[0.8704  0.10895]
18/100: train_loss=[0.41112143 9.956242  ], train_acc=[0.86675 0.1082 ]
20/100: train_loss=[0.3606609 9.605399 ], train_acc=[0.88    0.10985]
22/100: train_loss=[0.3594851 9.582106 ], train_acc=[0.881   0.10995]
24/100: train_loss=[0.35379928 9.38867   ], train_acc=[0.8828 0.1072]
26/100: train_loss=[0.33936352 9.141113  ], train_acc=[0.88745 0.10985]
28/100: train_loss=[0.33523577 8.993378  ], train_acc=[0.8906 0.1108]
30/100: train_loss=[0.33005178 8.826406  ], train_acc=[0.89235 0.1101 ]
32/100: train_loss=[0.3280519 8.811494 ], train_acc=[0.89265 0.10925]
34/100: train_loss=[0.32888135 8.536359  ], train_acc=[0.8903  0.11135]
36/100: train_loss=[0.3187889 8.498803 ], train_acc=[0.8957  0.10965]
38/100: train_loss=[0.32284617 8.452979  ], train_acc=[0.89275 0.1119 ]
40/100: train_loss=[0.3156733 8.39424  ], train_acc=[0.8944 0.1123]
42/100: train_loss=[0.3122577 8.233613 ], train_acc=[0.8982 0.1102]
44/100: train_loss=[0.30844823 8.306496  ], train_acc=[0.9001  0.11065]
46/100: train_loss=[0.30766156 8.069144  ], train_acc=[0.89815 0.1105 ]
48/100: train_loss=[0.3007053 8.1479225], train_acc=[0.90195 0.1119 ]
50/100: train_loss=[0.30444258 8.106809  ], train_acc=[0.8994  0.11115]
52/100: train_loss=[0.30762264 8.180806  ], train_acc=[0.89895 0.11065]
54/100: train_loss=[0.29547262 7.86881   ], train_acc=[0.90405 0.11075]
56/100: train_loss=[0.29853132 7.9328585 ], train_acc=[0.9027  0.11155]
58/100: train_loss=[0.2947573 7.8466396], train_acc=[0.90265 0.1114 ]
60/100: train_loss=[0.2913105 7.9457684], train_acc=[0.9049 0.1106]
62/100: train_loss=[0.28698862 7.757902  ], train_acc=[0.9061 0.1105]
64/100: train_loss=[0.28994417 7.8374557 ], train_acc=[0.9061  0.11265]
66/100: train_loss=[0.28782734 7.7080674 ], train_acc=[0.9038  0.11205]
68/100: train_loss=[0.2832036 7.704313 ], train_acc=[0.90685 0.11155]
70/100: train_loss=[0.2890094 7.6831727], train_acc=[0.90455 0.1102 ]
72/100: train_loss=[0.2865885 7.6557217], train_acc=[0.90675 0.1083 ]
74/100: train_loss=[0.28004152 7.5513716 ], train_acc=[0.90745 0.1106 ]
76/100: train_loss=[0.2927696 7.613169 ], train_acc=[0.9036 0.109 ]
78/100: train_loss=[0.282315 7.534396], train_acc=[0.90765 0.11105]
80/100: train_loss=[0.28221488 7.4573035 ], train_acc=[0.90795 0.1111 ]
82/100: train_loss=[0.2801372 7.591576 ], train_acc=[0.9071  0.11125]
84/100: train_loss=[0.27524716 7.555681  ], train_acc=[0.90935 0.1088 ]
86/100: train_loss=[0.27597976 7.4867563 ], train_acc=[0.90995 0.1116 ]
88/100: train_loss=[0.28087726 7.5266943 ], train_acc=[0.9096 0.112 ]
90/100: train_loss=[0.27404413 7.5129128 ], train_acc=[0.9101 0.1098]
92/100: train_loss=[0.2715372 7.4466724], train_acc=[0.9112  0.10845]
94/100: train_loss=[0.2677679 7.3641744], train_acc=[0.9124  0.11025]
96/100: train_loss=[0.2722006 7.3918643], train_acc=[0.91065 0.10855]
98/100: train_loss=[0.27772468 7.305199  ], train_acc=[0.91095 0.1071 ]
100/100: train_loss=[0.2761035 7.418661 ], train_acc=[0.9085 0.1096]
**** Time taken for mnist_4 = 1864.3253149986267
**** Time taken for mnist = 6373.706862211227
Preference Vector = [1.57079632e-04 9.99999988e-01]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[7.8894277 1.0451481], train_acc=[0.11345 0.61215]
2/100: train_loss=[8.660403   0.92497635], train_acc=[0.11785 0.66135]
4/100: train_loss=[8.458129  0.8332546], train_acc=[0.12355 0.6969 ]
6/100: train_loss=[7.7016907  0.76935476], train_acc=[0.12545 0.71975]
8/100: train_loss=[7.499008   0.71151197], train_acc=[0.12165 0.74265]
10/100: train_loss=[7.680169   0.70442414], train_acc=[0.1159 0.7373]
12/100: train_loss=[7.6938186 0.6706694], train_acc=[0.1111  0.75655]
14/100: train_loss=[7.4591584 0.6541234], train_acc=[0.115   0.76355]
16/100: train_loss=[7.3088765 0.6438616], train_acc=[0.1158 0.7654]
18/100: train_loss=[7.1423726  0.62108773], train_acc=[0.11535 0.77425]
20/100: train_loss=[7.2363167  0.60957867], train_acc=[0.11565 0.77735]
22/100: train_loss=[7.193695  0.6254096], train_acc=[0.11675 0.7679 ]
24/100: train_loss=[7.050776   0.59490204], train_acc=[0.1187  0.78075]
26/100: train_loss=[7.0976768  0.59991527], train_acc=[0.1166  0.78255]
28/100: train_loss=[6.8838086  0.59669834], train_acc=[0.1207 0.7785]
30/100: train_loss=[6.7086363 0.5844394], train_acc=[0.11935 0.7849 ]
32/100: train_loss=[6.7556987  0.57999104], train_acc=[0.12215 0.78565]
34/100: train_loss=[6.8210907 0.5636883], train_acc=[0.1216 0.7944]
36/100: train_loss=[6.6710067  0.56224316], train_acc=[0.12295 0.79215]
38/100: train_loss=[6.5461936 0.5655564], train_acc=[0.11715 0.7939 ]
40/100: train_loss=[6.666178   0.56234413], train_acc=[0.11775 0.79535]
42/100: train_loss=[6.2783885  0.55238533], train_acc=[0.1224  0.79615]
44/100: train_loss=[6.6085405 0.5459246], train_acc=[0.12105 0.80085]
46/100: train_loss=[6.44738   0.5437333], train_acc=[0.12365 0.7996 ]
48/100: train_loss=[6.438365  0.5450448], train_acc=[0.11875 0.80155]
50/100: train_loss=[6.269852   0.56148374], train_acc=[0.1237 0.7937]
52/100: train_loss=[6.1975865 0.5409031], train_acc=[0.1166  0.80395]
54/100: train_loss=[5.9844694 0.5431354], train_acc=[0.1186 0.8008]
56/100: train_loss=[6.257714 0.533508], train_acc=[0.119   0.80365]
58/100: train_loss=[6.0736585 0.5347147], train_acc=[0.1188  0.80315]
60/100: train_loss=[6.0431943 0.5386675], train_acc=[0.11475 0.8044 ]
62/100: train_loss=[5.8947115 0.5293974], train_acc=[0.12015 0.80675]
64/100: train_loss=[5.9266133  0.52249557], train_acc=[0.1227  0.80665]
66/100: train_loss=[5.9984345 0.5327272], train_acc=[0.12235 0.8043 ]
68/100: train_loss=[5.636491   0.53645265], train_acc=[0.1198  0.80285]
70/100: train_loss=[5.9152994  0.52590036], train_acc=[0.1182 0.8076]
72/100: train_loss=[5.71904    0.52738786], train_acc=[0.12005 0.8096 ]
74/100: train_loss=[5.7702084  0.52476853], train_acc=[0.1187  0.80805]
76/100: train_loss=[5.712937   0.53481823], train_acc=[0.12315 0.80305]
78/100: train_loss=[5.518003  0.5182004], train_acc=[0.11665 0.8102 ]
80/100: train_loss=[5.4818363 0.5211862], train_acc=[0.1206 0.8094]
82/100: train_loss=[5.5641537 0.511836 ], train_acc=[0.11935 0.81165]
84/100: train_loss=[5.519451  0.5145908], train_acc=[0.1206 0.8102]
86/100: train_loss=[5.5530057 0.5101427], train_acc=[0.11815 0.81265]
88/100: train_loss=[5.5637536  0.51908153], train_acc=[0.11355 0.8097 ]
90/100: train_loss=[5.2943406  0.50602967], train_acc=[0.11715 0.81385]
92/100: train_loss=[5.394993 0.510256], train_acc=[0.11735 0.8138 ]
94/100: train_loss=[5.3739033 0.5108413], train_acc=[0.1183  0.81205]
96/100: train_loss=[5.336385  0.5073719], train_acc=[0.11525 0.81495]
98/100: train_loss=[5.2455626 0.5101158], train_acc=[0.1191  0.81315]
100/100: train_loss=[5.2363176 0.5183573], train_acc=[0.11535 0.8076 ]
**** Time taken for fashion_0 = 1966.2014968395233
Preference Vector = [0.38275599 0.92384947]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.564673  1.2328202], train_acc=[0.4282  0.54615]
2/100: train_loss=[1.2496018 1.0145786], train_acc=[0.54355 0.63415]
4/100: train_loss=[1.0322207 0.8964102], train_acc=[0.61515 0.67235]
6/100: train_loss=[0.93394625 0.8119196 ], train_acc=[0.65025 0.7003 ]
8/100: train_loss=[0.8827195 0.7448779], train_acc=[0.66375 0.7234 ]
10/100: train_loss=[0.84054005 0.73838866], train_acc=[0.6832 0.725 ]
12/100: train_loss=[0.8073668 0.7084757], train_acc=[0.69715 0.7437 ]
14/100: train_loss=[0.7949964 0.6804209], train_acc=[0.70465 0.75145]
16/100: train_loss=[0.76843387 0.6744538 ], train_acc=[0.7119 0.7515]
18/100: train_loss=[0.75420886 0.64472556], train_acc=[0.7172  0.76255]
20/100: train_loss=[0.74251103 0.6365763 ], train_acc=[0.7202 0.7662]
22/100: train_loss=[0.73270243 0.6295789 ], train_acc=[0.7232  0.76895]
24/100: train_loss=[0.71892804 0.621538  ], train_acc=[0.7271  0.77285]
26/100: train_loss=[0.707231   0.60616857], train_acc=[0.7344 0.7769]
28/100: train_loss=[0.7026306 0.6113588], train_acc=[0.73525 0.7787 ]
30/100: train_loss=[0.6927902 0.5967094], train_acc=[0.73855 0.7829 ]
32/100: train_loss=[0.68697274 0.5960299 ], train_acc=[0.742   0.78115]
34/100: train_loss=[0.6761281  0.59210366], train_acc=[0.74375 0.7865 ]
36/100: train_loss=[0.68248916 0.58428353], train_acc=[0.7424 0.7854]
38/100: train_loss=[0.6873224  0.60531735], train_acc=[0.74255 0.77565]
40/100: train_loss=[0.6627685  0.57838655], train_acc=[0.74995 0.7918 ]
42/100: train_loss=[0.656292   0.57225055], train_acc=[0.75425 0.7921 ]
44/100: train_loss=[0.6580348  0.59316254], train_acc=[0.7537  0.78725]
46/100: train_loss=[0.6519336 0.5702162], train_acc=[0.7581  0.79155]
48/100: train_loss=[0.64875436 0.6015981 ], train_acc=[0.7536  0.78065]
50/100: train_loss=[0.6577314 0.5735413], train_acc=[0.75265 0.7914 ]
52/100: train_loss=[0.6377836  0.57427466], train_acc=[0.76405 0.78525]
54/100: train_loss=[0.63483787 0.5548333 ], train_acc=[0.76335 0.79715]
56/100: train_loss=[0.6389058 0.5555982], train_acc=[0.7634  0.79605]
58/100: train_loss=[0.62800795 0.55316395], train_acc=[0.76475 0.7963 ]
60/100: train_loss=[0.62529063 0.5557514 ], train_acc=[0.7701  0.79505]
62/100: train_loss=[0.625843   0.54905665], train_acc=[0.7668 0.8005]
64/100: train_loss=[0.63253254 0.575735  ], train_acc=[0.76315 0.78655]
66/100: train_loss=[0.62069327 0.5482942 ], train_acc=[0.7669  0.80005]
68/100: train_loss=[0.62204015 0.5586558 ], train_acc=[0.7681 0.797 ]
70/100: train_loss=[0.61276877 0.55577755], train_acc=[0.77295 0.7998 ]
72/100: train_loss=[0.61146647 0.5496679 ], train_acc=[0.7755 0.7996]
74/100: train_loss=[0.60959953 0.54779464], train_acc=[0.7726  0.79995]
76/100: train_loss=[0.61373794 0.5493639 ], train_acc=[0.77165 0.79815]
78/100: train_loss=[0.6015172 0.5451428], train_acc=[0.7786 0.8033]
80/100: train_loss=[0.6061024  0.54932296], train_acc=[0.77515 0.7981 ]
82/100: train_loss=[0.6047816  0.57386035], train_acc=[0.77755 0.78835]
84/100: train_loss=[0.59927434 0.5567012 ], train_acc=[0.7798  0.80125]
86/100: train_loss=[0.6167329 0.5380897], train_acc=[0.7677 0.8046]
88/100: train_loss=[0.5958319  0.53473014], train_acc=[0.7827  0.80515]
90/100: train_loss=[0.5965493 0.5546556], train_acc=[0.78045 0.797  ]
92/100: train_loss=[0.5956085 0.5354553], train_acc=[0.7818 0.8029]
94/100: train_loss=[0.59117854 0.55101997], train_acc=[0.78215 0.7963 ]
96/100: train_loss=[0.59310764 0.5379916 ], train_acc=[0.78125 0.80095]
98/100: train_loss=[0.59247446 0.5384904 ], train_acc=[0.78215 0.8009 ]
100/100: train_loss=[0.59415734 0.5373325 ], train_acc=[0.7795  0.80455]
**** Time taken for fashion_1 = 2024.726594209671
Preference Vector = [0.70710678 0.70710678]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.1998248 1.3560337], train_acc=[0.5603  0.52395]
2/100: train_loss=[1.0091287 1.0732329], train_acc=[0.6266 0.6061]
4/100: train_loss=[0.8730072  0.91350585], train_acc=[0.67725 0.6662 ]
6/100: train_loss=[0.8151886  0.84514225], train_acc=[0.6982 0.6926]
8/100: train_loss=[0.7711497  0.78868353], train_acc=[0.71825 0.7113 ]
10/100: train_loss=[0.782064   0.77454823], train_acc=[0.70715 0.713  ]
12/100: train_loss=[0.72909796 0.72382224], train_acc=[0.7335  0.73825]
14/100: train_loss=[0.72280526 0.7033747 ], train_acc=[0.7336  0.74315]
16/100: train_loss=[0.69156444 0.6999463 ], train_acc=[0.7466  0.74425]
18/100: train_loss=[0.6723313 0.6707552], train_acc=[0.75475 0.7572 ]
20/100: train_loss=[0.6593481  0.66238755], train_acc=[0.757 0.759]
22/100: train_loss=[0.6438296  0.67144185], train_acc=[0.7632  0.75575]
24/100: train_loss=[0.6559181 0.6492279], train_acc=[0.7605 0.7607]
26/100: train_loss=[0.6665915  0.66089576], train_acc=[0.75515 0.75945]
28/100: train_loss=[0.65723157 0.67145884], train_acc=[0.7531 0.7569]
30/100: train_loss=[0.61408144 0.6273009 ], train_acc=[0.7751  0.77105]
32/100: train_loss=[0.6150796 0.6255617], train_acc=[0.774 0.771]
34/100: train_loss=[0.6314182 0.6301943], train_acc=[0.76305 0.77185]
36/100: train_loss=[0.61394304 0.60822713], train_acc=[0.77095 0.77785]
38/100: train_loss=[0.5926128  0.60516745], train_acc=[0.78215 0.7799 ]
40/100: train_loss=[0.59753436 0.609093  ], train_acc=[0.7812  0.77865]
42/100: train_loss=[0.58482355 0.5971278 ], train_acc=[0.7864 0.782 ]
44/100: train_loss=[0.61128354 0.5975774 ], train_acc=[0.7737 0.7826]
46/100: train_loss=[0.59381336 0.59168273], train_acc=[0.78295 0.78325]
48/100: train_loss=[0.5799269 0.6032262], train_acc=[0.7887  0.77705]
50/100: train_loss=[0.5816925  0.58037215], train_acc=[0.786   0.78815]
52/100: train_loss=[0.5735746 0.5839695], train_acc=[0.79325 0.7864 ]
54/100: train_loss=[0.57151806 0.58787555], train_acc=[0.7912 0.7851]
56/100: train_loss=[0.5786838  0.57641166], train_acc=[0.78855 0.7898 ]
58/100: train_loss=[0.57039046 0.5759206 ], train_acc=[0.7907  0.78835]
60/100: train_loss=[0.5757187 0.5842205], train_acc=[0.789  0.7877]
62/100: train_loss=[0.5700283 0.5714243], train_acc=[0.7891 0.7903]
64/100: train_loss=[0.5626007  0.58791804], train_acc=[0.79395 0.7833 ]
66/100: train_loss=[0.55380106 0.5610594 ], train_acc=[0.7989  0.79665]
68/100: train_loss=[0.5667373 0.5758142], train_acc=[0.795  0.7864]
70/100: train_loss=[0.55658436 0.5799244 ], train_acc=[0.7983 0.7873]
72/100: train_loss=[0.5555084 0.5731342], train_acc=[0.798   0.79285]
74/100: train_loss=[0.5498881  0.56157255], train_acc=[0.7993  0.79355]
76/100: train_loss=[0.5520893 0.5647478], train_acc=[0.79775 0.796  ]
78/100: train_loss=[0.5590049  0.56521636], train_acc=[0.79645 0.7944 ]
80/100: train_loss=[0.5496299 0.5613454], train_acc=[0.80165 0.79505]
82/100: train_loss=[0.54924405 0.5538276 ], train_acc=[0.79985 0.7975 ]
84/100: train_loss=[0.54306734 0.5641366 ], train_acc=[0.8006  0.79495]
86/100: train_loss=[0.5406425  0.55379057], train_acc=[0.8023  0.79875]
88/100: train_loss=[0.55192333 0.56256735], train_acc=[0.79845 0.7935 ]
90/100: train_loss=[0.53891623 0.54892397], train_acc=[0.803   0.79965]
92/100: train_loss=[0.54361254 0.5502993 ], train_acc=[0.8018  0.79935]
94/100: train_loss=[0.54971915 0.5618806 ], train_acc=[0.80075 0.79215]
96/100: train_loss=[0.53608936 0.5592471 ], train_acc=[0.80435 0.79565]
98/100: train_loss=[0.5444168  0.54525864], train_acc=[0.8016  0.80175]
100/100: train_loss=[0.5549649 0.547459 ], train_acc=[0.8011  0.80335]
**** Time taken for fashion_2 = 2026.472867488861
Preference Vector = [0.92384947 0.38275599]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.0986046 1.6031402], train_acc=[0.59155 0.4168 ]
2/100: train_loss=[0.9208847 1.2773725], train_acc=[0.65405 0.5428 ]
4/100: train_loss=[0.81522757 1.0889039 ], train_acc=[0.69545 0.60505]
6/100: train_loss=[0.75805885 0.9743444 ], train_acc=[0.71535 0.6476 ]
8/100: train_loss=[0.71079177 0.91905975], train_acc=[0.73765 0.66385]
10/100: train_loss=[0.6947393  0.88145614], train_acc=[0.74065 0.6772 ]
12/100: train_loss=[0.69244015 0.86149496], train_acc=[0.7432 0.679 ]
14/100: train_loss=[0.6456294  0.81938577], train_acc=[0.7627  0.69885]
16/100: train_loss=[0.65572643 0.81522864], train_acc=[0.7629 0.6963]
18/100: train_loss=[0.6195921 0.7835787], train_acc=[0.77115 0.709  ]
20/100: train_loss=[0.62369955 0.7737167 ], train_acc=[0.7711  0.71365]
22/100: train_loss=[0.6248504 0.7587686], train_acc=[0.77135 0.7205 ]
24/100: train_loss=[0.6089966 0.7512576], train_acc=[0.77315 0.72505]
26/100: train_loss=[0.58601254 0.73885804], train_acc=[0.7866 0.7271]
28/100: train_loss=[0.5938042 0.75527  ], train_acc=[0.7833 0.7205]
30/100: train_loss=[0.5902889  0.73723924], train_acc=[0.7841  0.72895]
32/100: train_loss=[0.57811904 0.7265352 ], train_acc=[0.78565 0.7321 ]
34/100: train_loss=[0.5814451  0.71549433], train_acc=[0.787   0.73965]
36/100: train_loss=[0.5641364  0.71261185], train_acc=[0.7911  0.74065]
38/100: train_loss=[0.56879914 0.70195174], train_acc=[0.79185 0.7426 ]
40/100: train_loss=[0.6078429  0.70066154], train_acc=[0.7747  0.74115]
42/100: train_loss=[0.5700371 0.6939423], train_acc=[0.78505 0.74575]
44/100: train_loss=[0.55120945 0.6968346 ], train_acc=[0.7974  0.74265]
46/100: train_loss=[0.5604794 0.6808683], train_acc=[0.793   0.75115]
48/100: train_loss=[0.5449099  0.68929285], train_acc=[0.79955 0.7477 ]
50/100: train_loss=[0.54820204 0.68196595], train_acc=[0.79775 0.75215]
52/100: train_loss=[0.54227185 0.67715526], train_acc=[0.7968  0.75215]
54/100: train_loss=[0.57341343 0.6818947 ], train_acc=[0.7892 0.7476]
56/100: train_loss=[0.5452317 0.6735443], train_acc=[0.8002 0.7509]
58/100: train_loss=[0.54718775 0.6694308 ], train_acc=[0.7981  0.75265]
60/100: train_loss=[0.5346823 0.6707672], train_acc=[0.8031  0.75455]
62/100: train_loss=[0.55345786 0.664355  ], train_acc=[0.79465 0.7551 ]
64/100: train_loss=[0.5264119  0.66486686], train_acc=[0.80555 0.756  ]
66/100: train_loss=[0.52714586 0.660711  ], train_acc=[0.8057  0.75765]
68/100: train_loss=[0.5466667 0.6609872], train_acc=[0.79925 0.75845]
70/100: train_loss=[0.541592   0.65766656], train_acc=[0.80045 0.75755]
72/100: train_loss=[0.53088146 0.6629334 ], train_acc=[0.80375 0.7556 ]
74/100: train_loss=[0.5255383 0.6645851], train_acc=[0.8053 0.7568]
76/100: train_loss=[0.52801704 0.66138965], train_acc=[0.80175 0.75965]
78/100: train_loss=[0.52069914 0.65494406], train_acc=[0.80735 0.75865]
80/100: train_loss=[0.5520078  0.65175563], train_acc=[0.7974 0.7612]
82/100: train_loss=[0.5170236 0.6446478], train_acc=[0.8072 0.7651]
84/100: train_loss=[0.52181494 0.65032583], train_acc=[0.80575 0.76355]
86/100: train_loss=[0.52110183 0.6413014 ], train_acc=[0.80925 0.7663 ]
88/100: train_loss=[0.5203186 0.6482869], train_acc=[0.8065 0.7647]
90/100: train_loss=[0.5276697  0.63895136], train_acc=[0.804  0.7664]
92/100: train_loss=[0.5324816  0.64800715], train_acc=[0.80155 0.76525]
94/100: train_loss=[0.5091789 0.6339365], train_acc=[0.8112  0.77025]
96/100: train_loss=[0.52145815 0.63445264], train_acc=[0.8073  0.76815]
98/100: train_loss=[0.5133991  0.63615227], train_acc=[0.80725 0.76895]
100/100: train_loss=[0.5154819 0.6377943], train_acc=[0.80825 0.7702 ]
**** Time taken for fashion_3 = 2027.0865678787231
Preference Vector = [9.99999988e-01 1.57079632e-04]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.0571084 5.1813154], train_acc=[0.60215 0.09595]
2/100: train_loss=[0.9590516 5.6415205], train_acc=[0.6358  0.09125]
4/100: train_loss=[0.8049643 5.7577186], train_acc=[0.70575 0.0909 ]
6/100: train_loss=[0.75304663 5.5722885 ], train_acc=[0.7253  0.09355]
8/100: train_loss=[0.71172535 5.732992  ], train_acc=[0.7379 0.0929]
10/100: train_loss=[0.6890625 5.9435134], train_acc=[0.748  0.0931]
12/100: train_loss=[0.6777834 5.8501506], train_acc=[0.7534 0.0938]
14/100: train_loss=[0.6441039 5.76551  ], train_acc=[0.7617 0.0943]
16/100: train_loss=[0.6373718 5.976816 ], train_acc=[0.76775 0.09525]
18/100: train_loss=[0.62072957 5.824318  ], train_acc=[0.77305 0.0959 ]
20/100: train_loss=[0.62277174 5.756068  ], train_acc=[0.77095 0.09645]
22/100: train_loss=[0.6090261 5.6319456], train_acc=[0.77385 0.0965 ]
24/100: train_loss=[0.60737497 5.6167254 ], train_acc=[0.77545 0.097  ]
26/100: train_loss=[0.59022695 5.4713216 ], train_acc=[0.7816  0.10115]
28/100: train_loss=[0.59219885 5.327285  ], train_acc=[0.7818 0.1025]
30/100: train_loss=[0.5862001 5.3204503], train_acc=[0.7843  0.10475]
32/100: train_loss=[0.5692568 5.1631913], train_acc=[0.78975 0.10445]
34/100: train_loss=[0.57251537 5.223288  ], train_acc=[0.7887  0.10655]
36/100: train_loss=[0.56922525 5.20851   ], train_acc=[0.7882  0.10525]
38/100: train_loss=[0.5530246 5.113271 ], train_acc=[0.79695 0.1062 ]
40/100: train_loss=[0.55307645 5.0864425 ], train_acc=[0.7976  0.10895]
42/100: train_loss=[0.5693063 5.076802 ], train_acc=[0.7904  0.10855]
44/100: train_loss=[0.55260944 5.16391   ], train_acc=[0.79545 0.1114 ]
46/100: train_loss=[0.5457218 5.079652 ], train_acc=[0.8012  0.10865]
48/100: train_loss=[0.5484923 5.105022 ], train_acc=[0.80015 0.10955]
50/100: train_loss=[0.53906703 5.028025  ], train_acc=[0.80295 0.11095]
52/100: train_loss=[0.54476225 5.049381  ], train_acc=[0.799   0.11225]
54/100: train_loss=[0.55565995 4.907756  ], train_acc=[0.7965  0.11385]
56/100: train_loss=[0.5386059 5.068847 ], train_acc=[0.8015  0.11125]
58/100: train_loss=[0.53380316 4.965957  ], train_acc=[0.8031 0.1118]
60/100: train_loss=[0.52749854 4.9433417 ], train_acc=[0.80675 0.1109 ]
62/100: train_loss=[0.53665864 4.9500346 ], train_acc=[0.8022 0.1117]
64/100: train_loss=[0.52701765 4.9492135 ], train_acc=[0.8051 0.1114]
66/100: train_loss=[0.52001727 4.8917985 ], train_acc=[0.81115 0.10985]
68/100: train_loss=[0.52357095 4.89443   ], train_acc=[0.80695 0.11   ]
70/100: train_loss=[0.5230889 4.913326 ], train_acc=[0.80855 0.1121 ]
72/100: train_loss=[0.52070165 4.839329  ], train_acc=[0.8083  0.11145]
74/100: train_loss=[0.52609074 4.8792415 ], train_acc=[0.8082 0.1078]
76/100: train_loss=[0.51773053 4.8612604 ], train_acc=[0.8121  0.10955]
78/100: train_loss=[0.52885175 4.9512215 ], train_acc=[0.80565 0.1079 ]
80/100: train_loss=[0.51346034 4.877568  ], train_acc=[0.8099  0.11245]
82/100: train_loss=[0.5141206 4.87453  ], train_acc=[0.81165 0.1113 ]
84/100: train_loss=[0.51343745 4.8539505 ], train_acc=[0.8104  0.11315]
86/100: train_loss=[0.51052153 4.9647093 ], train_acc=[0.81315 0.1091 ]
88/100: train_loss=[0.5180399 4.9348483], train_acc=[0.808  0.1093]
90/100: train_loss=[0.50662136 4.858642  ], train_acc=[0.8137 0.1071]
92/100: train_loss=[0.5170789 4.881269 ], train_acc=[0.80985 0.11105]
94/100: train_loss=[0.5347399 4.9452963], train_acc=[0.8045 0.1132]
96/100: train_loss=[0.50064486 4.837488  ], train_acc=[0.814  0.1108]
98/100: train_loss=[0.50097626 4.793798  ], train_acc=[0.8152 0.1126]
100/100: train_loss=[0.50639564 4.8837495 ], train_acc=[0.8134  0.11085]
**** Time taken for fashion_4 = 2029.2439661026
**** Time taken for fashion = 10073.824584960938
Preference Vector = [1.57079632e-04 9.99999988e-01]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[6.2005615  0.87868476], train_acc=[0.10485 0.6785 ]
2/100: train_loss=[6.478018  0.7807433], train_acc=[0.10675 0.71415]
4/100: train_loss=[6.751503   0.69893616], train_acc=[0.1057  0.73965]
6/100: train_loss=[7.0063972 0.6501703], train_acc=[0.1059 0.7557]
8/100: train_loss=[6.7970753  0.64863586], train_acc=[0.107   0.75225]
10/100: train_loss=[6.7604046 0.585971 ], train_acc=[0.11035 0.78345]
12/100: train_loss=[6.696082   0.57628167], train_acc=[0.1097 0.7825]
14/100: train_loss=[6.7566953 0.564124 ], train_acc=[0.10885 0.78965]
16/100: train_loss=[6.7630377 0.5420518], train_acc=[0.1078  0.79895]
18/100: train_loss=[6.484974  0.5329586], train_acc=[0.10765 0.8034 ]
20/100: train_loss=[6.6003203 0.5435238], train_acc=[0.10325 0.80015]
22/100: train_loss=[6.566068  0.5263951], train_acc=[0.10375 0.80305]
24/100: train_loss=[6.4413433 0.5077148], train_acc=[0.1058  0.81135]
26/100: train_loss=[6.448196  0.5057095], train_acc=[0.1056  0.81215]
28/100: train_loss=[6.564757   0.50032765], train_acc=[0.10505 0.816  ]
30/100: train_loss=[6.50244    0.49594918], train_acc=[0.1041 0.8141]
32/100: train_loss=[6.5185795 0.4860586], train_acc=[0.1047  0.82115]
34/100: train_loss=[6.3274245  0.47426122], train_acc=[0.10755 0.82595]
36/100: train_loss=[6.254454   0.47681072], train_acc=[0.10835 0.82205]
38/100: train_loss=[6.3469024  0.47532824], train_acc=[0.1039  0.82435]
40/100: train_loss=[6.3559856  0.46442124], train_acc=[0.10665 0.8297 ]
42/100: train_loss=[6.3519692  0.47110006], train_acc=[0.1061 0.8272]
44/100: train_loss=[6.2640166  0.46376696], train_acc=[0.10705 0.82695]
46/100: train_loss=[6.2310305 0.4646709], train_acc=[0.1078 0.827 ]
48/100: train_loss=[6.2708793 0.4521021], train_acc=[0.1065 0.8334]
50/100: train_loss=[6.1204643  0.45480695], train_acc=[0.1076  0.83165]
52/100: train_loss=[6.088403   0.44731808], train_acc=[0.1102 0.8355]
54/100: train_loss=[6.080054   0.44381455], train_acc=[0.1073 0.8376]
56/100: train_loss=[6.128841   0.43928394], train_acc=[0.10905 0.8376 ]
58/100: train_loss=[6.0328045  0.44160467], train_acc=[0.1078 0.8383]
60/100: train_loss=[6.0053     0.43555775], train_acc=[0.1075  0.84005]
62/100: train_loss=[6.1179376  0.44063511], train_acc=[0.10815 0.83715]
64/100: train_loss=[5.9815993  0.44173834], train_acc=[0.10795 0.83595]
66/100: train_loss=[6.104434   0.43971708], train_acc=[0.1091  0.83925]
68/100: train_loss=[5.892948 0.433068], train_acc=[0.10675 0.83975]
70/100: train_loss=[5.9200234  0.43456554], train_acc=[0.10795 0.84135]
72/100: train_loss=[5.9462996  0.42953616], train_acc=[0.10715 0.84025]
74/100: train_loss=[5.799391   0.43282267], train_acc=[0.10835 0.8404 ]
76/100: train_loss=[5.80153   0.4304769], train_acc=[0.10645 0.8402 ]
78/100: train_loss=[5.8452005  0.42877552], train_acc=[0.106  0.8438]
80/100: train_loss=[5.8166866  0.44044554], train_acc=[0.10715 0.83755]
82/100: train_loss=[5.7850723 0.4207814], train_acc=[0.1083 0.8447]
84/100: train_loss=[5.738446   0.42556444], train_acc=[0.1071  0.84165]
86/100: train_loss=[5.854898   0.42530176], train_acc=[0.10635 0.8433 ]
88/100: train_loss=[5.7827187 0.4231554], train_acc=[0.109  0.8447]
90/100: train_loss=[5.778747  0.4236826], train_acc=[0.1063  0.84435]
92/100: train_loss=[5.716302   0.42354468], train_acc=[0.1063 0.8452]
94/100: train_loss=[5.731874   0.42117456], train_acc=[0.106  0.8466]
96/100: train_loss=[5.6979795  0.42559144], train_acc=[0.10515 0.84295]
98/100: train_loss=[5.6940694  0.42025083], train_acc=[0.106  0.8454]
100/100: train_loss=[5.684701   0.41614965], train_acc=[0.10585 0.84725]
**** Time taken for fashion_and_mnist_0 = 2072.2263152599335
Preference Vector = [0.38275599 0.92384947]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[2.0383313 1.0245984], train_acc=[0.29   0.6174]
2/100: train_loss=[1.5842699 0.8507611], train_acc=[0.4465 0.6812]
4/100: train_loss=[1.1199442  0.74892026], train_acc=[0.621   0.72385]
6/100: train_loss=[0.9011282  0.70506316], train_acc=[0.69635 0.74295]
8/100: train_loss=[0.76922077 0.6460106 ], train_acc=[0.7422  0.75495]
10/100: train_loss=[0.70930785 0.6521919 ], train_acc=[0.76495 0.74725]
12/100: train_loss=[0.67798364 0.61408854], train_acc=[0.7742  0.76735]
14/100: train_loss=[0.61594594 0.6164412 ], train_acc=[0.79595 0.76565]
16/100: train_loss=[0.6536645  0.58367866], train_acc=[0.7859  0.77875]
18/100: train_loss=[0.5709089  0.57264584], train_acc=[0.81205 0.784  ]
20/100: train_loss=[0.5441943 0.5620628], train_acc=[0.82205 0.7891 ]
22/100: train_loss=[0.5252474 0.5652079], train_acc=[0.82955 0.78835]
24/100: train_loss=[0.506009  0.5585687], train_acc=[0.83385 0.7863 ]
26/100: train_loss=[0.50286126 0.54772943], train_acc=[0.8358 0.7939]
28/100: train_loss=[0.49238232 0.53788817], train_acc=[0.8356 0.8007]
30/100: train_loss=[0.47313768 0.5279457 ], train_acc=[0.84615 0.8055 ]
32/100: train_loss=[0.46538532 0.52957356], train_acc=[0.84945 0.80155]
34/100: train_loss=[0.49354708 0.5201883 ], train_acc=[0.83625 0.8046 ]
36/100: train_loss=[0.4534526  0.52508336], train_acc=[0.85225 0.804  ]
38/100: train_loss=[0.4489713 0.5358963], train_acc=[0.853  0.7974]
40/100: train_loss=[0.44413054 0.5131281 ], train_acc=[0.85495 0.8092 ]
42/100: train_loss=[0.42536443 0.5026846 ], train_acc=[0.8625 0.8143]
44/100: train_loss=[0.42761594 0.4985255 ], train_acc=[0.8638 0.8125]
46/100: train_loss=[0.42881113 0.5173932 ], train_acc=[0.86165 0.8094 ]
48/100: train_loss=[0.4135692  0.49907777], train_acc=[0.86645 0.8138 ]
50/100: train_loss=[0.43439022 0.4939192 ], train_acc=[0.85995 0.8168 ]
52/100: train_loss=[0.40606955 0.5149191 ], train_acc=[0.86885 0.81045]
54/100: train_loss=[0.41207278 0.49827233], train_acc=[0.86605 0.8124 ]
56/100: train_loss=[0.39612174 0.50267917], train_acc=[0.8721 0.8122]
58/100: train_loss=[0.41308147 0.48127875], train_acc=[0.86705 0.8223 ]
60/100: train_loss=[0.40230867 0.49739212], train_acc=[0.87075 0.8139 ]
62/100: train_loss=[0.40094563 0.4952451 ], train_acc=[0.8709  0.81805]
64/100: train_loss=[0.39534664 0.48196992], train_acc=[0.8691  0.82055]
66/100: train_loss=[0.4094952  0.47513402], train_acc=[0.86705 0.8261 ]
68/100: train_loss=[0.41273853 0.4683618 ], train_acc=[0.86545 0.8283 ]
70/100: train_loss=[0.37650815 0.4704469 ], train_acc=[0.8791  0.82905]
72/100: train_loss=[0.39453793 0.52135205], train_acc=[0.87335 0.81115]
74/100: train_loss=[0.38101056 0.48139367], train_acc=[0.876   0.82615]
76/100: train_loss=[0.38145468 0.4712894 ], train_acc=[0.8761  0.82815]
78/100: train_loss=[0.3748896 0.4699903], train_acc=[0.87825 0.826  ]
80/100: train_loss=[0.36705726 0.46591324], train_acc=[0.8822 0.8301]
82/100: train_loss=[0.37689096 0.45685577], train_acc=[0.87845 0.8332 ]
84/100: train_loss=[0.37372243 0.46554694], train_acc=[0.8784 0.8303]
86/100: train_loss=[0.36647278 0.46777445], train_acc=[0.8814  0.82835]
88/100: train_loss=[0.36549702 0.46067405], train_acc=[0.8826  0.83355]
90/100: train_loss=[0.36353287 0.46448934], train_acc=[0.88355 0.83275]
92/100: train_loss=[0.3586142  0.46573606], train_acc=[0.88515 0.8296 ]
94/100: train_loss=[0.35681042 0.45298448], train_acc=[0.88555 0.83615]
96/100: train_loss=[0.3545666  0.46078497], train_acc=[0.8867  0.82985]
98/100: train_loss=[0.35985348 0.45832643], train_acc=[0.88255 0.83205]
100/100: train_loss=[0.3618239  0.46077734], train_acc=[0.88   0.8331]
**** Time taken for fashion_and_mnist_1 = 2079.6592111587524
Preference Vector = [0.70710678 0.70710678]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.4771752 1.1629765], train_acc=[0.49775 0.5727 ]
2/100: train_loss=[1.0711124 0.9113383], train_acc=[0.63965 0.6614 ]
4/100: train_loss=[0.7458256 0.7853744], train_acc=[0.75225 0.7127 ]
6/100: train_loss=[0.65392387 0.716306  ], train_acc=[0.78365 0.735  ]
8/100: train_loss=[0.58647835 0.6827373 ], train_acc=[0.80405 0.7452 ]
10/100: train_loss=[0.49973372 0.6681937 ], train_acc=[0.8375 0.7511]
12/100: train_loss=[0.48886907 0.6518078 ], train_acc=[0.8406 0.754 ]
14/100: train_loss=[0.4520313 0.6328232], train_acc=[0.8548 0.7624]
16/100: train_loss=[0.43379694 0.6067095 ], train_acc=[0.85855 0.77635]
18/100: train_loss=[0.4294838  0.61098003], train_acc=[0.8603  0.77445]
20/100: train_loss=[0.39431092 0.5946598 ], train_acc=[0.87235 0.77865]
22/100: train_loss=[0.3758242  0.59816897], train_acc=[0.8786  0.78025]
24/100: train_loss=[0.36447462 0.5786519 ], train_acc=[0.8811 0.7866]
26/100: train_loss=[0.36178762 0.5618255 ], train_acc=[0.8819 0.7931]
28/100: train_loss=[0.3606019 0.5829354], train_acc=[0.88355 0.7841 ]
30/100: train_loss=[0.3510044 0.5669548], train_acc=[0.8872 0.789 ]
32/100: train_loss=[0.33990055 0.559954  ], train_acc=[0.88935 0.7966 ]
34/100: train_loss=[0.33009163 0.54343843], train_acc=[0.89275 0.80025]
36/100: train_loss=[0.3492202 0.5458491], train_acc=[0.88545 0.801  ]
38/100: train_loss=[0.32755047 0.54157746], train_acc=[0.89385 0.8012 ]
40/100: train_loss=[0.31577715 0.55020237], train_acc=[0.89715 0.7961 ]
42/100: train_loss=[0.31023672 0.5540008 ], train_acc=[0.8995  0.79815]
44/100: train_loss=[0.3086264  0.52750516], train_acc=[0.90005 0.8067 ]
46/100: train_loss=[0.304943   0.53392637], train_acc=[0.901  0.8051]
48/100: train_loss=[0.30644146 0.52504754], train_acc=[0.901   0.80915]
50/100: train_loss=[0.31440368 0.521602  ], train_acc=[0.89855 0.81115]
52/100: train_loss=[0.3003157  0.52031726], train_acc=[0.90305 0.80995]
54/100: train_loss=[0.29799035 0.5234407 ], train_acc=[0.9042 0.8068]
56/100: train_loss=[0.30047902 0.52356887], train_acc=[0.90385 0.8113 ]
58/100: train_loss=[0.29455373 0.52275753], train_acc=[0.9058  0.80895]
60/100: train_loss=[0.2881851 0.5095631], train_acc=[0.9062 0.8148]
62/100: train_loss=[0.29238057 0.5139672 ], train_acc=[0.90775 0.81175]
64/100: train_loss=[0.284244   0.50517267], train_acc=[0.91005 0.81435]
66/100: train_loss=[0.27956086 0.51289177], train_acc=[0.91065 0.8117 ]
68/100: train_loss=[0.29143757 0.5048395 ], train_acc=[0.90675 0.81865]
70/100: train_loss=[0.2920642 0.5092849], train_acc=[0.9066  0.81365]
72/100: train_loss=[0.27776805 0.5031375 ], train_acc=[0.9109  0.81545]
74/100: train_loss=[0.28241315 0.5128664 ], train_acc=[0.9089 0.8141]
76/100: train_loss=[0.28258237 0.5015253 ], train_acc=[0.90895 0.8167 ]
78/100: train_loss=[0.2714973 0.4985006], train_acc=[0.9131 0.8175]
80/100: train_loss=[0.26724452 0.49964356], train_acc=[0.91555 0.81715]
82/100: train_loss=[0.27251643 0.5074877 ], train_acc=[0.9154  0.81395]
84/100: train_loss=[0.26510522 0.49797857], train_acc=[0.9164  0.81655]
86/100: train_loss=[0.27247742 0.49835804], train_acc=[0.9117 0.8138]
88/100: train_loss=[0.2725521 0.5103423], train_acc=[0.91335 0.81095]
90/100: train_loss=[0.2613414  0.49712044], train_acc=[0.9184  0.81645]
92/100: train_loss=[0.259818  0.4939089], train_acc=[0.9189  0.81965]
94/100: train_loss=[0.26328462 0.50900215], train_acc=[0.91625 0.8127 ]
96/100: train_loss=[0.26612672 0.4863727 ], train_acc=[0.9149  0.82055]
98/100: train_loss=[0.31298822 0.51353717], train_acc=[0.90205 0.8127 ]
100/100: train_loss=[0.26174483 0.4908596 ], train_acc=[0.9177  0.81975]
**** Time taken for fashion_and_mnist_2 = 2079.041469812393
Preference Vector = [0.92384947 0.38275599]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.1904331 1.3950652], train_acc=[0.6062  0.48415]
2/100: train_loss=[0.88698405 1.1102554 ], train_acc=[0.7112 0.588 ]
4/100: train_loss=[0.6845786  0.91963834], train_acc=[0.7731  0.66085]
6/100: train_loss=[0.5249496  0.84345895], train_acc=[0.8306  0.68685]
8/100: train_loss=[0.5143034 0.800347 ], train_acc=[0.8325  0.69615]
10/100: train_loss=[0.4367137  0.75830454], train_acc=[0.8599 0.7146]
12/100: train_loss=[0.43446147 0.740429  ], train_acc=[0.8575 0.7223]
14/100: train_loss=[0.4101185 0.7217821], train_acc=[0.86895 0.7294 ]
16/100: train_loss=[0.37279513 0.69937724], train_acc=[0.8815 0.7369]
18/100: train_loss=[0.3553762 0.6881557], train_acc=[0.8866  0.74325]
20/100: train_loss=[0.334767   0.67689335], train_acc=[0.89225 0.7477 ]
22/100: train_loss=[0.31392953 0.6707204 ], train_acc=[0.89885 0.74585]
24/100: train_loss=[0.29975343 0.65945745], train_acc=[0.90455 0.7502 ]
26/100: train_loss=[0.30462068 0.66595966], train_acc=[0.90155 0.74665]
28/100: train_loss=[0.2983581 0.6478794], train_acc=[0.9027 0.7567]
30/100: train_loss=[0.2799574 0.642137 ], train_acc=[0.9106 0.7592]
32/100: train_loss=[0.28085676 0.63637763], train_acc=[0.9094  0.76015]
34/100: train_loss=[0.27735555 0.6408263 ], train_acc=[0.9088 0.7551]
36/100: train_loss=[0.27847716 0.6274095 ], train_acc=[0.90875 0.76385]
38/100: train_loss=[0.26299563 0.6302201 ], train_acc=[0.91765 0.7623 ]
40/100: train_loss=[0.28216144 0.6223907 ], train_acc=[0.9091 0.768 ]
42/100: train_loss=[0.3017711 0.6266811], train_acc=[0.90295 0.7636 ]
44/100: train_loss=[0.263844 0.616714], train_acc=[0.91455 0.77165]
46/100: train_loss=[0.27011794 0.6171447 ], train_acc=[0.9119 0.7641]
48/100: train_loss=[0.2457077 0.6138557], train_acc=[0.9208  0.77145]
50/100: train_loss=[0.24555562 0.60732305], train_acc=[0.9216  0.77415]
52/100: train_loss=[0.23962939 0.6019307 ], train_acc=[0.9225  0.77485]
54/100: train_loss=[0.24438375 0.59576887], train_acc=[0.9208 0.7759]
56/100: train_loss=[0.2429918  0.59648705], train_acc=[0.92305 0.77685]
58/100: train_loss=[0.23784633 0.5894129 ], train_acc=[0.92385 0.779  ]
60/100: train_loss=[0.23642294 0.6114844 ], train_acc=[0.92525 0.77255]
62/100: train_loss=[0.24454388 0.58986974], train_acc=[0.9219  0.77765]
64/100: train_loss=[0.22917202 0.58432585], train_acc=[0.9268  0.78205]
66/100: train_loss=[0.227841  0.5807156], train_acc=[0.9262 0.7835]
68/100: train_loss=[0.22814006 0.5786014 ], train_acc=[0.92715 0.78335]
70/100: train_loss=[0.22930291 0.5761589 ], train_acc=[0.9269 0.7845]
72/100: train_loss=[0.22645861 0.5809439 ], train_acc=[0.92705 0.7821 ]
74/100: train_loss=[0.22853543 0.570059  ], train_acc=[0.9269 0.7863]
76/100: train_loss=[0.21584004 0.5699433 ], train_acc=[0.93125 0.7888 ]
78/100: train_loss=[0.22095948 0.56905335], train_acc=[0.9294 0.7867]
80/100: train_loss=[0.24761775 0.58519584], train_acc=[0.9206  0.78175]
82/100: train_loss=[0.22046147 0.5725696 ], train_acc=[0.9302  0.78595]
84/100: train_loss=[0.24017535 0.5622197 ], train_acc=[0.92145 0.7885 ]
86/100: train_loss=[0.21637304 0.5616678 ], train_acc=[0.9319 0.7899]
88/100: train_loss=[0.22444735 0.57199234], train_acc=[0.92695 0.7852 ]
90/100: train_loss=[0.21423876 0.5614224 ], train_acc=[0.93215 0.7896 ]
92/100: train_loss=[0.21191429 0.5574636 ], train_acc=[0.93345 0.79175]
94/100: train_loss=[0.21427122 0.5543393 ], train_acc=[0.9325  0.79335]
96/100: train_loss=[0.22168346 0.5621017 ], train_acc=[0.93075 0.78915]
98/100: train_loss=[0.2144887 0.5540651], train_acc=[0.9305  0.79265]
100/100: train_loss=[0.2513209  0.56110054], train_acc=[0.91945 0.79095]
**** Time taken for fashion_and_mnist_3 = 2066.481764316559
Preference Vector = [9.99999988e-01 1.57079632e-04]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.0962272 5.1819396], train_acc=[0.6289 0.1062]
2/100: train_loss=[0.8252678 6.200764 ], train_acc=[0.7245  0.10335]
4/100: train_loss=[0.6598202 7.3658323], train_acc=[0.781  0.1014]
6/100: train_loss=[0.5220306 8.06849  ], train_acc=[0.83095 0.1017 ]
8/100: train_loss=[0.46588647 8.1213455 ], train_acc=[0.8464 0.0945]
10/100: train_loss=[0.38468218 8.637769  ], train_acc=[0.8731  0.09345]
12/100: train_loss=[0.3511924 8.565331 ], train_acc=[0.8858 0.099 ]
14/100: train_loss=[0.33088106 8.527673  ], train_acc=[0.8933 0.0981]
16/100: train_loss=[0.3056999 8.801713 ], train_acc=[0.9014 0.0987]
18/100: train_loss=[0.30858296 9.345502  ], train_acc=[0.90125 0.1037 ]
20/100: train_loss=[0.26715022 9.191076  ], train_acc=[0.9149  0.09765]
22/100: train_loss=[0.25807518 8.920081  ], train_acc=[0.91825 0.0984 ]
24/100: train_loss=[0.25055984 8.974773  ], train_acc=[0.92085 0.09885]
26/100: train_loss=[0.24423131 9.050153  ], train_acc=[0.9231  0.09885]
28/100: train_loss=[0.24008319 9.015558  ], train_acc=[0.92295 0.0977 ]
30/100: train_loss=[0.23337224 8.784646  ], train_acc=[0.92615 0.0981 ]
32/100: train_loss=[0.22334716 8.9034    ], train_acc=[0.9294 0.0997]
34/100: train_loss=[0.21708226 8.851692  ], train_acc=[0.93175 0.0997 ]
36/100: train_loss=[0.2370036 9.303366 ], train_acc=[0.92315 0.10345]
38/100: train_loss=[0.21088472 8.911458  ], train_acc=[0.9332  0.10145]
40/100: train_loss=[0.20574978 8.931853  ], train_acc=[0.93605 0.1009 ]
42/100: train_loss=[0.20624498 8.801085  ], train_acc=[0.93465 0.1035 ]
44/100: train_loss=[0.20244166 8.699198  ], train_acc=[0.93665 0.1045 ]
46/100: train_loss=[0.19912426 8.884602  ], train_acc=[0.93695 0.10275]
48/100: train_loss=[0.20568994 8.646345  ], train_acc=[0.9352  0.10085]
50/100: train_loss=[0.19282752 8.758615  ], train_acc=[0.9403  0.10485]
52/100: train_loss=[0.18907146 8.683325  ], train_acc=[0.9402 0.1049]
54/100: train_loss=[0.19206864 8.526024  ], train_acc=[0.93955 0.1045 ]
56/100: train_loss=[0.1855433 8.717167 ], train_acc=[0.9415 0.1043]
58/100: train_loss=[0.18499619 8.688317  ], train_acc=[0.9413 0.1047]
60/100: train_loss=[0.1812621 8.796829 ], train_acc=[0.94315 0.10555]
62/100: train_loss=[0.18292889 8.724177  ], train_acc=[0.94245 0.1048 ]
64/100: train_loss=[0.17905676 8.804518  ], train_acc=[0.9427  0.10545]
66/100: train_loss=[0.18208306 8.671163  ], train_acc=[0.94255 0.106  ]
68/100: train_loss=[0.1805762 8.801176 ], train_acc=[0.94265 0.108  ]
70/100: train_loss=[0.17490196 8.5927725 ], train_acc=[0.9457 0.107 ]
72/100: train_loss=[0.17439517 8.701548  ], train_acc=[0.94575 0.10785]
74/100: train_loss=[0.17575243 8.691887  ], train_acc=[0.9452  0.10905]
76/100: train_loss=[0.17229673 8.863391  ], train_acc=[0.94645 0.10755]
78/100: train_loss=[0.18150388 8.546785  ], train_acc=[0.9433  0.10825]
80/100: train_loss=[0.17934379 8.801559  ], train_acc=[0.94385 0.108  ]
82/100: train_loss=[0.17812721 8.712923  ], train_acc=[0.94495 0.10855]
84/100: train_loss=[0.16973603 8.77784   ], train_acc=[0.9478 0.1083]
86/100: train_loss=[0.18358935 8.301612  ], train_acc=[0.94335 0.1068 ]
88/100: train_loss=[0.17391145 8.718468  ], train_acc=[0.94585 0.10685]
90/100: train_loss=[0.16906822 8.567147  ], train_acc=[0.94785 0.1092 ]
92/100: train_loss=[0.17679454 8.614618  ], train_acc=[0.9446 0.108 ]
94/100: train_loss=[0.17063949 8.677889  ], train_acc=[0.94765 0.10915]
96/100: train_loss=[0.16667497 8.532556  ], train_acc=[0.94875 0.10865]
98/100: train_loss=[0.16840507 8.517849  ], train_acc=[0.9478  0.10795]
100/100: train_loss=[0.17182544 8.560214  ], train_acc=[0.9474  0.11105]
**** Time taken for fashion_and_mnist_4 = 2051.938383579254
**** Time taken for fashion_and_mnist = 10349.399211406708
