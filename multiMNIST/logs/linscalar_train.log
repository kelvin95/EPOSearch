Preference Vector = [1.57079632e-04 9.99999988e-01]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[6.4971247 1.0610186], train_acc=[0.10275 0.6444 ]
2/100: train_loss=[7.3275185 0.8126287], train_acc=[0.1013 0.7321]
4/100: train_loss=[8.059224  0.6704121], train_acc=[0.10495 0.7725 ]
6/100: train_loss=[7.443783  0.5973038], train_acc=[0.09995 0.80115]
8/100: train_loss=[7.7624593  0.54660386], train_acc=[0.10145 0.81645]
10/100: train_loss=[7.712397   0.53253347], train_acc=[0.1027  0.82095]
12/100: train_loss=[8.007528   0.48340395], train_acc=[0.1006  0.83685]
14/100: train_loss=[7.91468    0.46371168], train_acc=[0.10135 0.84685]
16/100: train_loss=[7.6152205 0.4439566], train_acc=[0.09735 0.8526 ]
18/100: train_loss=[7.7280455  0.43779075], train_acc=[0.09935 0.8528 ]
20/100: train_loss=[7.5307865  0.42365614], train_acc=[0.0996  0.86035]
22/100: train_loss=[7.287788   0.44549027], train_acc=[0.09695 0.85315]
24/100: train_loss=[7.1433153  0.41331723], train_acc=[0.09625 0.86235]
26/100: train_loss=[7.14458   0.3931352], train_acc=[0.0968  0.86845]
28/100: train_loss=[7.342362   0.39483872], train_acc=[0.09495 0.86905]
30/100: train_loss=[7.0682454 0.3834207], train_acc=[0.0948  0.87435]
32/100: train_loss=[7.118713  0.3863784], train_acc=[0.0961 0.8717]
34/100: train_loss=[7.2822266  0.38295287], train_acc=[0.093   0.87105]
36/100: train_loss=[7.075336   0.38620564], train_acc=[0.09295 0.87125]
38/100: train_loss=[6.9489965  0.36246756], train_acc=[0.0935 0.8797]
40/100: train_loss=[6.994827   0.37378207], train_acc=[0.09435 0.8758 ]
42/100: train_loss=[6.9853487 0.3606468], train_acc=[0.0953  0.87935]
44/100: train_loss=[6.954911   0.36886406], train_acc=[0.0945  0.87875]
46/100: train_loss=[6.8566976 0.3699101], train_acc=[0.09205 0.8787 ]
48/100: train_loss=[6.818038   0.36821973], train_acc=[0.09445 0.8786 ]
50/100: train_loss=[6.9335947  0.36252388], train_acc=[0.09415 0.87965]
52/100: train_loss=[6.849782   0.34585673], train_acc=[0.09375 0.88485]
54/100: train_loss=[6.8670206  0.36594322], train_acc=[0.09565 0.87815]
56/100: train_loss=[6.769221   0.35026917], train_acc=[0.0946 0.8834]
58/100: train_loss=[6.912114   0.34795195], train_acc=[0.09545 0.88275]
60/100: train_loss=[6.7420177  0.34077814], train_acc=[0.0947  0.88645]
62/100: train_loss=[6.778017   0.34799835], train_acc=[0.09595 0.88315]
64/100: train_loss=[6.6483436  0.35437977], train_acc=[0.09485 0.88225]
66/100: train_loss=[6.7720942  0.33974913], train_acc=[0.0965 0.8885]
68/100: train_loss=[6.6081724  0.33356145], train_acc=[0.0973 0.8886]
70/100: train_loss=[6.804073   0.34479418], train_acc=[0.09615 0.88645]
72/100: train_loss=[6.6662946  0.33551836], train_acc=[0.0987  0.88735]
74/100: train_loss=[6.542793   0.33615717], train_acc=[0.09745 0.889  ]
76/100: train_loss=[6.606059   0.33247957], train_acc=[0.10015 0.88945]
78/100: train_loss=[6.59505   0.3326668], train_acc=[0.0975 0.8893]
80/100: train_loss=[6.536204   0.33258522], train_acc=[0.097   0.88805]
82/100: train_loss=[6.5582457 0.3493914], train_acc=[0.10095 0.88145]
84/100: train_loss=[6.408482   0.33112326], train_acc=[0.0987  0.88825]
86/100: train_loss=[6.443046   0.33074084], train_acc=[0.10125 0.8883 ]
88/100: train_loss=[6.4629703 0.3294227], train_acc=[0.1     0.89015]
90/100: train_loss=[6.496938   0.33175159], train_acc=[0.10005 0.8894 ]
92/100: train_loss=[6.4623036  0.32584074], train_acc=[0.1016 0.8899]
94/100: train_loss=[6.488682 0.32375 ], train_acc=[0.10125 0.89025]
96/100: train_loss=[6.4571514  0.32331458], train_acc=[0.1006 0.8922]
98/100: train_loss=[6.514505   0.34777805], train_acc=[0.1007  0.88355]
100/100: train_loss=[6.3992233 0.327715 ], train_acc=[0.10395 0.8893 ]
**** Time taken for mnist_0 = 3378.3894832134247
Preference Vector = [0.38275599 0.92384947]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[2.190008  1.6595604], train_acc=[0.24605 0.4422 ]
2/100: train_loss=[1.4150805 1.0698317], train_acc=[0.5217 0.6452]
4/100: train_loss=[0.92434704 0.76866084], train_acc=[0.69365 0.74525]
6/100: train_loss=[0.70196474 0.6103284 ], train_acc=[0.77105 0.79575]
8/100: train_loss=[0.6317693  0.58514553], train_acc=[0.7936 0.8071]
10/100: train_loss=[0.57647    0.50849175], train_acc=[0.81035 0.8299 ]
12/100: train_loss=[0.55159616 0.50102574], train_acc=[0.81915 0.831  ]
14/100: train_loss=[0.50821525 0.47856325], train_acc=[0.8338  0.83775]
16/100: train_loss=[0.49496403 0.4704067 ], train_acc=[0.83825 0.83985]
18/100: train_loss=[0.47168845 0.4382659 ], train_acc=[0.8448  0.85145]
20/100: train_loss=[0.45894536 0.43810448], train_acc=[0.8496 0.8501]
22/100: train_loss=[0.44599813 0.4191225 ], train_acc=[0.85305 0.85925]
24/100: train_loss=[0.43519095 0.41432443], train_acc=[0.8566  0.86155]
26/100: train_loss=[0.42479348 0.40228155], train_acc=[0.8618  0.86495]
28/100: train_loss=[0.4194772 0.4187431], train_acc=[0.86425 0.85885]
30/100: train_loss=[0.40798652 0.39125976], train_acc=[0.866   0.86775]
32/100: train_loss=[0.4047393  0.38337985], train_acc=[0.8673 0.8703]
34/100: train_loss=[0.3991328  0.41942018], train_acc=[0.87115 0.85685]
36/100: train_loss=[0.3908327  0.38109615], train_acc=[0.8711 0.869 ]
38/100: train_loss=[0.39533004 0.37047702], train_acc=[0.8713  0.87155]
40/100: train_loss=[0.3843903  0.36954162], train_acc=[0.8745 0.8758]
42/100: train_loss=[0.38231766 0.37729916], train_acc=[0.8749 0.8741]
44/100: train_loss=[0.37963125 0.374807  ], train_acc=[0.8758  0.87165]
46/100: train_loss=[0.37883705 0.35962248], train_acc=[0.87725 0.87735]
48/100: train_loss=[0.37120262 0.35624808], train_acc=[0.87985 0.8782 ]
50/100: train_loss=[0.3657241  0.35034052], train_acc=[0.87995 0.8795 ]
52/100: train_loss=[0.36432138 0.3542483 ], train_acc=[0.881   0.87935]
54/100: train_loss=[0.3591031  0.34849513], train_acc=[0.88225 0.8812 ]
56/100: train_loss=[0.35485187 0.34601754], train_acc=[0.88445 0.8816 ]
58/100: train_loss=[0.35415146 0.3555945 ], train_acc=[0.88355 0.88005]
60/100: train_loss=[0.35206905 0.34871188], train_acc=[0.88585 0.88185]
62/100: train_loss=[0.35177818 0.34179327], train_acc=[0.8848  0.88425]
64/100: train_loss=[0.34908482 0.34033737], train_acc=[0.88715 0.88505]
66/100: train_loss=[0.34538242 0.33988315], train_acc=[0.886  0.8859]
68/100: train_loss=[0.34428358 0.3377669 ], train_acc=[0.8876 0.886 ]
70/100: train_loss=[0.34424317 0.33202928], train_acc=[0.8879 0.889 ]
72/100: train_loss=[0.34363014 0.3360493 ], train_acc=[0.888  0.8872]
74/100: train_loss=[0.34806573 0.32869947], train_acc=[0.8861 0.8898]
76/100: train_loss=[0.3452685  0.33143714], train_acc=[0.88685 0.88955]
78/100: train_loss=[0.34322268 0.34216842], train_acc=[0.8894  0.88655]
80/100: train_loss=[0.34449297 0.3511072 ], train_acc=[0.88755 0.8832 ]
82/100: train_loss=[0.33823693 0.33400026], train_acc=[0.88925 0.8886 ]
84/100: train_loss=[0.3346408  0.33013526], train_acc=[0.89045 0.8881 ]
86/100: train_loss=[0.33387828 0.32404646], train_acc=[0.8905  0.89245]
88/100: train_loss=[0.33230653 0.33474737], train_acc=[0.8929  0.88825]
90/100: train_loss=[0.3317771 0.3278123], train_acc=[0.89265 0.8904 ]
92/100: train_loss=[0.33461484 0.33015788], train_acc=[0.8908 0.8899]
94/100: train_loss=[0.32974738 0.3347513 ], train_acc=[0.8919 0.8885]
96/100: train_loss=[0.32987097 0.32485315], train_acc=[0.89265 0.8922 ]
98/100: train_loss=[0.32764176 0.32355887], train_acc=[0.89405 0.8921 ]
100/100: train_loss=[0.33230925 0.3227641 ], train_acc=[0.89145 0.8928 ]
**** Time taken for mnist_1 = 2237.9008464813232
Preference Vector = [0.70710678 0.70710678]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.6833678 1.8044716], train_acc=[0.43355 0.38495]
2/100: train_loss=[1.1034908 1.2870557], train_acc=[0.63045 0.5634 ]
4/100: train_loss=[0.71496433 0.84260803], train_acc=[0.76185 0.7188 ]
6/100: train_loss=[0.6133753 0.7225405], train_acc=[0.7986 0.757 ]
8/100: train_loss=[0.54792476 0.65333074], train_acc=[0.81835 0.7787 ]
10/100: train_loss=[0.50466037 0.60612285], train_acc=[0.83455 0.79815]
12/100: train_loss=[0.47742733 0.56041145], train_acc=[0.84445 0.8138 ]
14/100: train_loss=[0.4609009  0.53952146], train_acc=[0.8519 0.8194]
16/100: train_loss=[0.443866  0.5098696], train_acc=[0.85525 0.8297 ]
18/100: train_loss=[0.42969945 0.49944055], train_acc=[0.86075 0.83315]
20/100: train_loss=[0.42164573 0.48793703], train_acc=[0.86305 0.8372 ]
22/100: train_loss=[0.42840907 0.47500128], train_acc=[0.86155 0.84305]
24/100: train_loss=[0.39922497 0.46083233], train_acc=[0.87025 0.8466 ]
26/100: train_loss=[0.40079457 0.46768808], train_acc=[0.871  0.8437]
28/100: train_loss=[0.38489985 0.443879  ], train_acc=[0.87585 0.8528 ]
30/100: train_loss=[0.3868932  0.44386622], train_acc=[0.8743  0.85115]
32/100: train_loss=[0.3724659  0.43790263], train_acc=[0.87805 0.8534 ]
34/100: train_loss=[0.37034234 0.42470503], train_acc=[0.87985 0.85825]
36/100: train_loss=[0.36654213 0.41842395], train_acc=[0.88095 0.8617 ]
38/100: train_loss=[0.36451268 0.4225495 ], train_acc=[0.88305 0.857  ]
40/100: train_loss=[0.35976803 0.411459  ], train_acc=[0.88265 0.8624 ]
42/100: train_loss=[0.3603456 0.4148776], train_acc=[0.88385 0.8616 ]
44/100: train_loss=[0.35290098 0.40271425], train_acc=[0.88425 0.8653 ]
46/100: train_loss=[0.34802166 0.40325016], train_acc=[0.8872 0.8659]
48/100: train_loss=[0.34442115 0.39550146], train_acc=[0.88915 0.86865]
50/100: train_loss=[0.34400603 0.4077746 ], train_acc=[0.889   0.86395]
52/100: train_loss=[0.341004   0.41030094], train_acc=[0.89125 0.8628 ]
54/100: train_loss=[0.34142116 0.38848   ], train_acc=[0.8887  0.87035]
56/100: train_loss=[0.33974785 0.3883496 ], train_acc=[0.89015 0.87125]
58/100: train_loss=[0.34113947 0.39159188], train_acc=[0.8901  0.87045]
60/100: train_loss=[0.33109075 0.38379437], train_acc=[0.89355 0.8714 ]
62/100: train_loss=[0.33203784 0.40028864], train_acc=[0.8915  0.86715]
64/100: train_loss=[0.33922806 0.38519457], train_acc=[0.8901 0.8744]
66/100: train_loss=[0.330457 0.382717], train_acc=[0.8939  0.87405]
68/100: train_loss=[0.32276917 0.38187113], train_acc=[0.8955  0.87165]
70/100: train_loss=[0.3243606 0.3751269], train_acc=[0.89515 0.87595]
72/100: train_loss=[0.32717398 0.3829207 ], train_acc=[0.8925  0.87375]
74/100: train_loss=[0.3201687  0.37503874], train_acc=[0.8966  0.87615]
76/100: train_loss=[0.33358485 0.37243262], train_acc=[0.8919  0.87715]
78/100: train_loss=[0.32431737 0.37172708], train_acc=[0.89505 0.8775 ]
80/100: train_loss=[0.32249203 0.37958258], train_acc=[0.8944 0.8738]
82/100: train_loss=[0.32110694 0.38139793], train_acc=[0.89655 0.8725 ]
84/100: train_loss=[0.31822664 0.3640229 ], train_acc=[0.89545 0.8792 ]
86/100: train_loss=[0.32878065 0.3667129 ], train_acc=[0.8933  0.87835]
88/100: train_loss=[0.31310984 0.36404315], train_acc=[0.8991  0.87945]
90/100: train_loss=[0.31296366 0.3706257 ], train_acc=[0.89725 0.87875]
92/100: train_loss=[0.30919054 0.3656636 ], train_acc=[0.9007  0.87785]
94/100: train_loss=[0.32565293 0.368249  ], train_acc=[0.8956 0.8779]
96/100: train_loss=[0.30942756 0.360211  ], train_acc=[0.89945 0.88175]
98/100: train_loss=[0.32434595 0.35841572], train_acc=[0.89535 0.8825 ]
100/100: train_loss=[0.3135291 0.3666626], train_acc=[0.8977 0.8765]
**** Time taken for mnist_2 = 2151.8989713191986
Preference Vector = [0.92384947 0.38275599]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.1596073 1.9247692], train_acc=[0.61735 0.33355]
2/100: train_loss=[0.83379155 1.5079495 ], train_acc=[0.7277 0.4964]
4/100: train_loss=[0.6446549 1.0734969], train_acc=[0.7899 0.6418]
6/100: train_loss=[0.5200216  0.90057665], train_acc=[0.83205 0.6986 ]
8/100: train_loss=[0.49086434 0.7884622 ], train_acc=[0.84065 0.7328 ]
10/100: train_loss=[0.4441471  0.72544724], train_acc=[0.85585 0.7535 ]
12/100: train_loss=[0.46129164 0.6706348 ], train_acc=[0.8476 0.7735]
14/100: train_loss=[0.41965997 0.63871485], train_acc=[0.8639 0.7809]
16/100: train_loss=[0.3883414 0.6084691], train_acc=[0.8742  0.79455]
18/100: train_loss=[0.46183246 0.5908114 ], train_acc=[0.84835 0.80205]
20/100: train_loss=[0.3728471 0.5598966], train_acc=[0.87775 0.8114 ]
22/100: train_loss=[0.35961026 0.54378   ], train_acc=[0.88215 0.81735]
24/100: train_loss=[0.37395865 0.53301644], train_acc=[0.88065 0.82035]
26/100: train_loss=[0.34776935 0.52106726], train_acc=[0.88455 0.82455]
28/100: train_loss=[0.3759515 0.5101404], train_acc=[0.87945 0.82615]
30/100: train_loss=[0.32336634 0.49476513], train_acc=[0.8937 0.8344]
32/100: train_loss=[0.32703382 0.4794984 ], train_acc=[0.89215 0.8385 ]
34/100: train_loss=[0.32889274 0.47129664], train_acc=[0.893  0.8432]
36/100: train_loss=[0.32209292 0.46341273], train_acc=[0.89175 0.84515]
38/100: train_loss=[0.3080597 0.4570665], train_acc=[0.8979 0.8477]
40/100: train_loss=[0.3045617 0.4553523], train_acc=[0.89725 0.84705]
42/100: train_loss=[0.2965028 0.4485299], train_acc=[0.90105 0.85035]
44/100: train_loss=[0.29407114 0.44297343], train_acc=[0.9043 0.8505]
46/100: train_loss=[0.29355544 0.4366573 ], train_acc=[0.90335 0.85495]
48/100: train_loss=[0.28799352 0.43315798], train_acc=[0.904  0.8531]
50/100: train_loss=[0.28433183 0.4274325 ], train_acc=[0.90525 0.85655]
52/100: train_loss=[0.29132834 0.4235895 ], train_acc=[0.9033  0.85785]
54/100: train_loss=[0.30341196 0.41983718], train_acc=[0.90085 0.8591 ]
56/100: train_loss=[0.28141397 0.42154503], train_acc=[0.90645 0.8585 ]
58/100: train_loss=[0.2877158 0.4210297], train_acc=[0.90575 0.85885]
60/100: train_loss=[0.29448754 0.41202277], train_acc=[0.90075 0.86055]
62/100: train_loss=[0.27279267 0.40817627], train_acc=[0.90915 0.8639 ]
64/100: train_loss=[0.2760095 0.4051705], train_acc=[0.90855 0.8644 ]
66/100: train_loss=[0.2847406 0.4081037], train_acc=[0.90605 0.8645 ]
68/100: train_loss=[0.26658362 0.39874393], train_acc=[0.91015 0.8636 ]
70/100: train_loss=[0.28145522 0.40296072], train_acc=[0.9058  0.86275]
72/100: train_loss=[0.27326187 0.4000038 ], train_acc=[0.9077 0.8651]
74/100: train_loss=[0.26656216 0.3899022 ], train_acc=[0.91105 0.86815]
76/100: train_loss=[0.26721075 0.3957422 ], train_acc=[0.91215 0.868  ]
78/100: train_loss=[0.26141697 0.38568372], train_acc=[0.91335 0.87   ]
80/100: train_loss=[0.2789828  0.39598495], train_acc=[0.90935 0.8657 ]
82/100: train_loss=[0.26468995 0.38596344], train_acc=[0.91335 0.8696 ]
84/100: train_loss=[0.26564685 0.3856328 ], train_acc=[0.9122  0.87135]
86/100: train_loss=[0.27177992 0.37655216], train_acc=[0.9118 0.8737]
88/100: train_loss=[0.2597925 0.3811238], train_acc=[0.91375 0.8721 ]
90/100: train_loss=[0.26655054 0.38512927], train_acc=[0.91365 0.8692 ]
92/100: train_loss=[0.2699291  0.37652954], train_acc=[0.9111  0.87405]
94/100: train_loss=[0.25960466 0.37924233], train_acc=[0.91515 0.8718 ]
96/100: train_loss=[0.25894228 0.3720861 ], train_acc=[0.9166 0.875 ]
98/100: train_loss=[0.2644989 0.3821032], train_acc=[0.91465 0.87165]
100/100: train_loss=[0.26258793 0.3765115 ], train_acc=[0.9139  0.87355]
**** Time taken for mnist_3 = 1951.0983605384827
Preference Vector = [9.99999988e-01 1.57079632e-04]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[0.87077993 6.725454  ], train_acc=[0.7184  0.09065]
2/100: train_loss=[0.68706185 8.18279   ], train_acc=[0.7779  0.09385]
4/100: train_loss=[0.5676486 8.307167 ], train_acc=[0.81795 0.09295]
6/100: train_loss=[0.48849005 8.080452  ], train_acc=[0.8414  0.09385]
8/100: train_loss=[0.46980897 8.494685  ], train_acc=[0.8477 0.0943]
10/100: train_loss=[0.42637306 7.976124  ], train_acc=[0.8601  0.09345]
12/100: train_loss=[0.40678474 8.150644  ], train_acc=[0.86905 0.0942 ]
14/100: train_loss=[0.3933756 8.05299  ], train_acc=[0.87255 0.0945 ]
16/100: train_loss=[0.37477514 8.06742   ], train_acc=[0.8793 0.0945]
18/100: train_loss=[0.3671707 8.033955 ], train_acc=[0.8831  0.09475]
20/100: train_loss=[0.36157593 7.700394  ], train_acc=[0.88385 0.0971 ]
22/100: train_loss=[0.3520215 7.5471764], train_acc=[0.88555 0.0958 ]
24/100: train_loss=[0.34781155 7.7199864 ], train_acc=[0.88645 0.0976 ]
26/100: train_loss=[0.3459594 7.7922087], train_acc=[0.8886 0.0982]
28/100: train_loss=[0.33450088 7.560366  ], train_acc=[0.89095 0.09765]
30/100: train_loss=[0.3411871 7.793168 ], train_acc=[0.8885 0.0966]
32/100: train_loss=[0.32673302 7.5528846 ], train_acc=[0.8946 0.0992]
34/100: train_loss=[0.32981384 7.638313  ], train_acc=[0.8933 0.0999]
36/100: train_loss=[0.32362273 7.6612873 ], train_acc=[0.89485 0.09695]
38/100: train_loss=[0.3548253 7.606826 ], train_acc=[0.8842 0.0966]
40/100: train_loss=[0.31186584 7.4416485 ], train_acc=[0.89735 0.0994 ]
42/100: train_loss=[0.31767115 7.3886495 ], train_acc=[0.89695 0.10075]
44/100: train_loss=[0.31157565 7.442997  ], train_acc=[0.89845 0.1    ]
46/100: train_loss=[0.3228524 7.3666763], train_acc=[0.89625 0.10115]
48/100: train_loss=[0.31175956 7.493881  ], train_acc=[0.8981 0.0998]
50/100: train_loss=[0.30212045 7.4642415 ], train_acc=[0.90195 0.102  ]
52/100: train_loss=[0.30494392 7.3688226 ], train_acc=[0.902  0.1035]
54/100: train_loss=[0.30393854 7.3766627 ], train_acc=[0.9013  0.10055]
56/100: train_loss=[0.30899274 7.335905  ], train_acc=[0.9    0.1021]
58/100: train_loss=[0.2965517 7.3716593], train_acc=[0.9033 0.104 ]
60/100: train_loss=[0.29720032 7.273574  ], train_acc=[0.9028 0.1049]
62/100: train_loss=[0.29799634 7.3043013 ], train_acc=[0.90245 0.10465]
64/100: train_loss=[0.2919498 7.3530183], train_acc=[0.9043  0.10585]
66/100: train_loss=[0.29361987 7.2247906 ], train_acc=[0.9061 0.1089]
68/100: train_loss=[0.29187387 7.2994094 ], train_acc=[0.90675 0.10575]
70/100: train_loss=[0.28817874 7.251776  ], train_acc=[0.90605 0.1052 ]
72/100: train_loss=[0.2882954 7.24363  ], train_acc=[0.90655 0.1081 ]
74/100: train_loss=[0.29189947 7.2221804 ], train_acc=[0.9077 0.1062]
76/100: train_loss=[0.2852344 7.1192813], train_acc=[0.90875 0.10555]
78/100: train_loss=[0.2930013 7.047355 ], train_acc=[0.9074 0.106 ]
80/100: train_loss=[0.28508142 7.196407  ], train_acc=[0.9091 0.106 ]
82/100: train_loss=[0.28803617 7.1164064 ], train_acc=[0.90625 0.1077 ]
84/100: train_loss=[0.28975412 7.13622   ], train_acc=[0.9092  0.10735]
86/100: train_loss=[0.28330648 7.099458  ], train_acc=[0.90805 0.10465]
88/100: train_loss=[0.28205547 7.160949  ], train_acc=[0.90895 0.1056 ]
90/100: train_loss=[0.2875209 7.187253 ], train_acc=[0.9075 0.1063]
92/100: train_loss=[0.28075096 7.0348244 ], train_acc=[0.90965 0.1087 ]
94/100: train_loss=[0.28289998 7.122689  ], train_acc=[0.90885 0.1093 ]
96/100: train_loss=[0.28853565 7.114422  ], train_acc=[0.9088  0.10655]
98/100: train_loss=[0.28294006 7.250238  ], train_acc=[0.9094 0.1082]
100/100: train_loss=[0.28111747 7.1352315 ], train_acc=[0.909  0.1083]
**** Time taken for mnist_4 = 1906.3457729816437
**** Time taken for mnist = 11625.751846790314
Preference Vector = [1.57079632e-04 9.99999988e-01]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[6.88624   1.1207552], train_acc=[0.08225 0.5812 ]
2/100: train_loss=[7.439606  0.9747775], train_acc=[0.08455 0.6358 ]
4/100: train_loss=[7.521548  0.8538167], train_acc=[0.08615 0.6878 ]
6/100: train_loss=[7.569424   0.78631353], train_acc=[0.08385 0.71085]
8/100: train_loss=[7.928358 0.73233 ], train_acc=[0.0826  0.73245]
10/100: train_loss=[7.9620957  0.72008187], train_acc=[0.07735 0.7377 ]
12/100: train_loss=[7.8065753 0.6896142], train_acc=[0.07865 0.74625]
14/100: train_loss=[7.7457957 0.6768984], train_acc=[0.079   0.74945]
16/100: train_loss=[7.6795473 0.649921 ], train_acc=[0.07545 0.76345]
18/100: train_loss=[7.545618   0.65181917], train_acc=[0.0803 0.7576]
20/100: train_loss=[7.5056014 0.6295236], train_acc=[0.0825 0.7689]
22/100: train_loss=[7.6912556  0.62488365], train_acc=[0.0802  0.77255]
24/100: train_loss=[7.0938554  0.62111074], train_acc=[0.08345 0.77055]
26/100: train_loss=[7.0772886  0.60610396], train_acc=[0.07835 0.777  ]
28/100: train_loss=[7.091722   0.60177886], train_acc=[0.08355 0.78025]
30/100: train_loss=[7.4680195  0.59937674], train_acc=[0.08195 0.77895]
32/100: train_loss=[6.948289  0.5896527], train_acc=[0.08475 0.78325]
34/100: train_loss=[6.8560004  0.59064543], train_acc=[0.08885 0.7825 ]
36/100: train_loss=[7.0261216 0.5823333], train_acc=[0.08785 0.7873 ]
38/100: train_loss=[6.780434   0.57886773], train_acc=[0.0841 0.7879]
40/100: train_loss=[6.789908   0.57763565], train_acc=[0.09045 0.78915]
42/100: train_loss=[6.6539617 0.5792354], train_acc=[0.0866  0.78615]
44/100: train_loss=[6.5359497 0.5816041], train_acc=[0.09275 0.7912 ]
46/100: train_loss=[6.574931   0.56745994], train_acc=[0.0848 0.7937]
48/100: train_loss=[6.3495827 0.5686185], train_acc=[0.0895  0.79105]
50/100: train_loss=[6.441371   0.55839354], train_acc=[0.0851 0.7981]
52/100: train_loss=[6.39457   0.5535799], train_acc=[0.0923  0.79725]
54/100: train_loss=[6.2759166 0.5545148], train_acc=[0.0919  0.79745]
56/100: train_loss=[6.1795497 0.5448059], train_acc=[0.09225 0.8026 ]
58/100: train_loss=[6.3352294 0.5745471], train_acc=[0.08985 0.7961 ]
60/100: train_loss=[6.1663218 0.5460272], train_acc=[0.09075 0.80105]
62/100: train_loss=[6.2038593 0.5455614], train_acc=[0.0918  0.80215]
64/100: train_loss=[6.1872163  0.53665537], train_acc=[0.0927  0.80625]
66/100: train_loss=[6.1717067 0.5436375], train_acc=[0.09075 0.804  ]
68/100: train_loss=[6.107514  0.5489437], train_acc=[0.0913  0.80305]
70/100: train_loss=[6.09922   0.5452846], train_acc=[0.09625 0.80145]
72/100: train_loss=[6.1800756  0.53862345], train_acc=[0.0882  0.80525]
74/100: train_loss=[6.079358   0.54831123], train_acc=[0.09025 0.80095]
76/100: train_loss=[6.065634  0.5366236], train_acc=[0.09245 0.80505]
78/100: train_loss=[5.9455175  0.52712065], train_acc=[0.0929  0.80925]
80/100: train_loss=[5.98821    0.52856237], train_acc=[0.09335 0.8087 ]
82/100: train_loss=[5.846317   0.53595805], train_acc=[0.0964 0.8086]
84/100: train_loss=[5.8278623  0.52685565], train_acc=[0.0927  0.80925]
86/100: train_loss=[5.847846  0.5324576], train_acc=[0.0924 0.8079]
88/100: train_loss=[5.737572  0.5289722], train_acc=[0.09495 0.80815]
90/100: train_loss=[5.681079  0.5186536], train_acc=[0.09975 0.8146 ]
92/100: train_loss=[5.8966923 0.5246641], train_acc=[0.0972 0.8114]
94/100: train_loss=[5.687232  0.5256932], train_acc=[0.09395 0.8109 ]
96/100: train_loss=[5.6472116 0.5242932], train_acc=[0.0982  0.81085]
98/100: train_loss=[5.768377   0.51633734], train_acc=[0.093   0.81415]
100/100: train_loss=[5.540247  0.5195268], train_acc=[0.09825 0.81195]
**** Time taken for fashion_0 = 1813.9286143779755
Preference Vector = [0.38275599 0.92384947]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.4532175 1.2564355], train_acc=[0.4599  0.54355]
2/100: train_loss=[1.240015  1.1109705], train_acc=[0.53995 0.58185]
4/100: train_loss=[1.0447434 0.933711 ], train_acc=[0.6093 0.6573]
6/100: train_loss=[0.97394633 0.86690474], train_acc=[0.6366  0.68505]
8/100: train_loss=[0.9199647 0.8167765], train_acc=[0.6574 0.6984]
10/100: train_loss=[0.88402605 0.7790678 ], train_acc=[0.6724  0.71495]
12/100: train_loss=[0.84186035 0.7506455 ], train_acc=[0.6911 0.726 ]
14/100: train_loss=[0.8259159 0.7104357], train_acc=[0.695  0.7378]
16/100: train_loss=[0.80362886 0.7275393 ], train_acc=[0.7062 0.7315]
18/100: train_loss=[0.78764755 0.6760152 ], train_acc=[0.709 0.748]
20/100: train_loss=[0.7787523 0.667344 ], train_acc=[0.71565 0.75265]
22/100: train_loss=[0.7797081 0.6696232], train_acc=[0.7134 0.7531]
24/100: train_loss=[0.7648522  0.65321547], train_acc=[0.71995 0.7567 ]
26/100: train_loss=[0.74524474 0.63983625], train_acc=[0.72455 0.7659 ]
28/100: train_loss=[0.7369557  0.62426287], train_acc=[0.7284 0.7693]
30/100: train_loss=[0.7257243 0.6233016], train_acc=[0.73085 0.7694 ]
32/100: train_loss=[0.71914357 0.6091806 ], train_acc=[0.7355  0.77385]
34/100: train_loss=[0.7287424 0.6088587], train_acc=[0.7303  0.77345]
36/100: train_loss=[0.70986986 0.59772384], train_acc=[0.73715 0.77845]
38/100: train_loss=[0.7015236  0.60174894], train_acc=[0.74075 0.7786 ]
40/100: train_loss=[0.71936274 0.6110886 ], train_acc=[0.7327 0.7731]
42/100: train_loss=[0.7044713 0.6055627], train_acc=[0.73775 0.7743 ]
44/100: train_loss=[0.6947104 0.6181348], train_acc=[0.74255 0.76935]
46/100: train_loss=[0.6868379 0.5834574], train_acc=[0.74735 0.78565]
48/100: train_loss=[0.6757436 0.580045 ], train_acc=[0.75125 0.7853 ]
50/100: train_loss=[0.67566603 0.5753792 ], train_acc=[0.7509  0.79035]
52/100: train_loss=[0.6795848 0.5792416], train_acc=[0.74525 0.7864 ]
54/100: train_loss=[0.6813303  0.56858766], train_acc=[0.7495  0.79175]
56/100: train_loss=[0.66686726 0.57383263], train_acc=[0.75215 0.78995]
58/100: train_loss=[0.6638763 0.563508 ], train_acc=[0.75785 0.7947 ]
60/100: train_loss=[0.6551102  0.56335187], train_acc=[0.7589  0.79265]
62/100: train_loss=[0.6547318 0.5586654], train_acc=[0.76055 0.79705]
64/100: train_loss=[0.6548796 0.5622697], train_acc=[0.7608 0.7941]
66/100: train_loss=[0.65339595 0.5551531 ], train_acc=[0.76055 0.7968 ]
68/100: train_loss=[0.6535733 0.5474936], train_acc=[0.75685 0.80235]
70/100: train_loss=[0.65014976 0.5495856 ], train_acc=[0.7613 0.8014]
72/100: train_loss=[0.64060515 0.543001  ], train_acc=[0.76215 0.802  ]
74/100: train_loss=[0.6448973  0.55338687], train_acc=[0.7635 0.7993]
76/100: train_loss=[0.63641334 0.5446607 ], train_acc=[0.76715 0.80145]
78/100: train_loss=[0.64008284 0.54618967], train_acc=[0.7619 0.7984]
80/100: train_loss=[0.63513535 0.550667  ], train_acc=[0.76755 0.79505]
82/100: train_loss=[0.63111603 0.5287017 ], train_acc=[0.76745 0.80875]
84/100: train_loss=[0.6399296  0.53366613], train_acc=[0.7641  0.80755]
86/100: train_loss=[0.6297025  0.53401816], train_acc=[0.7698  0.80625]
88/100: train_loss=[0.63236815 0.5479883 ], train_acc=[0.76815 0.80285]
90/100: train_loss=[0.62846977 0.5311869 ], train_acc=[0.7679 0.8068]
92/100: train_loss=[0.6313111 0.5348512], train_acc=[0.7655 0.8063]
94/100: train_loss=[0.63332504 0.5395706 ], train_acc=[0.76575 0.80365]
96/100: train_loss=[0.62842035 0.53028417], train_acc=[0.7683 0.8089]
98/100: train_loss=[0.63135904 0.52972674], train_acc=[0.76895 0.8073 ]
100/100: train_loss=[0.6226717  0.53539443], train_acc=[0.77155 0.80685]
**** Time taken for fashion_1 = 1814.836844444275
Preference Vector = [0.70710678 0.70710678]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.1615074 1.2907013], train_acc=[0.5651 0.518 ]
2/100: train_loss=[1.0664163 1.1671493], train_acc=[0.6019  0.57915]
4/100: train_loss=[0.902815  0.9984409], train_acc=[0.66565 0.62195]
6/100: train_loss=[0.89152664 0.89812326], train_acc=[0.66225 0.67065]
8/100: train_loss=[0.80270785 0.83956826], train_acc=[0.6997 0.6859]
10/100: train_loss=[0.77051836 0.8010645 ], train_acc=[0.7113 0.7033]
12/100: train_loss=[0.75823134 0.7607608 ], train_acc=[0.71445 0.72215]
14/100: train_loss=[0.7604343 0.7596575], train_acc=[0.71795 0.7207 ]
16/100: train_loss=[0.7166618 0.7447383], train_acc=[0.73505 0.72355]
18/100: train_loss=[0.7011681 0.7136289], train_acc=[0.74055 0.7396 ]
20/100: train_loss=[0.7024553  0.70832485], train_acc=[0.73495 0.74045]
22/100: train_loss=[0.67501014 0.70827055], train_acc=[0.7503  0.73585]
24/100: train_loss=[0.68835175 0.69476444], train_acc=[0.7353  0.75005]
26/100: train_loss=[0.66741234 0.68190926], train_acc=[0.75   0.7526]
28/100: train_loss=[0.66459423 0.6841345 ], train_acc=[0.7506  0.74985]
30/100: train_loss=[0.64304525 0.699348  ], train_acc=[0.76155 0.74525]
32/100: train_loss=[0.64114636 0.6666936 ], train_acc=[0.759 0.757]
34/100: train_loss=[0.63307935 0.6810843 ], train_acc=[0.76375 0.7468 ]
36/100: train_loss=[0.6387192 0.6506325], train_acc=[0.7617 0.7649]
38/100: train_loss=[0.63532615 0.66261023], train_acc=[0.76035 0.7629 ]
40/100: train_loss=[0.6155868 0.6508658], train_acc=[0.7728  0.76525]
42/100: train_loss=[0.6139235  0.64893657], train_acc=[0.7721  0.76545]
44/100: train_loss=[0.6143039 0.6417912], train_acc=[0.77065 0.76415]
46/100: train_loss=[0.60962766 0.6279777 ], train_acc=[0.7755  0.77355]
48/100: train_loss=[0.6002745 0.62777  ], train_acc=[0.77865 0.7686 ]
50/100: train_loss=[0.60859734 0.6353244 ], train_acc=[0.7787  0.76835]
52/100: train_loss=[0.605509  0.6236338], train_acc=[0.77905 0.7711 ]
54/100: train_loss=[0.60039115 0.63493204], train_acc=[0.7794  0.76785]
56/100: train_loss=[0.59098303 0.6158233 ], train_acc=[0.77975 0.77615]
58/100: train_loss=[0.5931449 0.6201879], train_acc=[0.78385 0.7726 ]
60/100: train_loss=[0.6014665 0.6217405], train_acc=[0.78185 0.7725 ]
62/100: train_loss=[0.583848   0.61482847], train_acc=[0.7854  0.77465]
64/100: train_loss=[0.5787839 0.6200027], train_acc=[0.78245 0.7742 ]
66/100: train_loss=[0.5760808 0.6176297], train_acc=[0.7873  0.77165]
68/100: train_loss=[0.5892128  0.60754687], train_acc=[0.7805  0.77995]
70/100: train_loss=[0.5840706  0.60460407], train_acc=[0.78185 0.7796 ]
72/100: train_loss=[0.58029515 0.603305  ], train_acc=[0.78505 0.7832 ]
74/100: train_loss=[0.57627535 0.6054689 ], train_acc=[0.7876  0.78205]
76/100: train_loss=[0.57292765 0.60724914], train_acc=[0.78955 0.7791 ]
78/100: train_loss=[0.57491875 0.6077812 ], train_acc=[0.78955 0.77825]
80/100: train_loss=[0.5724151  0.60762113], train_acc=[0.78745 0.77975]
82/100: train_loss=[0.58071244 0.59086734], train_acc=[0.7856  0.78415]
84/100: train_loss=[0.56905967 0.5962712 ], train_acc=[0.7888 0.7846]
86/100: train_loss=[0.57072186 0.5987821 ], train_acc=[0.7905 0.7829]
88/100: train_loss=[0.5650549  0.58892816], train_acc=[0.78965 0.78305]
90/100: train_loss=[0.5711198 0.5909726], train_acc=[0.79  0.784]
92/100: train_loss=[0.56877947 0.6002025 ], train_acc=[0.791   0.78045]
94/100: train_loss=[0.5664162 0.5906545], train_acc=[0.78875 0.7845 ]
96/100: train_loss=[0.55656844 0.5827811 ], train_acc=[0.7934 0.7893]
98/100: train_loss=[0.5619466 0.5950859], train_acc=[0.7944  0.78525]
100/100: train_loss=[0.5590342 0.5883946], train_acc=[0.7958  0.78395]
**** Time taken for fashion_2 = 1815.4388782978058
Preference Vector = [0.92384947 0.38275599]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.1304119 1.7082659], train_acc=[0.56725 0.37325]
2/100: train_loss=[0.9599084 1.3256577], train_acc=[0.6358  0.51125]
4/100: train_loss=[0.8343993 1.0877964], train_acc=[0.68965 0.6037 ]
6/100: train_loss=[0.76163685 1.0090796 ], train_acc=[0.71585 0.63345]
8/100: train_loss=[0.72779846 0.94119495], train_acc=[0.7297 0.6584]
10/100: train_loss=[0.7139447 0.8943329], train_acc=[0.73685 0.6738 ]
12/100: train_loss=[0.6908523  0.85315204], train_acc=[0.73965 0.68845]
14/100: train_loss=[0.66265166 0.83085936], train_acc=[0.75505 0.6972 ]
16/100: train_loss=[0.6460551 0.820882 ], train_acc=[0.7592 0.6988]
18/100: train_loss=[0.6375265  0.80613244], train_acc=[0.7651 0.711 ]
20/100: train_loss=[0.62979704 0.7978308 ], train_acc=[0.76605 0.70885]
22/100: train_loss=[0.62195414 0.77047867], train_acc=[0.7688 0.7181]
24/100: train_loss=[0.6067029 0.7620456], train_acc=[0.7749 0.7198]
26/100: train_loss=[0.605264  0.7474817], train_acc=[0.77465 0.72665]
28/100: train_loss=[0.5924369 0.7393658], train_acc=[0.7817  0.73035]
30/100: train_loss=[0.6015584  0.75407773], train_acc=[0.7791  0.72175]
32/100: train_loss=[0.5890138  0.72921735], train_acc=[0.7797 0.7341]
34/100: train_loss=[0.5822289 0.7223457], train_acc=[0.78425 0.73605]
36/100: train_loss=[0.5842181 0.7241419], train_acc=[0.7805  0.73615]
38/100: train_loss=[0.58122045 0.711066  ], train_acc=[0.78185 0.74095]
40/100: train_loss=[0.5714844  0.70980257], train_acc=[0.7889  0.74255]
42/100: train_loss=[0.5643133 0.6995334], train_acc=[0.7898 0.7483]
44/100: train_loss=[0.58069485 0.73236585], train_acc=[0.78215 0.73305]
46/100: train_loss=[0.5663123 0.6916418], train_acc=[0.7937  0.75045]
48/100: train_loss=[0.5681532 0.6931416], train_acc=[0.78975 0.7502 ]
50/100: train_loss=[0.55245304 0.685233  ], train_acc=[0.7941  0.75435]
52/100: train_loss=[0.54877967 0.6810729 ], train_acc=[0.7967  0.75415]
54/100: train_loss=[0.5566849 0.674958 ], train_acc=[0.79105 0.75615]
56/100: train_loss=[0.55821073 0.6732761 ], train_acc=[0.7902  0.75605]
58/100: train_loss=[0.54717565 0.67246276], train_acc=[0.8005  0.75885]
60/100: train_loss=[0.54284275 0.6726549 ], train_acc=[0.79945 0.75875]
62/100: train_loss=[0.55218065 0.67051625], train_acc=[0.7947 0.7569]
64/100: train_loss=[0.5431854  0.67834634], train_acc=[0.7998 0.7543]
66/100: train_loss=[0.53928024 0.6638973 ], train_acc=[0.8026 0.7603]
68/100: train_loss=[0.5326363  0.65989554], train_acc=[0.80455 0.761  ]
70/100: train_loss=[0.5349061 0.6615397], train_acc=[0.8017 0.7621]
72/100: train_loss=[0.53958774 0.65768063], train_acc=[0.79815 0.7627 ]
74/100: train_loss=[0.52864575 0.6505185 ], train_acc=[0.80395 0.76575]
76/100: train_loss=[0.53418595 0.6679781 ], train_acc=[0.80445 0.76215]
78/100: train_loss=[0.52623844 0.6549437 ], train_acc=[0.8044  0.76355]
80/100: train_loss=[0.5331858  0.64885783], train_acc=[0.8011 0.7649]
82/100: train_loss=[0.5445941 0.6586956], train_acc=[0.80235 0.7623 ]
84/100: train_loss=[0.5342421 0.6486569], train_acc=[0.8004  0.76515]
86/100: train_loss=[0.51518655 0.6410419 ], train_acc=[0.80835 0.7664 ]
88/100: train_loss=[0.5256713 0.6568814], train_acc=[0.806  0.7629]
90/100: train_loss=[0.5229265 0.6477297], train_acc=[0.8055 0.7661]
92/100: train_loss=[0.5191213 0.6372811], train_acc=[0.8074  0.77095]
94/100: train_loss=[0.51992697 0.6375293 ], train_acc=[0.80995 0.77135]
96/100: train_loss=[0.53257203 0.6409869 ], train_acc=[0.8033  0.76895]
98/100: train_loss=[0.5290526 0.6341714], train_acc=[0.80395 0.77015]
100/100: train_loss=[0.5159836  0.63779294], train_acc=[0.8077  0.77055]
**** Time taken for fashion_3 = 1816.2509899139404
Preference Vector = [9.99999988e-01 1.57079632e-04]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.011037  7.2034774], train_acc=[0.6177  0.11525]
2/100: train_loss=[0.875576  7.7378845], train_acc=[0.6714  0.10985]
4/100: train_loss=[0.78365666 8.413199  ], train_acc=[0.7014  0.10175]
6/100: train_loss=[0.7625043 7.7309957], train_acc=[0.7127  0.10125]
8/100: train_loss=[0.6882386 7.6374483], train_acc=[0.74285 0.1027 ]
10/100: train_loss=[0.6818353 8.045367 ], train_acc=[0.7471 0.1042]
12/100: train_loss=[0.65325344 7.5532475 ], train_acc=[0.75515 0.1042 ]
14/100: train_loss=[0.62650365 7.1862664 ], train_acc=[0.76855 0.1051 ]
16/100: train_loss=[0.6066374 7.1608486], train_acc=[0.77775 0.1046 ]
18/100: train_loss=[0.610458  6.9862556], train_acc=[0.77555 0.10665]
20/100: train_loss=[0.5981712 7.0274982], train_acc=[0.7789  0.10355]
22/100: train_loss=[0.5825654 6.6924253], train_acc=[0.78445 0.10545]
24/100: train_loss=[0.60402983 6.965835  ], train_acc=[0.7741 0.1041]
26/100: train_loss=[0.5830123 6.4611683], train_acc=[0.785   0.10615]
28/100: train_loss=[0.5620283 6.5070534], train_acc=[0.7905  0.10465]
30/100: train_loss=[0.5660196 6.5726414], train_acc=[0.7913  0.10695]
32/100: train_loss=[0.54608816 6.3324127 ], train_acc=[0.7974 0.1044]
34/100: train_loss=[0.5473013 6.239352 ], train_acc=[0.79695 0.10745]
36/100: train_loss=[0.5495333 6.3740683], train_acc=[0.7968  0.10435]
38/100: train_loss=[0.548991 5.999066], train_acc=[0.79485 0.1051 ]
40/100: train_loss=[0.53778636 6.037313  ], train_acc=[0.8018 0.1087]
42/100: train_loss=[0.53458333 6.056946  ], train_acc=[0.80385 0.1049 ]
44/100: train_loss=[0.5358354 5.995042 ], train_acc=[0.8006 0.1073]
46/100: train_loss=[0.5343757 5.865397 ], train_acc=[0.80255 0.10765]
48/100: train_loss=[0.52163815 5.910142  ], train_acc=[0.8072  0.10675]
50/100: train_loss=[0.526467 5.868097], train_acc=[0.806   0.10775]
52/100: train_loss=[0.52514684 5.84218   ], train_acc=[0.80565 0.1059 ]
54/100: train_loss=[0.5166918 5.5870724], train_acc=[0.80775 0.1057 ]
56/100: train_loss=[0.5185522 5.7453084], train_acc=[0.8084  0.10575]
58/100: train_loss=[0.52750796 5.662363  ], train_acc=[0.8024 0.1085]
60/100: train_loss=[0.52318543 5.726516  ], train_acc=[0.80655 0.1051 ]
62/100: train_loss=[0.5076489 5.624623 ], train_acc=[0.8121  0.10815]
64/100: train_loss=[0.50624955 5.486711  ], train_acc=[0.81335 0.1108 ]
66/100: train_loss=[0.50811005 5.609805  ], train_acc=[0.81335 0.10725]
68/100: train_loss=[0.5196847 5.439363 ], train_acc=[0.81025 0.1104 ]
70/100: train_loss=[0.50188893 5.5250993 ], train_acc=[0.81595 0.10835]
72/100: train_loss=[0.5070483 5.4408913], train_acc=[0.8147 0.1127]
74/100: train_loss=[0.49742684 5.375543  ], train_acc=[0.81695 0.11115]
76/100: train_loss=[0.50158024 5.420062  ], train_acc=[0.8132  0.11165]
78/100: train_loss=[0.497384  5.4043436], train_acc=[0.8149 0.1094]
80/100: train_loss=[0.49422437 5.2714334 ], train_acc=[0.81735 0.1102 ]
82/100: train_loss=[0.49684298 5.353721  ], train_acc=[0.81665 0.11315]
84/100: train_loss=[0.4986842 5.206887 ], train_acc=[0.81895 0.11185]
86/100: train_loss=[0.5000015 5.1895638], train_acc=[0.81705 0.1105 ]
88/100: train_loss=[0.48801625 5.1485004 ], train_acc=[0.822   0.11575]
90/100: train_loss=[0.49460396 5.287432  ], train_acc=[0.81755 0.1143 ]
92/100: train_loss=[0.5017878 5.157029 ], train_acc=[0.8163 0.1155]
94/100: train_loss=[0.495608 5.193157], train_acc=[0.8174 0.1128]
96/100: train_loss=[0.5006265 5.1302385], train_acc=[0.81665 0.11785]
98/100: train_loss=[0.48710823 5.0808105 ], train_acc=[0.8214 0.1154]
100/100: train_loss=[0.50672156 5.070996  ], train_acc=[0.81545 0.1161 ]
**** Time taken for fashion_4 = 1817.7505025863647
**** Time taken for fashion = 9078.262796401978
Preference Vector = [1.57079632e-04 9.99999988e-01]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[8.757382   0.88752896], train_acc=[0.1046 0.6695]
2/100: train_loss=[9.302667  0.7829574], train_acc=[0.1034 0.7083]
4/100: train_loss=[9.116164   0.68796057], train_acc=[0.1048  0.74665]
6/100: train_loss=[8.528591  0.6545517], train_acc=[0.1073 0.7526]
8/100: train_loss=[8.2277    0.6120111], train_acc=[0.10545 0.7729 ]
10/100: train_loss=[8.309371   0.58458215], train_acc=[0.10565 0.78465]
12/100: train_loss=[8.020365  0.5619805], train_acc=[0.10435 0.7922 ]
14/100: train_loss=[7.873941  0.5554745], train_acc=[0.10265 0.7971 ]
16/100: train_loss=[7.5965285 0.5323449], train_acc=[0.10245 0.8033 ]
18/100: train_loss=[7.5255    0.5208282], train_acc=[0.10175 0.8093 ]
20/100: train_loss=[7.07489   0.5104035], train_acc=[0.1021 0.8147]
22/100: train_loss=[6.983823   0.50537366], train_acc=[0.1018 0.815 ]
24/100: train_loss=[7.2466025  0.50940424], train_acc=[0.101 0.81 ]
26/100: train_loss=[7.0090914  0.50348395], train_acc=[0.10125 0.81425]
28/100: train_loss=[6.6955776  0.48688847], train_acc=[0.1014  0.82235]
30/100: train_loss=[6.8903933  0.48014566], train_acc=[0.1014  0.82465]
32/100: train_loss=[6.6524677  0.48161992], train_acc=[0.1009  0.82515]
34/100: train_loss=[6.644159  0.4725118], train_acc=[0.1003 0.8288]
36/100: train_loss=[6.626163   0.48675972], train_acc=[0.10075 0.81885]
38/100: train_loss=[6.530587   0.46396956], train_acc=[0.1011 0.8312]
40/100: train_loss=[6.3546567  0.46381775], train_acc=[0.1003 0.8312]
42/100: train_loss=[6.414975 0.455209], train_acc=[0.10145 0.83395]
44/100: train_loss=[6.2145057 0.4626702], train_acc=[0.10125 0.82985]
46/100: train_loss=[6.2208514  0.44978955], train_acc=[0.10105 0.83655]
48/100: train_loss=[6.237506  0.4569241], train_acc=[0.10105 0.83515]
50/100: train_loss=[6.203704  0.4511781], train_acc=[0.102   0.83535]
52/100: train_loss=[6.133667   0.44708693], train_acc=[0.10185 0.83745]
54/100: train_loss=[6.1358094  0.44500387], train_acc=[0.102 0.841]
56/100: train_loss=[6.0182867 0.4458524], train_acc=[0.10095 0.83765]
58/100: train_loss=[6.0364637  0.44502217], train_acc=[0.1013 0.8386]
60/100: train_loss=[6.0010386  0.44365066], train_acc=[0.101   0.83765]
62/100: train_loss=[5.928957   0.44144702], train_acc=[0.0999 0.8382]
64/100: train_loss=[5.7376404  0.44495234], train_acc=[0.1004 0.8357]
66/100: train_loss=[5.88009    0.43736166], train_acc=[0.10055 0.83965]
68/100: train_loss=[5.8206573  0.43631518], train_acc=[0.10035 0.8408 ]
70/100: train_loss=[5.6604805  0.43677202], train_acc=[0.10055 0.84135]
72/100: train_loss=[5.613855   0.43821248], train_acc=[0.09995 0.8401 ]
74/100: train_loss=[5.470431   0.44750646], train_acc=[0.0991  0.83625]
76/100: train_loss=[5.5968566 0.4295929], train_acc=[0.09935 0.84265]
78/100: train_loss=[5.574986   0.43417084], train_acc=[0.10015 0.84245]
80/100: train_loss=[5.4772887  0.43183592], train_acc=[0.09935 0.8439 ]
82/100: train_loss=[5.480464   0.42726833], train_acc=[0.09895 0.8454 ]
84/100: train_loss=[5.3936553  0.43053818], train_acc=[0.0972 0.8432]
86/100: train_loss=[5.388597   0.43417364], train_acc=[0.09965 0.84145]
88/100: train_loss=[5.4506183 0.4262821], train_acc=[0.0992 0.8462]
90/100: train_loss=[5.321982  0.4302095], train_acc=[0.0995 0.8446]
92/100: train_loss=[5.3122005  0.42344758], train_acc=[0.09955 0.8465 ]
94/100: train_loss=[5.278161   0.42978302], train_acc=[0.1016 0.8441]
96/100: train_loss=[5.2282224  0.43221554], train_acc=[0.09825 0.8432 ]
98/100: train_loss=[5.3072095  0.42929518], train_acc=[0.09995 0.8453 ]
100/100: train_loss=[5.2441506  0.43060365], train_acc=[0.0985 0.8431]
**** Time taken for fashion_and_mnist_0 = 1816.5571177005768
Preference Vector = [0.38275599 0.92384947]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.840977  0.9542541], train_acc=[0.35665 0.64285]
2/100: train_loss=[1.4727745 0.83353  ], train_acc=[0.4966  0.69125]
4/100: train_loss=[1.180979   0.79122037], train_acc=[0.6031  0.71945]
6/100: train_loss=[0.9992987 0.7412769], train_acc=[0.66565 0.71525]
8/100: train_loss=[0.8581324 0.6846906], train_acc=[0.71555 0.73995]
10/100: train_loss=[0.7811522 0.6638768], train_acc=[0.7396  0.74325]
12/100: train_loss=[0.7014421 0.648692 ], train_acc=[0.7681  0.75275]
14/100: train_loss=[0.65583915 0.6355313 ], train_acc=[0.78395 0.75875]
16/100: train_loss=[0.6114536  0.60834163], train_acc=[0.79785 0.77275]
18/100: train_loss=[0.59697235 0.6028801 ], train_acc=[0.8028  0.77615]
20/100: train_loss=[0.55504054 0.62545574], train_acc=[0.8158 0.762 ]
22/100: train_loss=[0.52526915 0.5815003 ], train_acc=[0.8288 0.7824]
24/100: train_loss=[0.5189061 0.6193214], train_acc=[0.83245 0.7621 ]
26/100: train_loss=[0.4944575 0.5621831], train_acc=[0.8409  0.78975]
28/100: train_loss=[0.4877415  0.56085557], train_acc=[0.8425 0.7938]
30/100: train_loss=[0.4699992  0.54476005], train_acc=[0.8483  0.79725]
32/100: train_loss=[0.4617532 0.5588964], train_acc=[0.85085 0.79395]
34/100: train_loss=[0.4466875 0.5607799], train_acc=[0.85555 0.7898 ]
36/100: train_loss=[0.4777054 0.553659 ], train_acc=[0.8444  0.79625]
38/100: train_loss=[0.43749258 0.5442627 ], train_acc=[0.8583  0.79925]
40/100: train_loss=[0.4243286 0.5340931], train_acc=[0.86245 0.80105]
42/100: train_loss=[0.41514462 0.5352778 ], train_acc=[0.8656 0.7976]
44/100: train_loss=[0.4216615  0.52355725], train_acc=[0.86335 0.80265]
46/100: train_loss=[0.41157582 0.54150766], train_acc=[0.86615 0.79555]
48/100: train_loss=[0.39913487 0.5303829 ], train_acc=[0.87285 0.8028 ]
50/100: train_loss=[0.4036991 0.5174983], train_acc=[0.87035 0.8074 ]
52/100: train_loss=[0.399572  0.5113244], train_acc=[0.87205 0.8073 ]
54/100: train_loss=[0.3928694  0.51025736], train_acc=[0.87485 0.8131 ]
56/100: train_loss=[0.38229677 0.5119064 ], train_acc=[0.87755 0.8099 ]
58/100: train_loss=[0.37546167 0.5046355 ], train_acc=[0.8792 0.812 ]
60/100: train_loss=[0.37880623 0.50176406], train_acc=[0.8785 0.8147]
62/100: train_loss=[0.37454477 0.5148747 ], train_acc=[0.879 0.805]
64/100: train_loss=[0.37047637 0.50259924], train_acc=[0.8817 0.8119]
66/100: train_loss=[0.38772386 0.5180088 ], train_acc=[0.87465 0.80575]
68/100: train_loss=[0.37631902 0.49753913], train_acc=[0.8781 0.8158]
70/100: train_loss=[0.35724983 0.49379906], train_acc=[0.88665 0.81755]
72/100: train_loss=[0.3561352 0.5140917], train_acc=[0.88675 0.80665]
74/100: train_loss=[0.3499355 0.4961801], train_acc=[0.88855 0.81385]
76/100: train_loss=[0.35410166 0.48199564], train_acc=[0.88645 0.821  ]
78/100: train_loss=[0.35013932 0.4854156 ], train_acc=[0.88855 0.81865]
80/100: train_loss=[0.34297216 0.4860274 ], train_acc=[0.8907 0.8187]
82/100: train_loss=[0.3515344  0.49748766], train_acc=[0.8869 0.817 ]
84/100: train_loss=[0.35995087 0.49577704], train_acc=[0.88605 0.8162 ]
86/100: train_loss=[0.3379447  0.48522738], train_acc=[0.89305 0.82005]
88/100: train_loss=[0.33642465 0.4936537 ], train_acc=[0.89285 0.8176 ]
90/100: train_loss=[0.33556408 0.48256797], train_acc=[0.89265 0.81985]
92/100: train_loss=[0.33133176 0.4771042 ], train_acc=[0.8949 0.8208]
94/100: train_loss=[0.33010492 0.4719432 ], train_acc=[0.89445 0.82485]
96/100: train_loss=[0.33815432 0.46979514], train_acc=[0.89115 0.8237 ]
98/100: train_loss=[0.32571128 0.47443342], train_acc=[0.89595 0.8224 ]
100/100: train_loss=[0.3281009 0.4869975], train_acc=[0.89345 0.82015]
**** Time taken for fashion_and_mnist_1 = 1815.9055559635162
Preference Vector = [0.70710678 0.70710678]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.4033365 1.1031275], train_acc=[0.52435 0.59695]
2/100: train_loss=[1.053671   0.90788454], train_acc=[0.64445 0.66485]
4/100: train_loss=[0.760749  0.7799962], train_acc=[0.74695 0.70275]
6/100: train_loss=[0.6135929 0.718164 ], train_acc=[0.79875 0.7333 ]
8/100: train_loss=[0.54497105 0.69672143], train_acc=[0.82145 0.739  ]
10/100: train_loss=[0.5383258 0.6668781], train_acc=[0.82435 0.7521 ]
12/100: train_loss=[0.4768784 0.6522924], train_acc=[0.8457  0.75655]
14/100: train_loss=[0.43943876 0.6339585 ], train_acc=[0.8579 0.7595]
16/100: train_loss=[0.42652604 0.6127447 ], train_acc=[0.86335 0.77395]
18/100: train_loss=[0.43076986 0.6001693 ], train_acc=[0.86195 0.7781 ]
20/100: train_loss=[0.39982542 0.5919617 ], train_acc=[0.87145 0.7796 ]
22/100: train_loss=[0.3777331 0.5860449], train_acc=[0.8805  0.78375]
24/100: train_loss=[0.40748546 0.5745839 ], train_acc=[0.8677 0.787 ]
26/100: train_loss=[0.3547475 0.5767156], train_acc=[0.88895 0.78805]
28/100: train_loss=[0.3418491 0.5617622], train_acc=[0.8909  0.79105]
30/100: train_loss=[0.33584514 0.55134076], train_acc=[0.89395 0.7935 ]
32/100: train_loss=[0.33198303 0.56159174], train_acc=[0.8938 0.7883]
34/100: train_loss=[0.323104  0.5570703], train_acc=[0.8965 0.7925]
36/100: train_loss=[0.32737336 0.5366125 ], train_acc=[0.89555 0.80135]
38/100: train_loss=[0.3388257  0.54212385], train_acc=[0.89075 0.7969 ]
40/100: train_loss=[0.31496263 0.54091436], train_acc=[0.9   0.796]
42/100: train_loss=[0.31377035 0.53929275], train_acc=[0.9004 0.8004]
44/100: train_loss=[0.3085802 0.5180686], train_acc=[0.9008  0.80585]
46/100: train_loss=[0.2937552 0.5137959], train_acc=[0.9064  0.80805]
48/100: train_loss=[0.29388636 0.5132364 ], train_acc=[0.9065  0.80975]
50/100: train_loss=[0.30062944 0.510086  ], train_acc=[0.9034 0.8118]
52/100: train_loss=[0.2863454 0.5145911], train_acc=[0.909  0.8088]
54/100: train_loss=[0.2836123  0.51328874], train_acc=[0.90945 0.81095]
56/100: train_loss=[0.304375   0.50838846], train_acc=[0.90405 0.81195]
58/100: train_loss=[0.27738544 0.5043103 ], train_acc=[0.91235 0.81405]
60/100: train_loss=[0.28728998 0.49907434], train_acc=[0.90775 0.81375]
62/100: train_loss=[0.27130225 0.4951818 ], train_acc=[0.91455 0.8174 ]
64/100: train_loss=[0.27244464 0.4938872 ], train_acc=[0.9151  0.81515]
66/100: train_loss=[0.27468675 0.5028787 ], train_acc=[0.91435 0.81085]
68/100: train_loss=[0.27103314 0.4891062 ], train_acc=[0.9137 0.8194]
70/100: train_loss=[0.266104   0.49533015], train_acc=[0.91635 0.817  ]
72/100: train_loss=[0.27295724 0.49654573], train_acc=[0.9133 0.815 ]
74/100: train_loss=[0.28396145 0.48590302], train_acc=[0.9112 0.8177]
76/100: train_loss=[0.26510534 0.49213678], train_acc=[0.9165 0.8159]
78/100: train_loss=[0.27944967 0.4834942 ], train_acc=[0.9106 0.8191]
80/100: train_loss=[0.28416342 0.49209905], train_acc=[0.9093  0.81625]
82/100: train_loss=[0.26736945 0.48696557], train_acc=[0.91435 0.8197 ]
84/100: train_loss=[0.25640285 0.48542705], train_acc=[0.9195  0.81975]
86/100: train_loss=[0.26612535 0.47442272], train_acc=[0.91635 0.82375]
88/100: train_loss=[0.2511828  0.48042458], train_acc=[0.92045 0.8214 ]
90/100: train_loss=[0.256524   0.47920248], train_acc=[0.9194 0.8213]
92/100: train_loss=[0.2497043 0.4829531], train_acc=[0.92155 0.82195]
94/100: train_loss=[0.2527574  0.47262865], train_acc=[0.92035 0.8233 ]
96/100: train_loss=[0.25606635 0.47463208], train_acc=[0.91935 0.82465]
98/100: train_loss=[0.27581635 0.47870308], train_acc=[0.91165 0.8237 ]
100/100: train_loss=[0.24843813 0.475091  ], train_acc=[0.9218  0.82495]
**** Time taken for fashion_and_mnist_2 = 1817.5167908668518
Preference Vector = [0.92384947 0.38275599]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.2621858 1.5812556], train_acc=[0.5735 0.4554]
2/100: train_loss=[0.9026221 1.2004706], train_acc=[0.69995 0.58145]
4/100: train_loss=[0.6683939 0.9847785], train_acc=[0.77865 0.6422 ]
6/100: train_loss=[0.53323215 0.87506634], train_acc=[0.82385 0.67985]
8/100: train_loss=[0.4603769 0.8148178], train_acc=[0.84835 0.699  ]
10/100: train_loss=[0.42245623 0.78716904], train_acc=[0.86065 0.70935]
12/100: train_loss=[0.3946539  0.75189096], train_acc=[0.87215 0.7201 ]
14/100: train_loss=[0.37852967 0.7355725 ], train_acc=[0.8749  0.72825]
16/100: train_loss=[0.34597826 0.71299815], train_acc=[0.88835 0.73495]
18/100: train_loss=[0.33549857 0.7092471 ], train_acc=[0.89115 0.73545]
20/100: train_loss=[0.3325779 0.6912974], train_acc=[0.8913  0.74225]
22/100: train_loss=[0.34514338 0.67432976], train_acc=[0.8881 0.7482]
24/100: train_loss=[0.3138464 0.6728701], train_acc=[0.89965 0.74925]
26/100: train_loss=[0.29588014 0.6633117 ], train_acc=[0.907  0.7515]
28/100: train_loss=[0.2900349 0.6514517], train_acc=[0.9088  0.75855]
30/100: train_loss=[0.28347546 0.64660126], train_acc=[0.9099  0.76115]
32/100: train_loss=[0.2747385 0.6482662], train_acc=[0.9127 0.7577]
34/100: train_loss=[0.2854644 0.6343732], train_acc=[0.90845 0.7649 ]
36/100: train_loss=[0.2742247 0.626667 ], train_acc=[0.9131 0.7688]
38/100: train_loss=[0.26565778 0.6303496 ], train_acc=[0.91455 0.76795]
40/100: train_loss=[0.2601929  0.62252074], train_acc=[0.9163 0.7672]
42/100: train_loss=[0.25309873 0.61874074], train_acc=[0.9189  0.76895]
44/100: train_loss=[0.26710507 0.60888654], train_acc=[0.9134 0.7743]
46/100: train_loss=[0.25239313 0.607994  ], train_acc=[0.9181 0.7721]
48/100: train_loss=[0.2474218  0.60446185], train_acc=[0.9212 0.775 ]
50/100: train_loss=[0.248525  0.6038087], train_acc=[0.9207 0.7778]
52/100: train_loss=[0.24895495 0.5979141 ], train_acc=[0.9201 0.781 ]
54/100: train_loss=[0.2380464 0.6001022], train_acc=[0.92445 0.77785]
56/100: train_loss=[0.24510543 0.59504014], train_acc=[0.921   0.78005]
58/100: train_loss=[0.23601799 0.59396315], train_acc=[0.9232  0.78245]
60/100: train_loss=[0.23368107 0.5873354 ], train_acc=[0.9247  0.78255]
62/100: train_loss=[0.23354545 0.58878833], train_acc=[0.92595 0.7832 ]
64/100: train_loss=[0.23447807 0.5954686 ], train_acc=[0.92565 0.77935]
66/100: train_loss=[0.23534065 0.58437675], train_acc=[0.92445 0.7851 ]
68/100: train_loss=[0.23429297 0.5829387 ], train_acc=[0.9236  0.78605]
70/100: train_loss=[0.22717488 0.58379215], train_acc=[0.92745 0.78375]
72/100: train_loss=[0.23185866 0.5744758 ], train_acc=[0.9257  0.78985]
74/100: train_loss=[0.23248348 0.5739631 ], train_acc=[0.9238  0.78965]
76/100: train_loss=[0.23373841 0.57279485], train_acc=[0.9244  0.78885]
78/100: train_loss=[0.22560145 0.5736151 ], train_acc=[0.9288 0.7913]
80/100: train_loss=[0.2236939 0.5694313], train_acc=[0.92775 0.7911 ]
82/100: train_loss=[0.22239318 0.57028174], train_acc=[0.92805 0.7906 ]
84/100: train_loss=[0.21734837 0.56903654], train_acc=[0.9299  0.79005]
86/100: train_loss=[0.22726682 0.56697077], train_acc=[0.9266 0.7915]
88/100: train_loss=[0.22189552 0.5709807 ], train_acc=[0.92985 0.789  ]
90/100: train_loss=[0.21346632 0.5711386 ], train_acc=[0.9316  0.79005]
92/100: train_loss=[0.2247794  0.56279755], train_acc=[0.9283  0.79505]
94/100: train_loss=[0.21096836 0.5652412 ], train_acc=[0.93375 0.7915 ]
96/100: train_loss=[0.22451068 0.55881155], train_acc=[0.92755 0.79515]
98/100: train_loss=[0.21990784 0.5686281 ], train_acc=[0.9299 0.791 ]
100/100: train_loss=[0.21619512 0.56264794], train_acc=[0.9292 0.7947]
**** Time taken for fashion_and_mnist_3 = 1816.88521194458
Preference Vector = [9.99999988e-01 1.57079632e-04]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.0804148 5.8476233], train_acc=[0.63945 0.10275]
2/100: train_loss=[0.7620052 6.7301025], train_acc=[0.74885 0.10555]
4/100: train_loss=[0.5370842 7.1158595], train_acc=[0.8282  0.10295]
6/100: train_loss=[0.43360317 7.3446617 ], train_acc=[0.8633 0.1032]
8/100: train_loss=[0.38611668 7.312212  ], train_acc=[0.87895 0.1049 ]
10/100: train_loss=[0.35149077 7.2966914 ], train_acc=[0.89065 0.10505]
12/100: train_loss=[0.33043578 7.2546897 ], train_acc=[0.89775 0.1041 ]
14/100: train_loss=[0.321762  7.2402616], train_acc=[0.8992  0.10385]
16/100: train_loss=[0.2943667 7.146611 ], train_acc=[0.91015 0.1035 ]
18/100: train_loss=[0.2766531 7.1080794], train_acc=[0.91375 0.1027 ]
20/100: train_loss=[0.27653092 7.1471844 ], train_acc=[0.9124 0.102 ]
22/100: train_loss=[0.2582141 7.1940975], train_acc=[0.9209 0.105 ]
24/100: train_loss=[0.25420526 7.267051  ], train_acc=[0.9213 0.1033]
26/100: train_loss=[0.24278349 7.083612  ], train_acc=[0.92465 0.1043 ]
28/100: train_loss=[0.23660947 7.124355  ], train_acc=[0.9273 0.1016]
30/100: train_loss=[0.23300922 6.9833136 ], train_acc=[0.9279 0.1018]
32/100: train_loss=[0.22997026 7.097717  ], train_acc=[0.9273  0.10075]
34/100: train_loss=[0.22919841 7.0761304 ], train_acc=[0.9279  0.10235]
36/100: train_loss=[0.22172984 7.104681  ], train_acc=[0.93045 0.1034 ]
38/100: train_loss=[0.2175897 7.1262207], train_acc=[0.931 0.105]
40/100: train_loss=[0.21117115 6.99681   ], train_acc=[0.9333  0.10165]
42/100: train_loss=[0.21076824 7.0846043 ], train_acc=[0.93315 0.1019 ]
44/100: train_loss=[0.2077985 7.141331 ], train_acc=[0.93515 0.1033 ]
46/100: train_loss=[0.20209725 7.000371  ], train_acc=[0.93635 0.1042 ]
48/100: train_loss=[0.204301 7.025808], train_acc=[0.9363  0.10565]
50/100: train_loss=[0.19739605 7.0648723 ], train_acc=[0.9378 0.1035]
52/100: train_loss=[0.19458196 6.9432235 ], train_acc=[0.93815 0.1031 ]
54/100: train_loss=[0.19493753 6.9670877 ], train_acc=[0.93915 0.1035 ]
56/100: train_loss=[0.19635795 6.9584575 ], train_acc=[0.9376 0.1038]
58/100: train_loss=[0.18960051 7.021184  ], train_acc=[0.94025 0.1058 ]
60/100: train_loss=[0.19044487 6.9833307 ], train_acc=[0.9395  0.10485]
62/100: train_loss=[0.18897113 6.87676   ], train_acc=[0.9408  0.10375]
64/100: train_loss=[0.18688715 7.107271  ], train_acc=[0.9417  0.10555]
66/100: train_loss=[0.18626626 7.0257244 ], train_acc=[0.9412  0.10415]
68/100: train_loss=[0.20042393 6.677943  ], train_acc=[0.93655 0.10505]
70/100: train_loss=[0.18311653 7.061168  ], train_acc=[0.9424  0.10445]
72/100: train_loss=[0.18354394 7.110548  ], train_acc=[0.9424  0.10425]
74/100: train_loss=[0.18121435 7.1323304 ], train_acc=[0.94365 0.10615]
76/100: train_loss=[0.18278633 6.9914274 ], train_acc=[0.9419 0.1062]
78/100: train_loss=[0.18005784 7.067624  ], train_acc=[0.94325 0.10565]
80/100: train_loss=[0.1826748 7.022705 ], train_acc=[0.94335 0.1086 ]
82/100: train_loss=[0.17890033 7.03598   ], train_acc=[0.94455 0.10665]
84/100: train_loss=[0.17960843 6.9646096 ], train_acc=[0.9449  0.10695]
86/100: train_loss=[0.18343791 7.001876  ], train_acc=[0.94305 0.1057 ]
88/100: train_loss=[0.17626332 7.0009823 ], train_acc=[0.94525 0.107  ]
90/100: train_loss=[0.17600507 7.040271  ], train_acc=[0.94565 0.10605]
92/100: train_loss=[0.17584991 7.1210675 ], train_acc=[0.94625 0.10565]
94/100: train_loss=[0.17686328 6.9379516 ], train_acc=[0.94545 0.1112 ]
96/100: train_loss=[0.17689067 7.155489  ], train_acc=[0.9465  0.10845]
98/100: train_loss=[0.17370039 7.058506  ], train_acc=[0.9472  0.10805]
100/100: train_loss=[0.18146564 7.09859   ], train_acc=[0.945   0.10975]
**** Time taken for fashion_and_mnist_4 = 1815.9523553848267
**** Time taken for fashion_and_mnist = 9082.872689962387
