Preference Vector = [1. 0.]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[0.9929329 5.42119  ], train_acc=[0.6748 0.0875]
2/100: train_loss=[0.7718761 6.090582 ], train_acc=[0.7536 0.0917]
4/100: train_loss=[0.60043126 7.54425   ], train_acc=[0.80705 0.09215]
6/100: train_loss=[0.53662044 7.7582593 ], train_acc=[0.8306  0.09325]
8/100: train_loss=[0.47997323 7.812531  ], train_acc=[0.8459  0.09365]
10/100: train_loss=[0.45771578 7.599204  ], train_acc=[0.85235 0.09395]
12/100: train_loss=[0.4324909 7.900414 ], train_acc=[0.86125 0.09145]
14/100: train_loss=[0.41682097 7.777838  ], train_acc=[0.8662  0.09245]
16/100: train_loss=[0.3919146 7.8257804], train_acc=[0.8744  0.09345]
18/100: train_loss=[0.38347548 7.751646  ], train_acc=[0.87545 0.0936 ]
20/100: train_loss=[0.37591735 7.8131957 ], train_acc=[0.87855 0.09535]
22/100: train_loss=[0.3725713 8.071692 ], train_acc=[0.87915 0.09625]
24/100: train_loss=[0.3648413 7.683033 ], train_acc=[0.88105 0.0968 ]
26/100: train_loss=[0.35812593 7.700465  ], train_acc=[0.8833  0.09885]
28/100: train_loss=[0.3489869 7.858003 ], train_acc=[0.88745 0.0971 ]
30/100: train_loss=[0.34015858 7.7367887 ], train_acc=[0.88925 0.09665]
32/100: train_loss=[0.3423841 7.566255 ], train_acc=[0.8878  0.09635]
34/100: train_loss=[0.3329881 7.9082937], train_acc=[0.89065 0.0977 ]
36/100: train_loss=[0.33117738 7.696991  ], train_acc=[0.8905 0.0976]
38/100: train_loss=[0.33546197 7.593021  ], train_acc=[0.8899 0.0976]
40/100: train_loss=[0.3201178 7.6516743], train_acc=[0.8944 0.0984]
42/100: train_loss=[0.32917044 7.8241224 ], train_acc=[0.89325 0.09755]
44/100: train_loss=[0.32499105 8.018164  ], train_acc=[0.8942  0.09755]
46/100: train_loss=[0.32353148 7.835809  ], train_acc=[0.89355 0.0979 ]
48/100: train_loss=[0.3107498 7.749219 ], train_acc=[0.8989 0.0986]
50/100: train_loss=[0.30874202 7.9678183 ], train_acc=[0.8992  0.09885]
52/100: train_loss=[0.30798516 7.6703253 ], train_acc=[0.8985 0.0979]
54/100: train_loss=[0.30358782 7.944493  ], train_acc=[0.90175 0.09885]
56/100: train_loss=[0.3102121 7.7411094], train_acc=[0.8988  0.09725]
58/100: train_loss=[0.30162528 7.8026347 ], train_acc=[0.9001  0.09855]
60/100: train_loss=[0.2975127 7.8614054], train_acc=[0.9029 0.0991]
62/100: train_loss=[0.2941839 7.93218  ], train_acc=[0.9045 0.0966]
64/100: train_loss=[0.2986606 7.669938 ], train_acc=[0.9022  0.09815]
66/100: train_loss=[0.29606232 7.867303  ], train_acc=[0.90355 0.09705]
68/100: train_loss=[0.30364066 8.181077  ], train_acc=[0.902  0.0972]
70/100: train_loss=[0.28575578 8.05977   ], train_acc=[0.907 0.098]
72/100: train_loss=[0.28748056 7.834881  ], train_acc=[0.90685 0.09765]
74/100: train_loss=[0.28781688 7.9323006 ], train_acc=[0.9069 0.0971]
76/100: train_loss=[0.28894785 8.036974  ], train_acc=[0.90465 0.09725]
78/100: train_loss=[0.28970507 7.86646   ], train_acc=[0.9054 0.0969]
80/100: train_loss=[0.28716102 8.424315  ], train_acc=[0.90715 0.0957 ]
82/100: train_loss=[0.28462726 8.101541  ], train_acc=[0.90865 0.0956 ]
84/100: train_loss=[0.29862678 8.24347   ], train_acc=[0.90255 0.0953 ]
86/100: train_loss=[0.2779822 8.125253 ], train_acc=[0.9106  0.09735]
88/100: train_loss=[0.28280357 8.129905  ], train_acc=[0.90825 0.0947 ]
90/100: train_loss=[0.27759215 8.1606    ], train_acc=[0.9109  0.09555]
92/100: train_loss=[0.28202918 7.8623667 ], train_acc=[0.90795 0.09615]
94/100: train_loss=[0.27691087 8.204862  ], train_acc=[0.9109  0.09635]
96/100: train_loss=[0.27693316 8.249232  ], train_acc=[0.9114 0.0962]
98/100: train_loss=[0.27986118 8.0980015 ], train_acc=[0.909  0.0955]
100/100: train_loss=[0.28293008 8.104768  ], train_acc=[0.90875 0.0959 ]
**** Time taken for mnist_0 = 230.6002233028412
Preference Vector = [0. 1.]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[4.288947  1.2002248], train_acc=[0.09895 0.5956 ]
2/100: train_loss=[4.993703   0.96104836], train_acc=[0.0995 0.679 ]
4/100: train_loss=[5.861258   0.75627667], train_acc=[0.10145 0.7438 ]
6/100: train_loss=[6.0045695 0.6565208], train_acc=[0.1014  0.77895]
8/100: train_loss=[6.36672    0.57666636], train_acc=[0.0997  0.80735]
10/100: train_loss=[6.721749  0.5414727], train_acc=[0.0988 0.8161]
12/100: train_loss=[7.043544  0.5427114], train_acc=[0.10015 0.81845]
14/100: train_loss=[6.9384413  0.49344265], train_acc=[0.10155 0.8328 ]
16/100: train_loss=[7.1853547 0.5060016], train_acc=[0.1001  0.82845]
18/100: train_loss=[7.495041   0.47154856], train_acc=[0.09845 0.84055]
20/100: train_loss=[7.646579   0.46796355], train_acc=[0.09915 0.84295]
22/100: train_loss=[7.5009127 0.4458561], train_acc=[0.10195 0.8505 ]
24/100: train_loss=[7.86491    0.44895762], train_acc=[0.1022 0.8449]
26/100: train_loss=[7.9131193  0.42512727], train_acc=[0.1006 0.8585]
28/100: train_loss=[7.701605   0.41558254], train_acc=[0.10235 0.85825]
30/100: train_loss=[7.928701   0.41924495], train_acc=[0.10045 0.8576 ]
32/100: train_loss=[7.907458  0.3933599], train_acc=[0.103 0.866]
34/100: train_loss=[8.103184  0.4140171], train_acc=[0.10235 0.86005]
36/100: train_loss=[8.121019   0.39314798], train_acc=[0.10455 0.86675]
38/100: train_loss=[8.4842     0.38533717], train_acc=[0.10365 0.87045]
40/100: train_loss=[8.251582   0.39084306], train_acc=[0.10265 0.8676 ]
42/100: train_loss=[8.171823  0.3764078], train_acc=[0.10235 0.8723 ]
44/100: train_loss=[8.465696   0.39226773], train_acc=[0.1034  0.86785]
46/100: train_loss=[7.93626   0.3739203], train_acc=[0.1017  0.87235]
48/100: train_loss=[8.504416   0.36376426], train_acc=[0.10365 0.8772 ]
50/100: train_loss=[8.207004   0.36746478], train_acc=[0.09955 0.8739 ]
52/100: train_loss=[8.403118  0.3739521], train_acc=[0.1019  0.87335]
54/100: train_loss=[8.5717745  0.35920557], train_acc=[0.09965 0.8787 ]
56/100: train_loss=[8.473431  0.3565971], train_acc=[0.10105 0.87845]
58/100: train_loss=[8.43181    0.34749106], train_acc=[0.09975 0.8824 ]
60/100: train_loss=[8.822505   0.35196218], train_acc=[0.0983 0.8821]
62/100: train_loss=[8.704061   0.35437688], train_acc=[0.09935 0.88365]
64/100: train_loss=[8.946121   0.35182723], train_acc=[0.09805 0.88535]
66/100: train_loss=[8.733316  0.3470293], train_acc=[0.0979  0.88465]
68/100: train_loss=[8.6021385  0.33740216], train_acc=[0.09935 0.88515]
70/100: train_loss=[8.999297   0.34683836], train_acc=[0.0984  0.88545]
72/100: train_loss=[8.887825   0.34098217], train_acc=[0.0984 0.8863]
74/100: train_loss=[9.0224     0.33416224], train_acc=[0.09745 0.89075]
76/100: train_loss=[9.120686   0.33436185], train_acc=[0.0969 0.8894]
78/100: train_loss=[9.043334   0.33272958], train_acc=[0.09745 0.88955]
80/100: train_loss=[8.833726   0.33083522], train_acc=[0.098   0.89085]
82/100: train_loss=[9.194007  0.3270322], train_acc=[0.0972  0.89165]
84/100: train_loss=[9.081121   0.33373198], train_acc=[0.09695 0.88945]
86/100: train_loss=[8.980064  0.3301032], train_acc=[0.0972 0.8901]
88/100: train_loss=[9.279291   0.34134135], train_acc=[0.0969  0.88795]
90/100: train_loss=[8.874814   0.32515016], train_acc=[0.0955 0.8924]
92/100: train_loss=[8.99912   0.3391531], train_acc=[0.0976 0.8896]
94/100: train_loss=[9.107455   0.33662385], train_acc=[0.09695 0.8898 ]
96/100: train_loss=[9.177854   0.33401537], train_acc=[0.09695 0.89175]
98/100: train_loss=[9.1848955  0.34075215], train_acc=[0.0986  0.88905]
100/100: train_loss=[9.100964 0.34252 ], train_acc=[0.09805 0.88765]
**** Time taken for mnist_1 = 226.436541557312
**** Time taken for mnist = 457.33886885643005
Preference Vector = [1. 0.]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.1006703 6.5982103], train_acc=[0.5949  0.12455]
2/100: train_loss=[0.9403734 7.7228465], train_acc=[0.64985 0.11815]
4/100: train_loss=[0.8136484 8.426043 ], train_acc=[0.704   0.11425]
6/100: train_loss=[0.7600261 9.038948 ], train_acc=[0.7242 0.1127]
8/100: train_loss=[0.71875  8.938861], train_acc=[0.733   0.11165]
10/100: train_loss=[0.6953469 8.845739 ], train_acc=[0.7417 0.1122]
12/100: train_loss=[0.6771326 9.388787 ], train_acc=[0.7491  0.10655]
14/100: train_loss=[0.6504843 8.969515 ], train_acc=[0.76035 0.1092 ]
16/100: train_loss=[0.64569503 9.200941  ], train_acc=[0.76335 0.10465]
18/100: train_loss=[0.623161 9.229704], train_acc=[0.77115 0.10625]
20/100: train_loss=[0.6342595 9.567159 ], train_acc=[0.76585 0.10435]
22/100: train_loss=[0.6050637 9.374664 ], train_acc=[0.7794 0.1056]
24/100: train_loss=[0.59105426 9.20159   ], train_acc=[0.7843  0.10365]
26/100: train_loss=[0.58983505 9.273701  ], train_acc=[0.78345 0.10485]
28/100: train_loss=[0.58385193 9.394441  ], train_acc=[0.78735 0.10095]
30/100: train_loss=[0.5773198 9.347731 ], train_acc=[0.78935 0.1027 ]
32/100: train_loss=[0.5735463 9.245061 ], train_acc=[0.7894  0.10485]
34/100: train_loss=[0.56147754 9.245989  ], train_acc=[0.79665 0.10265]
36/100: train_loss=[0.56852025 9.00653   ], train_acc=[0.7924 0.1033]
38/100: train_loss=[0.5670079 8.995401 ], train_acc=[0.79385 0.10335]
40/100: train_loss=[0.5606489 9.043975 ], train_acc=[0.79355 0.1065 ]
42/100: train_loss=[0.587737 8.981192], train_acc=[0.78295 0.1071 ]
44/100: train_loss=[0.54649705 9.165209  ], train_acc=[0.7993 0.104 ]
46/100: train_loss=[0.5463035 9.247395 ], train_acc=[0.79965 0.10465]
48/100: train_loss=[0.5438237 9.275111 ], train_acc=[0.79985 0.10685]
50/100: train_loss=[0.53622913 9.380183  ], train_acc=[0.80465 0.10355]
52/100: train_loss=[0.54026145 9.18002   ], train_acc=[0.80215 0.1083 ]
54/100: train_loss=[0.53938717 8.89975   ], train_acc=[0.801   0.10335]
56/100: train_loss=[0.5465728 9.272328 ], train_acc=[0.80075 0.1047 ]
58/100: train_loss=[0.5341774 9.224563 ], train_acc=[0.8063  0.10425]
60/100: train_loss=[0.5316762 9.1351185], train_acc=[0.8061 0.1061]
62/100: train_loss=[0.5364366 9.029413 ], train_acc=[0.80325 0.10505]
64/100: train_loss=[0.5200243 9.1309805], train_acc=[0.8094  0.10615]
66/100: train_loss=[0.5385099 9.167109 ], train_acc=[0.8027  0.10605]
68/100: train_loss=[0.5259004 8.908225 ], train_acc=[0.80705 0.10315]
70/100: train_loss=[0.52907425 9.149149  ], train_acc=[0.8073  0.10395]
72/100: train_loss=[0.52072036 9.180964  ], train_acc=[0.8114 0.1055]
74/100: train_loss=[0.520347 9.151508], train_acc=[0.8104 0.1022]
76/100: train_loss=[0.5259236 8.94436  ], train_acc=[0.8078  0.10525]
78/100: train_loss=[0.51844364 8.981135  ], train_acc=[0.81045 0.1046 ]
80/100: train_loss=[0.51886946 9.028406  ], train_acc=[0.8123  0.10565]
82/100: train_loss=[0.51195866 8.988988  ], train_acc=[0.81355 0.1051 ]
84/100: train_loss=[0.51780975 8.9590845 ], train_acc=[0.8137  0.10635]
86/100: train_loss=[0.51833075 9.004201  ], train_acc=[0.809  0.1059]
88/100: train_loss=[0.5155192 9.044409 ], train_acc=[0.81395 0.10465]
90/100: train_loss=[0.50824946 8.975897  ], train_acc=[0.81425 0.10435]
92/100: train_loss=[0.50758576 9.0946665 ], train_acc=[0.81615 0.1046 ]
94/100: train_loss=[0.5090696 9.27923  ], train_acc=[0.8149 0.1065]
96/100: train_loss=[0.5080527 9.046251 ], train_acc=[0.8161 0.1056]
98/100: train_loss=[0.50933224 9.025276  ], train_acc=[0.81485 0.1028 ]
100/100: train_loss=[0.51156646 8.806069  ], train_acc=[0.8141 0.1045]
**** Time taken for fashion_0 = 226.77813410758972
Preference Vector = [0. 1.]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[5.665328  1.1124978], train_acc=[0.08755 0.5891 ]
2/100: train_loss=[6.6604886 0.9613242], train_acc=[0.0892  0.64345]
4/100: train_loss=[6.979602   0.85374993], train_acc=[0.094  0.6898]
6/100: train_loss=[7.292167  0.7759254], train_acc=[0.09245 0.7146 ]
8/100: train_loss=[7.5361247 0.743888 ], train_acc=[0.09205 0.7233 ]
10/100: train_loss=[7.5549636 0.7223562], train_acc=[0.09365 0.73365]
12/100: train_loss=[7.707838  0.7117826], train_acc=[0.0926 0.7377]
14/100: train_loss=[7.600811   0.69379085], train_acc=[0.0966 0.7437]
16/100: train_loss=[7.738302  0.6696005], train_acc=[0.09125 0.75265]
18/100: train_loss=[7.873484  0.6600239], train_acc=[0.09245 0.75825]
20/100: train_loss=[7.937298   0.65142596], train_acc=[0.0919  0.75975]
22/100: train_loss=[7.7775965  0.63874084], train_acc=[0.0942  0.76735]
24/100: train_loss=[7.8844776  0.63901895], train_acc=[0.09525 0.7652 ]
26/100: train_loss=[8.098262  0.6278926], train_acc=[0.09345 0.7721 ]
28/100: train_loss=[8.142966 0.620481], train_acc=[0.09395 0.7706 ]
30/100: train_loss=[8.133219  0.6178309], train_acc=[0.09485 0.77495]
32/100: train_loss=[7.975417   0.60725886], train_acc=[0.0936  0.77675]
34/100: train_loss=[8.046082  0.6458069], train_acc=[0.0942 0.7586]
36/100: train_loss=[8.1611595  0.59765774], train_acc=[0.0941  0.78205]
38/100: train_loss=[8.063733  0.6049724], train_acc=[0.09375 0.7796 ]
40/100: train_loss=[8.059025  0.5943169], train_acc=[0.09385 0.7826 ]
42/100: train_loss=[8.149451   0.60587573], train_acc=[0.09525 0.7744 ]
44/100: train_loss=[7.9377613 0.6024793], train_acc=[0.09365 0.77885]
46/100: train_loss=[8.222216  0.5830336], train_acc=[0.09395 0.7914 ]
48/100: train_loss=[8.241845  0.5811244], train_acc=[0.0928  0.79015]
50/100: train_loss=[8.414198  0.5782281], train_acc=[0.09435 0.79   ]
52/100: train_loss=[8.261479  0.5803428], train_acc=[0.09605 0.7881 ]
54/100: train_loss=[8.258484  0.5753779], train_acc=[0.09465 0.79325]
56/100: train_loss=[8.130893  0.5781542], train_acc=[0.0929 0.7894]
58/100: train_loss=[8.32013    0.56553364], train_acc=[0.0937 0.7958]
60/100: train_loss=[8.439513  0.5720198], train_acc=[0.0916  0.79455]
62/100: train_loss=[8.531869   0.56640685], train_acc=[0.0936  0.79545]
64/100: train_loss=[8.322688   0.56190073], train_acc=[0.0946 0.7952]
66/100: train_loss=[8.189777   0.55766326], train_acc=[0.0926 0.797 ]
68/100: train_loss=[8.522015   0.56215423], train_acc=[0.09285 0.79725]
70/100: train_loss=[8.373807   0.56728417], train_acc=[0.0917  0.79315]
72/100: train_loss=[8.340301  0.5555607], train_acc=[0.0942 0.8001]
74/100: train_loss=[8.450264  0.5670252], train_acc=[0.0951  0.79445]
76/100: train_loss=[8.63908   0.5515831], train_acc=[0.0925 0.7997]
78/100: train_loss=[8.397708  0.5693286], train_acc=[0.0938  0.79605]
80/100: train_loss=[8.545877   0.55686116], train_acc=[0.0925  0.80375]
82/100: train_loss=[8.450886 0.548235], train_acc=[0.09525 0.80365]
84/100: train_loss=[8.595594  0.5517231], train_acc=[0.0945 0.8015]
86/100: train_loss=[8.524516   0.55968976], train_acc=[0.0929  0.79685]
88/100: train_loss=[8.510254  0.5497786], train_acc=[0.0934  0.80295]
90/100: train_loss=[8.439473   0.56073105], train_acc=[0.0932  0.79725]
92/100: train_loss=[8.387764 0.547512], train_acc=[0.0942 0.8015]
94/100: train_loss=[8.724345  0.5590044], train_acc=[0.09385 0.79325]
96/100: train_loss=[8.636804  0.5405288], train_acc=[0.09275 0.8067 ]
98/100: train_loss=[8.619528  0.5384724], train_acc=[0.094   0.80805]
100/100: train_loss=[8.798807 0.555625], train_acc=[0.094 0.803]
**** Time taken for fashion_1 = 226.78695058822632
**** Time taken for fashion = 453.8678228855133
Preference Vector = [1. 0.]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.1619474 7.3969   ], train_acc=[0.6141  0.10415]
2/100: train_loss=[ 0.8759306 10.324477 ], train_acc=[0.70615 0.1044 ]
4/100: train_loss=[ 0.62307435 12.192277  ], train_acc=[0.7958  0.10445]
6/100: train_loss=[ 0.53130966 12.332367  ], train_acc=[0.82805 0.10575]
8/100: train_loss=[ 0.4580134 11.998714 ], train_acc=[0.85335 0.10675]
10/100: train_loss=[ 0.42281613 12.224424  ], train_acc=[0.86485 0.10725]
12/100: train_loss=[ 0.3877599 12.607436 ], train_acc=[0.8748  0.10575]
14/100: train_loss=[ 0.35993364 12.350728  ], train_acc=[0.8843  0.10465]
16/100: train_loss=[ 0.33253422 12.672562  ], train_acc=[0.8951 0.106 ]
18/100: train_loss=[ 0.33046284 12.836959  ], train_acc=[0.89395 0.1032 ]
20/100: train_loss=[ 0.29872045 12.72166   ], train_acc=[0.90545 0.1043 ]
22/100: train_loss=[ 0.2910534 12.290761 ], train_acc=[0.9074 0.1036]
24/100: train_loss=[ 0.27185422 12.813591  ], train_acc=[0.91345 0.10325]
26/100: train_loss=[ 0.26786456 12.755749  ], train_acc=[0.91535 0.1041 ]
28/100: train_loss=[ 0.25723344 12.801006  ], train_acc=[0.91875 0.10485]
30/100: train_loss=[ 0.25474954 12.73281   ], train_acc=[0.91935 0.1053 ]
32/100: train_loss=[ 0.24421978 12.932064  ], train_acc=[0.9222  0.10525]
34/100: train_loss=[ 0.2372334 12.870227 ], train_acc=[0.92455 0.1047 ]
36/100: train_loss=[ 0.23762543 12.850152  ], train_acc=[0.92325 0.102  ]
38/100: train_loss=[ 0.22941093 12.716573  ], train_acc=[0.92745 0.10355]
40/100: train_loss=[ 0.235311 12.856469], train_acc=[0.92495 0.10115]
42/100: train_loss=[ 0.22455308 12.308767  ], train_acc=[0.9293 0.1005]
44/100: train_loss=[ 0.23394941 13.560225  ], train_acc=[0.92645 0.10435]
46/100: train_loss=[ 0.21496677 13.046446  ], train_acc=[0.9329 0.1037]
48/100: train_loss=[ 0.21177723 13.362206  ], train_acc=[0.933   0.10255]
50/100: train_loss=[ 0.22522427 13.66098   ], train_acc=[0.9269 0.1018]
52/100: train_loss=[ 0.20690255 12.907146  ], train_acc=[0.93405 0.10135]
54/100: train_loss=[ 0.20231812 13.179037  ], train_acc=[0.93495 0.10185]
56/100: train_loss=[ 0.2041232 13.704723 ], train_acc=[0.93255 0.1022 ]
58/100: train_loss=[ 0.2003953 13.637782 ], train_acc=[0.9351  0.10155]
60/100: train_loss=[ 0.20013338 13.038636  ], train_acc=[0.9361  0.10095]
62/100: train_loss=[ 0.19302781 13.530257  ], train_acc=[0.93825 0.1023 ]
64/100: train_loss=[ 0.19549522 13.474089  ], train_acc=[0.9374  0.10195]
66/100: train_loss=[ 0.2070624 13.879435 ], train_acc=[0.9342  0.10285]
68/100: train_loss=[ 0.19269842 13.922722  ], train_acc=[0.9387 0.1011]
70/100: train_loss=[ 0.19214362 13.401207  ], train_acc=[0.9393 0.1039]
72/100: train_loss=[ 0.18998718 14.168122  ], train_acc=[0.9391  0.10175]
74/100: train_loss=[ 0.18968786 14.0700655 ], train_acc=[0.93965 0.1009 ]
76/100: train_loss=[ 0.18685734 13.804731  ], train_acc=[0.94035 0.10185]
78/100: train_loss=[ 0.19022559 14.585909  ], train_acc=[0.93895 0.10245]
80/100: train_loss=[ 0.188221 13.647617], train_acc=[0.93905 0.1016 ]
82/100: train_loss=[ 0.18733673 14.426411  ], train_acc=[0.94    0.10005]
84/100: train_loss=[ 0.18508342 14.28062   ], train_acc=[0.94065 0.10205]
86/100: train_loss=[ 0.18124084 13.932654  ], train_acc=[0.94265 0.10215]
88/100: train_loss=[ 0.17955624 14.286473  ], train_acc=[0.9428 0.1015]
90/100: train_loss=[ 0.18395169 14.111118  ], train_acc=[0.94205 0.1014 ]
92/100: train_loss=[ 0.18258938 14.360729  ], train_acc=[0.9416 0.1002]
94/100: train_loss=[ 0.18416427 14.697172  ], train_acc=[0.94175 0.1007 ]
96/100: train_loss=[ 0.18396135 14.491833  ], train_acc=[0.94195 0.1004 ]
98/100: train_loss=[ 0.18509299 14.896272  ], train_acc=[0.94115 0.1009 ]
100/100: train_loss=[ 0.19608262 14.240306  ], train_acc=[0.93705 0.09835]
**** Time taken for fashion_and_mnist_0 = 226.92407846450806
Preference Vector = [0. 1.]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[7.7458653 0.8905116], train_acc=[0.09885 0.66775]
2/100: train_loss=[7.6815944 0.7509536], train_acc=[0.10055 0.72275]
4/100: train_loss=[7.8376     0.66792166], train_acc=[0.1014  0.74925]
6/100: train_loss=[8.294374  0.6163257], train_acc=[0.10115 0.7695 ]
8/100: train_loss=[8.36752    0.59632665], train_acc=[0.1012  0.77555]
10/100: train_loss=[8.830524  0.5718883], train_acc=[0.10195 0.78685]
12/100: train_loss=[8.903432   0.56222665], train_acc=[0.10305 0.79145]
14/100: train_loss=[8.984728  0.5451469], train_acc=[0.10255 0.8008 ]
16/100: train_loss=[8.981974  0.5756806], train_acc=[0.104  0.7844]
18/100: train_loss=[9.112586   0.56774676], train_acc=[0.1043  0.78535]
20/100: train_loss=[8.666093   0.51894754], train_acc=[0.1048  0.80895]
22/100: train_loss=[8.692023   0.51719964], train_acc=[0.10665 0.8075 ]
24/100: train_loss=[8.414353  0.5083458], train_acc=[0.10775 0.81075]
26/100: train_loss=[8.518903   0.49068606], train_acc=[0.108   0.82115]
28/100: train_loss=[8.8012705  0.49216384], train_acc=[0.10655 0.8216 ]
30/100: train_loss=[8.347072  0.4856178], train_acc=[0.106  0.8188]
32/100: train_loss=[8.598355   0.47545248], train_acc=[0.10595 0.82505]
34/100: train_loss=[8.5981865  0.48376745], train_acc=[0.10685 0.8225 ]
36/100: train_loss=[8.896385   0.47160697], train_acc=[0.10425 0.82655]
38/100: train_loss=[8.570053 0.457517], train_acc=[0.1063  0.83405]
40/100: train_loss=[8.473079   0.46557203], train_acc=[0.1065  0.82955]
42/100: train_loss=[8.487678   0.45661145], train_acc=[0.1067 0.8329]
44/100: train_loss=[8.574786   0.45047376], train_acc=[0.10605 0.8358 ]
46/100: train_loss=[8.632921   0.44903576], train_acc=[0.1042 0.8344]
48/100: train_loss=[8.494022  0.4524953], train_acc=[0.1063  0.83375]
50/100: train_loss=[8.5379505  0.43864772], train_acc=[0.10565 0.8384 ]
52/100: train_loss=[8.538565   0.44455436], train_acc=[0.104   0.83565]
54/100: train_loss=[8.71728   0.4395158], train_acc=[0.10415 0.83995]
56/100: train_loss=[8.541808   0.43601266], train_acc=[0.10605 0.84165]
58/100: train_loss=[8.600154   0.44446874], train_acc=[0.1059  0.83415]
60/100: train_loss=[8.818042 0.439729], train_acc=[0.10545 0.8411 ]
62/100: train_loss=[8.73489    0.42222726], train_acc=[0.10585 0.84545]
64/100: train_loss=[8.73065    0.42889506], train_acc=[0.10475 0.84155]
66/100: train_loss=[8.626489   0.42095992], train_acc=[0.10545 0.8464 ]
68/100: train_loss=[8.659374   0.42266604], train_acc=[0.1061 0.8458]
70/100: train_loss=[8.748925   0.42249468], train_acc=[0.1056  0.84585]
72/100: train_loss=[8.670048  0.4170806], train_acc=[0.1051 0.8472]
74/100: train_loss=[8.661893   0.43608674], train_acc=[0.1055  0.83745]
76/100: train_loss=[8.684618   0.41489816], train_acc=[0.10585 0.84975]
78/100: train_loss=[8.642053   0.42210186], train_acc=[0.10565 0.8482 ]
80/100: train_loss=[8.931189   0.41506636], train_acc=[0.10605 0.8499 ]
82/100: train_loss=[8.661238   0.40952733], train_acc=[0.10665 0.84925]
84/100: train_loss=[8.700099  0.4136688], train_acc=[0.1056  0.84975]
86/100: train_loss=[9.005204   0.41409922], train_acc=[0.10655 0.84895]
88/100: train_loss=[8.604202   0.41245642], train_acc=[0.1045  0.84795]
90/100: train_loss=[8.689294  0.4144326], train_acc=[0.1057  0.84815]
92/100: train_loss=[8.826078   0.41371432], train_acc=[0.10465 0.85165]
94/100: train_loss=[8.81125   0.4093283], train_acc=[0.10575 0.85105]
96/100: train_loss=[8.810698   0.40832034], train_acc=[0.10455 0.85155]
98/100: train_loss=[8.785461   0.41381025], train_acc=[0.1073  0.84765]
100/100: train_loss=[8.924723  0.4063026], train_acc=[0.1054 0.8538]
**** Time taken for fashion_and_mnist_1 = 226.48955082893372
**** Time taken for fashion_and_mnist = 453.4229164123535
