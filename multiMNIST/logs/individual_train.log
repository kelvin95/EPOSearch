Preference Vector = [1. 0.]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.0043354 8.950572 ], train_acc=[0.6748  0.10655]
2/100: train_loss=[ 0.7624692 10.226028 ], train_acc=[0.75555 0.10615]
4/100: train_loss=[ 0.5917447 11.58917  ], train_acc=[0.80975 0.10145]
6/100: train_loss=[ 0.5053002 12.686679 ], train_acc=[0.83635 0.09955]
8/100: train_loss=[ 0.48032922 13.142305  ], train_acc=[0.84105 0.0995 ]
10/100: train_loss=[ 0.4275557 12.836503 ], train_acc=[0.8612 0.0985]
12/100: train_loss=[ 0.4228594 12.5991535], train_acc=[0.86255 0.09875]
14/100: train_loss=[ 0.3940789 12.691697 ], train_acc=[0.87045 0.09875]
16/100: train_loss=[ 0.38907418 12.746587  ], train_acc=[0.8736 0.0981]
18/100: train_loss=[ 0.38143674 13.072691  ], train_acc=[0.8743  0.09935]
20/100: train_loss=[ 0.36521327 13.114232  ], train_acc=[0.8821 0.0987]
22/100: train_loss=[ 0.3627937 13.215864 ], train_acc=[0.88145 0.0991 ]
24/100: train_loss=[ 0.35777006 12.921632  ], train_acc=[0.88335 0.0988 ]
26/100: train_loss=[ 0.34984097 13.056939  ], train_acc=[0.88625 0.0997 ]
28/100: train_loss=[ 0.33980036 12.689007  ], train_acc=[0.8894 0.0999]
30/100: train_loss=[ 0.35160163 12.780936  ], train_acc=[0.8846  0.09965]
32/100: train_loss=[ 0.33827245 12.999087  ], train_acc=[0.89015 0.0998 ]
34/100: train_loss=[ 0.3258766 12.729844 ], train_acc=[0.89325 0.09935]
36/100: train_loss=[ 0.33031675 12.8559675 ], train_acc=[0.8911 0.0997]
38/100: train_loss=[ 0.321632 12.941572], train_acc=[0.89555 0.1004 ]
40/100: train_loss=[ 0.32306823 13.068469  ], train_acc=[0.89265 0.10055]
42/100: train_loss=[ 0.31835356 13.275437  ], train_acc=[0.8964 0.1001]
44/100: train_loss=[ 0.32412985 13.082738  ], train_acc=[0.8941  0.10055]
46/100: train_loss=[ 0.31834403 13.488714  ], train_acc=[0.89605 0.1005 ]
48/100: train_loss=[ 0.31345722 13.263631  ], train_acc=[0.8978  0.10035]
50/100: train_loss=[ 0.32076228 12.812746  ], train_acc=[0.89595 0.1001 ]
52/100: train_loss=[ 0.31469342 13.604446  ], train_acc=[0.8972  0.10005]
54/100: train_loss=[ 0.31357387 13.059122  ], train_acc=[0.89865 0.0995 ]
56/100: train_loss=[ 0.3127501 13.179029 ], train_acc=[0.89825 0.10015]
58/100: train_loss=[ 0.3007806 13.173128 ], train_acc=[0.90145 0.10015]
60/100: train_loss=[ 0.30634987 13.395956  ], train_acc=[0.9012 0.1001]
62/100: train_loss=[ 0.30173224 13.402922  ], train_acc=[0.90195 0.10035]
64/100: train_loss=[ 0.29867133 13.819739  ], train_acc=[0.90205 0.10045]
66/100: train_loss=[ 0.29494172 13.862223  ], train_acc=[0.90335 0.10035]
68/100: train_loss=[ 0.2944511 13.309812 ], train_acc=[0.9025  0.10055]
70/100: train_loss=[ 0.29264656 13.5163145 ], train_acc=[0.9053 0.1   ]
72/100: train_loss=[ 0.2979989 13.623193 ], train_acc=[0.90305 0.10005]
74/100: train_loss=[ 0.30851868 13.481303  ], train_acc=[0.8997 0.1007]
76/100: train_loss=[ 0.28949445 13.510841  ], train_acc=[0.9059  0.09995]
78/100: train_loss=[ 0.30550858 13.68227   ], train_acc=[0.9008  0.10035]
80/100: train_loss=[ 0.30116785 13.6230755 ], train_acc=[0.90385 0.1    ]
82/100: train_loss=[ 0.2889146 13.813735 ], train_acc=[0.90765 0.10035]
84/100: train_loss=[ 0.28745696 13.211724  ], train_acc=[0.90705 0.1003 ]
86/100: train_loss=[ 0.289931 13.666773], train_acc=[0.90555 0.09995]
88/100: train_loss=[ 0.29050866 13.726807  ], train_acc=[0.9064 0.1002]
90/100: train_loss=[ 0.28940248 13.588363  ], train_acc=[0.9069 0.1001]
92/100: train_loss=[ 0.2869961 13.792468 ], train_acc=[0.90905 0.1    ]
94/100: train_loss=[ 0.29079452 13.577808  ], train_acc=[0.9069  0.10035]
96/100: train_loss=[ 0.3070407 13.650366 ], train_acc=[0.90355 0.10055]
98/100: train_loss=[ 0.28648958 13.668083  ], train_acc=[0.90915 0.10025]
100/100: train_loss=[ 0.2860064 13.593217 ], train_acc=[0.90825 0.10065]
**** Time taken for mnist_0 = 786.6991667747498
Preference Vector = [0. 1.]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[6.327219  1.1898224], train_acc=[0.09045 0.59655]
2/100: train_loss=[7.5371404  0.90140635], train_acc=[0.0992  0.70045]
4/100: train_loss=[8.458658  0.7063157], train_acc=[0.1035 0.7655]
6/100: train_loss=[8.994266 0.617911], train_acc=[0.1063  0.79445]
8/100: train_loss=[8.979789   0.57377684], train_acc=[0.10965 0.8052 ]
10/100: train_loss=[9.600966  0.5506876], train_acc=[0.10815 0.81585]
12/100: train_loss=[9.305378 0.510827], train_acc=[0.11    0.82905]
14/100: train_loss=[9.611308  0.5052231], train_acc=[0.107   0.83085]
16/100: train_loss=[9.528629   0.47071853], train_acc=[0.107  0.8424]
18/100: train_loss=[9.49949   0.4732894], train_acc=[0.10535 0.84125]
20/100: train_loss=[9.66373    0.44341955], train_acc=[0.1044  0.85175]
22/100: train_loss=[9.531672  0.4411983], train_acc=[0.1051  0.85395]
24/100: train_loss=[9.445706  0.4288474], train_acc=[0.1044 0.857 ]
26/100: train_loss=[9.541866   0.41495675], train_acc=[0.10435 0.86065]
28/100: train_loss=[9.70716    0.42066985], train_acc=[0.1023 0.8617]
30/100: train_loss=[9.652833   0.43243033], train_acc=[0.1035  0.85605]
32/100: train_loss=[9.817484   0.42047793], train_acc=[0.1029  0.85675]
34/100: train_loss=[9.700863  0.3908942], train_acc=[0.10385 0.8703 ]
36/100: train_loss=[9.898075 0.396326], train_acc=[0.1049 0.8655]
38/100: train_loss=[10.057174   0.3886496], train_acc=[0.1024 0.8701]
40/100: train_loss=[10.000593    0.40246657], train_acc=[0.10215 0.8662 ]
42/100: train_loss=[9.685496  0.3775663], train_acc=[0.10375 0.87415]
44/100: train_loss=[10.049415   0.3809558], train_acc=[0.10205 0.87435]
46/100: train_loss=[10.131649    0.38572845], train_acc=[0.10115 0.8707 ]
48/100: train_loss=[10.006925   0.3677136], train_acc=[0.1007 0.8765]
50/100: train_loss=[9.965931 0.370606], train_acc=[0.10215 0.8777 ]
52/100: train_loss=[9.970826   0.36414665], train_acc=[0.10155 0.8784 ]
54/100: train_loss=[9.842579  0.3607023], train_acc=[0.1033  0.87925]
56/100: train_loss=[9.993595   0.35748196], train_acc=[0.1021  0.87965]
58/100: train_loss=[9.847418   0.36190832], train_acc=[0.10295 0.87855]
60/100: train_loss=[10.245344   0.3557238], train_acc=[0.1024 0.8796]
62/100: train_loss=[10.034345    0.34947178], train_acc=[0.1014  0.88265]
64/100: train_loss=[10.15258     0.35008124], train_acc=[0.10025 0.8833 ]
66/100: train_loss=[10.414876    0.34870294], train_acc=[0.1005 0.8819]
68/100: train_loss=[10.338655   0.3706913], train_acc=[0.09955 0.8734 ]
70/100: train_loss=[10.164534    0.34807882], train_acc=[0.10095 0.8832 ]
72/100: train_loss=[10.36418    0.3462316], train_acc=[0.10075 0.8838 ]
74/100: train_loss=[10.352512    0.34798023], train_acc=[0.10105 0.88325]
76/100: train_loss=[10.712146    0.34694618], train_acc=[0.1001 0.8857]
78/100: train_loss=[10.464469    0.34247044], train_acc=[0.1002  0.88365]
80/100: train_loss=[10.603922   0.3412025], train_acc=[0.1004  0.88525]
82/100: train_loss=[10.427132    0.33377197], train_acc=[0.0998 0.8882]
84/100: train_loss=[10.497997   0.3435927], train_acc=[0.1    0.8849]
86/100: train_loss=[10.762963    0.33357447], train_acc=[0.099  0.8887]
88/100: train_loss=[10.601492    0.33816597], train_acc=[0.10045 0.8887 ]
90/100: train_loss=[10.540385   0.3415344], train_acc=[0.10095 0.886  ]
92/100: train_loss=[10.9379225   0.33531764], train_acc=[0.10005 0.8891 ]
94/100: train_loss=[10.562861   0.3322955], train_acc=[0.0999 0.888 ]
96/100: train_loss=[10.758096    0.33371335], train_acc=[0.1006 0.8889]
98/100: train_loss=[10.698771    0.32918566], train_acc=[0.10015 0.8887 ]
100/100: train_loss=[10.679329    0.32415506], train_acc=[0.10175 0.893  ]
**** Time taken for mnist_1 = 770.675751209259
**** Time taken for mnist = 1557.3812329769135
Preference Vector = [1. 0.]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[0.9771438 9.880488 ], train_acc=[0.6293 0.1006]
2/100: train_loss=[ 0.87595755 11.492664  ], train_acc=[0.66715 0.09945]
4/100: train_loss=[ 0.7744003 12.095307 ], train_acc=[0.708  0.0986]
6/100: train_loss=[ 0.71910423 12.385864  ], train_acc=[0.73185 0.09885]
8/100: train_loss=[ 0.69011915 12.521783  ], train_acc=[0.7415  0.09905]
10/100: train_loss=[ 0.6664376 13.582629 ], train_acc=[0.75405 0.099  ]
12/100: train_loss=[ 0.64135885 12.536056  ], train_acc=[0.7616  0.09895]
14/100: train_loss=[ 0.63317794 12.967273  ], train_acc=[0.76465 0.09905]
16/100: train_loss=[ 0.6124924 12.50316  ], train_acc=[0.7721 0.0987]
18/100: train_loss=[ 0.608998 12.164851], train_acc=[0.77285 0.09925]
20/100: train_loss=[ 0.6142716 12.83555  ], train_acc=[0.77005 0.0986 ]
22/100: train_loss=[ 0.5850827 12.766334 ], train_acc=[0.7827  0.09895]
24/100: train_loss=[ 0.6065693 12.791727 ], train_acc=[0.774  0.0984]
26/100: train_loss=[ 0.5689899 12.214892 ], train_acc=[0.78755 0.09895]
28/100: train_loss=[ 0.5664092 11.9814   ], train_acc=[0.78875 0.09975]
30/100: train_loss=[ 0.56577724 12.843981  ], train_acc=[0.78825 0.09895]
32/100: train_loss=[ 0.54933214 12.336552  ], train_acc=[0.79465 0.09955]
34/100: train_loss=[ 0.5461545 12.29527  ], train_acc=[0.79585 0.0991 ]
36/100: train_loss=[ 0.5401033 12.062421 ], train_acc=[0.7991 0.0998]
38/100: train_loss=[ 0.54832053 12.300531  ], train_acc=[0.79645 0.09975]
40/100: train_loss=[ 0.542222  11.9529705], train_acc=[0.79955 0.10095]
42/100: train_loss=[ 0.53879845 12.019368  ], train_acc=[0.7999 0.1003]
44/100: train_loss=[ 0.5272697 12.45985  ], train_acc=[0.8042 0.1002]
46/100: train_loss=[ 0.5195294 12.188462 ], train_acc=[0.8066 0.1005]
48/100: train_loss=[ 0.53783613 11.969793  ], train_acc=[0.8014 0.1015]
50/100: train_loss=[ 0.5310564 12.452736 ], train_acc=[0.8034 0.1003]
52/100: train_loss=[ 0.5146573 12.247221 ], train_acc=[0.80935 0.1008 ]
54/100: train_loss=[ 0.5179557 11.958752 ], train_acc=[0.8075 0.1009]
56/100: train_loss=[ 0.51236594 12.012622  ], train_acc=[0.81    0.10005]
58/100: train_loss=[ 0.514839 11.906624], train_acc=[0.8096 0.1012]
60/100: train_loss=[ 0.5136962 12.048096 ], train_acc=[0.8111 0.101 ]
62/100: train_loss=[ 0.5179176 12.168011 ], train_acc=[0.8081 0.1015]
64/100: train_loss=[ 0.50320405 11.903957  ], train_acc=[0.814   0.10185]
66/100: train_loss=[ 0.5022746 12.06046  ], train_acc=[0.81385 0.10085]
68/100: train_loss=[ 0.4999286 12.173361 ], train_acc=[0.81525 0.10065]
70/100: train_loss=[ 0.5030441 11.68012  ], train_acc=[0.8116  0.10125]
72/100: train_loss=[ 0.50555897 12.0463915 ], train_acc=[0.8132  0.10165]
74/100: train_loss=[ 0.49535686 12.013299  ], train_acc=[0.81775 0.10175]
76/100: train_loss=[ 0.49947107 12.11001   ], train_acc=[0.8147 0.1017]
78/100: train_loss=[ 0.49870571 12.216433  ], train_acc=[0.8165 0.1015]
80/100: train_loss=[ 0.4956078 11.74467  ], train_acc=[0.8167 0.1019]
82/100: train_loss=[ 0.4948546 11.890112 ], train_acc=[0.8177  0.10195]
84/100: train_loss=[ 0.49539158 12.428576  ], train_acc=[0.8199  0.10065]
86/100: train_loss=[ 0.4897583 12.365933 ], train_acc=[0.8187  0.10135]
88/100: train_loss=[ 0.4857207 12.117606 ], train_acc=[0.8201 0.1016]
90/100: train_loss=[ 0.4907471 12.182515 ], train_acc=[0.81815 0.10185]
92/100: train_loss=[ 0.4904426 12.090192 ], train_acc=[0.819   0.10135]
94/100: train_loss=[ 0.49012402 12.290897  ], train_acc=[0.82025 0.1012 ]
96/100: train_loss=[ 0.48031107 12.138969  ], train_acc=[0.82135 0.10185]
98/100: train_loss=[ 0.48334002 12.171912  ], train_acc=[0.82075 0.1018 ]
100/100: train_loss=[ 0.49119338 12.167364  ], train_acc=[0.8203  0.10205]
**** Time taken for fashion_0 = 770.050788640976
Preference Vector = [0. 1.]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[6.596778  1.1034167], train_acc=[0.07645 0.60095]
2/100: train_loss=[7.7245255 0.9255793], train_acc=[0.0801  0.65805]
4/100: train_loss=[8.036741  0.8077973], train_acc=[0.08155 0.7048 ]
6/100: train_loss=[8.297156   0.74775666], train_acc=[0.0831 0.7267]
8/100: train_loss=[8.263552   0.72774285], train_acc=[0.08575 0.7332 ]
10/100: train_loss=[8.095185  0.6815513], train_acc=[0.08645 0.7518 ]
12/100: train_loss=[8.31448   0.6729177], train_acc=[0.0888  0.75875]
14/100: train_loss=[8.070033  0.6602504], train_acc=[0.0897  0.76045]
16/100: train_loss=[8.026122   0.63949347], train_acc=[0.089   0.76485]
18/100: train_loss=[8.377073   0.65035665], train_acc=[0.08785 0.7648 ]
20/100: train_loss=[7.973763  0.6203227], train_acc=[0.0875  0.77625]
22/100: train_loss=[8.367369  0.6160153], train_acc=[0.0867 0.7764]
24/100: train_loss=[8.118048   0.59935707], train_acc=[0.08705 0.78075]
26/100: train_loss=[8.048155  0.5947488], train_acc=[0.087 0.782]
28/100: train_loss=[8.344822  0.6101539], train_acc=[0.0862  0.77265]
30/100: train_loss=[7.839901   0.58702844], train_acc=[0.08645 0.7835 ]
32/100: train_loss=[8.110078  0.5894601], train_acc=[0.0875 0.7829]
34/100: train_loss=[8.253167   0.58743864], train_acc=[0.08685 0.78415]
36/100: train_loss=[7.868079   0.57197744], train_acc=[0.0859  0.79225]
38/100: train_loss=[8.006058  0.5631773], train_acc=[0.0878 0.7938]
40/100: train_loss=[7.7798433 0.5674853], train_acc=[0.0871  0.79265]
42/100: train_loss=[7.832604  0.5641049], train_acc=[0.0877 0.7962]
44/100: train_loss=[7.896354   0.56101674], train_acc=[0.08645 0.79645]
46/100: train_loss=[7.83997  0.555145], train_acc=[0.08755 0.7985 ]
48/100: train_loss=[7.9236956 0.5522114], train_acc=[0.08715 0.7992 ]
50/100: train_loss=[7.9579864  0.54377156], train_acc=[0.0862 0.8017]
52/100: train_loss=[7.7211423 0.5638305], train_acc=[0.08625 0.7929 ]
54/100: train_loss=[7.8663216 0.5452036], train_acc=[0.08775 0.80265]
56/100: train_loss=[7.9031596 0.5569685], train_acc=[0.0879 0.7978]
58/100: train_loss=[8.009597  0.5553811], train_acc=[0.0865  0.79985]
60/100: train_loss=[7.843768  0.5377803], train_acc=[0.08625 0.8032 ]
62/100: train_loss=[7.779994  0.5481946], train_acc=[0.08615 0.802  ]
64/100: train_loss=[7.936859   0.53618896], train_acc=[0.0867  0.80495]
66/100: train_loss=[7.915305   0.54384863], train_acc=[0.0848  0.80185]
68/100: train_loss=[7.7298684 0.5291003], train_acc=[0.08675 0.80815]
70/100: train_loss=[8.017637   0.53252316], train_acc=[0.0865  0.80605]
72/100: train_loss=[7.937736  0.5258107], train_acc=[0.0864 0.81  ]
74/100: train_loss=[7.741962   0.53018665], train_acc=[0.08755 0.80725]
76/100: train_loss=[7.6767163 0.5449595], train_acc=[0.08825 0.8044 ]
78/100: train_loss=[8.003112   0.53471977], train_acc=[0.08675 0.80415]
80/100: train_loss=[7.9174805 0.5264226], train_acc=[0.0856  0.80945]
82/100: train_loss=[8.055018 0.526383], train_acc=[0.08555 0.8093 ]
84/100: train_loss=[7.9591107 0.519476 ], train_acc=[0.08515 0.8131 ]
86/100: train_loss=[7.9961543 0.5216474], train_acc=[0.08485 0.81115]
88/100: train_loss=[7.640666  0.5285987], train_acc=[0.0849 0.8081]
90/100: train_loss=[8.01128    0.51986104], train_acc=[0.084  0.8131]
92/100: train_loss=[8.082799  0.5147735], train_acc=[0.0831  0.81485]
94/100: train_loss=[7.9483027  0.51531136], train_acc=[0.0856 0.8152]
96/100: train_loss=[7.967939  0.5108147], train_acc=[0.0835  0.81615]
98/100: train_loss=[8.062253   0.52809393], train_acc=[0.0846  0.80945]
100/100: train_loss=[8.013585   0.51571816], train_acc=[0.08345 0.815  ]
**** Time taken for fashion_1 = 768.2987372875214
**** Time taken for fashion = 1538.3578667640686
Preference Vector = [1. 0.]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[0.9953374 4.692464 ], train_acc=[0.66525 0.08205]
2/100: train_loss=[0.7530291 5.7175407], train_acc=[0.7502 0.0808]
4/100: train_loss=[0.55260074 6.9380813 ], train_acc=[0.81635 0.08835]
6/100: train_loss=[0.45663655 7.8299084 ], train_acc=[0.8513  0.08925]
8/100: train_loss=[0.39812207 8.294329  ], train_acc=[0.87175 0.09   ]
10/100: train_loss=[0.36362442 8.515144  ], train_acc=[0.8816  0.09155]
12/100: train_loss=[0.3387558 9.063871 ], train_acc=[0.8909  0.09355]
14/100: train_loss=[0.33049458 8.85198   ], train_acc=[0.89045 0.09495]
16/100: train_loss=[0.3073195 9.290973 ], train_acc=[0.89875 0.0938 ]
18/100: train_loss=[0.28721228 9.1408    ], train_acc=[0.9038  0.09395]
20/100: train_loss=[0.26687965 9.294258  ], train_acc=[0.91185 0.0938 ]
22/100: train_loss=[0.2594223 9.351916 ], train_acc=[0.91395 0.0942 ]
24/100: train_loss=[0.247837 9.529337], train_acc=[0.91845 0.09405]
26/100: train_loss=[0.24011591 9.560106  ], train_acc=[0.92    0.09505]
28/100: train_loss=[0.23677513 9.873012  ], train_acc=[0.92145 0.09445]
30/100: train_loss=[0.23954542 9.555977  ], train_acc=[0.9209  0.09705]
32/100: train_loss=[ 0.22629514 10.085246  ], train_acc=[0.9261  0.09535]
34/100: train_loss=[0.22189915 9.775757  ], train_acc=[0.92895 0.0966 ]
36/100: train_loss=[0.21342884 9.976025  ], train_acc=[0.93045 0.096  ]
38/100: train_loss=[ 0.21535802 10.207759  ], train_acc=[0.93165 0.0979 ]
40/100: train_loss=[ 0.20347995 10.010759  ], train_acc=[0.93435 0.0969 ]
42/100: train_loss=[ 0.20660019 10.01509   ], train_acc=[0.93225 0.0953 ]
44/100: train_loss=[ 0.20023859 10.304792  ], train_acc=[0.93565 0.09585]
46/100: train_loss=[ 0.1996135 10.428348 ], train_acc=[0.93675 0.09665]
48/100: train_loss=[ 0.19471425 10.496276  ], train_acc=[0.93735 0.0957 ]
50/100: train_loss=[ 0.1984613 10.147159 ], train_acc=[0.93585 0.09665]
52/100: train_loss=[ 0.19293287 10.480854  ], train_acc=[0.9379  0.09585]
54/100: train_loss=[ 0.18839127 10.538314  ], train_acc=[0.9402  0.09615]
56/100: train_loss=[ 0.18820916 10.6124735 ], train_acc=[0.9395 0.0975]
58/100: train_loss=[ 0.18758747 10.9381895 ], train_acc=[0.94055 0.0963 ]
60/100: train_loss=[ 0.18627942 11.121218  ], train_acc=[0.941  0.0968]
62/100: train_loss=[ 0.18603186 10.986499  ], train_acc=[0.94115 0.09715]
64/100: train_loss=[ 0.18135764 11.003246  ], train_acc=[0.9416  0.09745]
66/100: train_loss=[ 0.17960383 10.96866   ], train_acc=[0.9434 0.098 ]
68/100: train_loss=[ 0.18184038 11.270856  ], train_acc=[0.94215 0.0987 ]
70/100: train_loss=[ 0.18714453 11.198472  ], train_acc=[0.9406 0.0989]
72/100: train_loss=[ 0.18411371 11.272141  ], train_acc=[0.9413  0.09775]
74/100: train_loss=[ 0.17727615 10.970625  ], train_acc=[0.9445  0.09875]
76/100: train_loss=[ 0.18257903 11.338321  ], train_acc=[0.94195 0.0982 ]
78/100: train_loss=[ 0.1796421 11.470832 ], train_acc=[0.9448  0.09855]
80/100: train_loss=[ 0.17562139 11.604101  ], train_acc=[0.9451  0.09915]
82/100: train_loss=[ 0.1758326 11.61523  ], train_acc=[0.9465  0.09845]
84/100: train_loss=[ 0.18301696 11.725651  ], train_acc=[0.94305 0.0991 ]
86/100: train_loss=[ 0.18001829 11.512769  ], train_acc=[0.9436 0.0986]
88/100: train_loss=[ 0.18163064 11.6025505 ], train_acc=[0.94405 0.09935]
90/100: train_loss=[ 0.17995527 11.886586  ], train_acc=[0.94335 0.0998 ]
92/100: train_loss=[ 0.17393981 11.781112  ], train_acc=[0.94775 0.10055]
94/100: train_loss=[ 0.17392828 11.782766  ], train_acc=[0.94605 0.09935]
96/100: train_loss=[ 0.17658399 11.864898  ], train_acc=[0.94565 0.0996 ]
98/100: train_loss=[ 0.18133058 11.838993  ], train_acc=[0.94355 0.09895]
100/100: train_loss=[ 0.17760746 11.872398  ], train_acc=[0.9454  0.09965]
**** Time taken for fashion_and_mnist_0 = 770.0431916713715
Preference Vector = [0. 1.]
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[5.8688507 0.9121289], train_acc=[0.0993 0.6659]
2/100: train_loss=[6.757305  0.8026583], train_acc=[0.0997  0.71045]
4/100: train_loss=[7.0292892  0.73458016], train_acc=[0.0979 0.7259]
6/100: train_loss=[6.697431   0.64495385], train_acc=[0.09615 0.76315]
8/100: train_loss=[6.3320107 0.6083585], train_acc=[0.09375 0.77535]
10/100: train_loss=[6.4760537  0.58323383], train_acc=[0.09295 0.7853 ]
12/100: train_loss=[7.3843894 0.5853228], train_acc=[0.0942 0.7896]
14/100: train_loss=[6.7112    0.5598478], train_acc=[0.0924 0.7908]
16/100: train_loss=[7.4275494  0.55738246], train_acc=[0.09375 0.7918 ]
18/100: train_loss=[7.0657697 0.5304796], train_acc=[0.0913  0.80405]
20/100: train_loss=[7.213684  0.5176214], train_acc=[0.0915  0.80855]
22/100: train_loss=[6.7679915 0.5211981], train_acc=[0.0904  0.80835]
24/100: train_loss=[7.698743  0.5114326], train_acc=[0.09215 0.81245]
26/100: train_loss=[7.607664   0.49657103], train_acc=[0.09315 0.81735]
28/100: train_loss=[8.013031   0.48885062], train_acc=[0.0919 0.8218]
30/100: train_loss=[7.944227   0.49054995], train_acc=[0.0918  0.81545]
32/100: train_loss=[9.095022   0.48516393], train_acc=[0.093  0.8221]
34/100: train_loss=[8.006728   0.47856402], train_acc=[0.091   0.82565]
36/100: train_loss=[8.353715   0.47273657], train_acc=[0.0923  0.82795]
38/100: train_loss=[7.95191    0.46618947], train_acc=[0.09095 0.83155]
40/100: train_loss=[8.684819   0.46189994], train_acc=[0.0924 0.831 ]
42/100: train_loss=[8.320548  0.4829235], train_acc=[0.09145 0.82085]
44/100: train_loss=[8.888837  0.4677302], train_acc=[0.0923  0.82825]
46/100: train_loss=[8.813303  0.4482873], train_acc=[0.0922  0.83725]
48/100: train_loss=[9.289261   0.45241645], train_acc=[0.09235 0.8344 ]
50/100: train_loss=[8.385395   0.46087956], train_acc=[0.0903  0.83275]
52/100: train_loss=[9.328136   0.46018925], train_acc=[0.0924  0.83205]
54/100: train_loss=[8.748976   0.43826854], train_acc=[0.09095 0.841  ]
56/100: train_loss=[9.341632   0.44462046], train_acc=[0.0917 0.8391]
58/100: train_loss=[8.9328575  0.43610024], train_acc=[0.09175 0.84105]
60/100: train_loss=[9.098996  0.4385055], train_acc=[0.09235 0.8389 ]
62/100: train_loss=[9.023945   0.43287376], train_acc=[0.0915  0.84385]
64/100: train_loss=[8.968948   0.43570068], train_acc=[0.0913  0.83885]
66/100: train_loss=[8.98881    0.42564672], train_acc=[0.091  0.8439]
68/100: train_loss=[8.938944   0.44523492], train_acc=[0.09035 0.8372 ]
70/100: train_loss=[9.142227   0.43202668], train_acc=[0.09165 0.8428 ]
72/100: train_loss=[8.572088   0.44039497], train_acc=[0.0896  0.84065]
74/100: train_loss=[9.503458   0.43450865], train_acc=[0.0923 0.8407]
76/100: train_loss=[9.498149   0.42815298], train_acc=[0.0918  0.84345]
78/100: train_loss=[9.313678   0.42211953], train_acc=[0.09225 0.84735]
80/100: train_loss=[9.072794   0.42453563], train_acc=[0.0909 0.8461]
82/100: train_loss=[9.567895   0.42151392], train_acc=[0.0926 0.847 ]
84/100: train_loss=[9.211157  0.4209488], train_acc=[0.09045 0.8471 ]
86/100: train_loss=[9.200735  0.4186106], train_acc=[0.09045 0.84605]
88/100: train_loss=[9.374274   0.42430976], train_acc=[0.09155 0.8455 ]
90/100: train_loss=[9.498622   0.43037996], train_acc=[0.0919 0.8434]
92/100: train_loss=[9.89974   0.4166595], train_acc=[0.09195 0.84905]
94/100: train_loss=[9.437273   0.41391692], train_acc=[0.09185 0.8497 ]
96/100: train_loss=[9.763826   0.41736475], train_acc=[0.09175 0.84885]
98/100: train_loss=[9.582181   0.41591564], train_acc=[0.0906  0.84865]
100/100: train_loss=[9.666341   0.41724318], train_acc=[0.09175 0.84735]
**** Time taken for fashion_and_mnist_1 = 769.0289976596832
**** Time taken for fashion_and_mnist = 1539.0793776512146
