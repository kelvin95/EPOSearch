Script started.
loading dataset mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.0339385 1.1985588], train_acc=[0.65775 0.5929 ]
2/100: train_loss=[0.8175562 0.9020655], train_acc=[0.73005 0.69505]
4/100: train_loss=[0.5872125 0.7336736], train_acc=[0.80745 0.75505]
6/100: train_loss=[0.5531577  0.63357735], train_acc=[0.8183 0.7849]
8/100: train_loss=[0.5078988 0.6051098], train_acc=[0.8323  0.79665]
10/100: train_loss=[0.4405125 0.5489023], train_acc=[0.8567  0.81565]
12/100: train_loss=[0.43771842 0.60075986], train_acc=[0.85675 0.7997 ]
14/100: train_loss=[0.4062404 0.5111734], train_acc=[0.8655  0.82895]
16/100: train_loss=[0.39452276 0.49011052], train_acc=[0.871   0.83335]
18/100: train_loss=[0.42839625 0.4753554 ], train_acc=[0.86    0.83955]
20/100: train_loss=[0.3732951 0.4578701], train_acc=[0.87975 0.84545]
22/100: train_loss=[0.37973076 0.4679943 ], train_acc=[0.87615 0.84145]
24/100: train_loss=[0.36268437 0.4472614 ], train_acc=[0.8799  0.84985]
26/100: train_loss=[0.35045564 0.4278062 ], train_acc=[0.88435 0.85565]
28/100: train_loss=[0.35157344 0.4261915 ], train_acc=[0.8826  0.85775]
30/100: train_loss=[0.34298572 0.42027834], train_acc=[0.88825 0.85775]
32/100: train_loss=[0.41042054 0.45046818], train_acc=[0.8687  0.84685]
34/100: train_loss=[0.33478707 0.4003235 ], train_acc=[0.88925 0.8661 ]
36/100: train_loss=[0.3281868  0.39702922], train_acc=[0.8931  0.86605]
38/100: train_loss=[0.32506722 0.4035277 ], train_acc=[0.89295 0.8644 ]
40/100: train_loss=[0.3228475  0.38742486], train_acc=[0.8947  0.87065]
42/100: train_loss=[0.3223712 0.4061909], train_acc=[0.89375 0.86135]
44/100: train_loss=[0.31309122 0.3843863 ], train_acc=[0.8957  0.87075]
46/100: train_loss=[0.308211   0.37467358], train_acc=[0.89815 0.87715]
48/100: train_loss=[0.32437298 0.37282547], train_acc=[0.8924 0.874 ]
50/100: train_loss=[0.31525427 0.38465938], train_acc=[0.8965 0.8728]
52/100: train_loss=[0.31804654 0.38261217], train_acc=[0.89725 0.87175]
54/100: train_loss=[0.31317872 0.36898878], train_acc=[0.8955  0.87495]
56/100: train_loss=[0.31017584 0.37203354], train_acc=[0.89635 0.87575]
58/100: train_loss=[0.30192804 0.3698858 ], train_acc=[0.90095 0.87615]
60/100: train_loss=[0.30368745 0.3706815 ], train_acc=[0.90035 0.87855]
62/100: train_loss=[0.29589784 0.3528978 ], train_acc=[0.9032 0.8828]
64/100: train_loss=[0.29885504 0.3536809 ], train_acc=[0.90385 0.8815 ]
66/100: train_loss=[0.30328697 0.36599463], train_acc=[0.90155 0.87785]
68/100: train_loss=[0.30004975 0.35987604], train_acc=[0.9019  0.88055]
70/100: train_loss=[0.32753333 0.35807863], train_acc=[0.89055 0.88255]
72/100: train_loss=[0.29348665 0.34979132], train_acc=[0.9049  0.88405]
74/100: train_loss=[0.29363808 0.35444313], train_acc=[0.90555 0.88355]
76/100: train_loss=[0.2927261  0.34498134], train_acc=[0.9028 0.8853]
78/100: train_loss=[0.32080486 0.3604374 ], train_acc=[0.8983 0.8806]
80/100: train_loss=[0.28651464 0.34878337], train_acc=[0.90645 0.8836 ]
82/100: train_loss=[0.29550958 0.34492528], train_acc=[0.90335 0.88515]
84/100: train_loss=[0.2883227  0.33989015], train_acc=[0.9063  0.88535]
86/100: train_loss=[0.2881823  0.33928087], train_acc=[0.90645 0.88745]
88/100: train_loss=[0.28940856 0.33736154], train_acc=[0.90595 0.8876 ]
90/100: train_loss=[0.2861343  0.33688506], train_acc=[0.90855 0.8873 ]
92/100: train_loss=[0.28660148 0.3333288 ], train_acc=[0.90625 0.88815]
94/100: train_loss=[0.2839486 0.3401912], train_acc=[0.9081  0.88875]
96/100: train_loss=[0.2885967 0.3372269], train_acc=[0.9072 0.8892]
98/100: train_loss=[0.28011343 0.34149107], train_acc=[0.9081 0.8864]
100/100: train_loss=[0.28604874 0.332235  ], train_acc=[0.9075 0.8898]
**** Time taken for mnist_0 = 4034.4823293685913
loading dataset mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[0.9938435 1.2353885], train_acc=[0.67235 0.5876 ]
2/100: train_loss=[0.7705671 0.929912 ], train_acc=[0.7466 0.6908]
4/100: train_loss=[0.6061557 0.7452873], train_acc=[0.80005 0.7519 ]
6/100: train_loss=[0.53307134 0.6413228 ], train_acc=[0.82545 0.78595]
8/100: train_loss=[0.46875972 0.5595705 ], train_acc=[0.8445  0.81405]
10/100: train_loss=[0.4382205  0.53446275], train_acc=[0.85475 0.8198 ]
12/100: train_loss=[0.42567924 0.5077804 ], train_acc=[0.86135 0.8282 ]
14/100: train_loss=[0.41181728 0.48387808], train_acc=[0.86455 0.83875]
16/100: train_loss=[0.38506323 0.46368563], train_acc=[0.87445 0.8445 ]
18/100: train_loss=[0.39077574 0.44963315], train_acc=[0.87395 0.8497 ]
20/100: train_loss=[0.37071112 0.4452577 ], train_acc=[0.8787  0.85105]
22/100: train_loss=[0.3659804  0.44120803], train_acc=[0.882   0.85235]
24/100: train_loss=[0.35468462 0.42226648], train_acc=[0.8854  0.85995]
26/100: train_loss=[0.34974834 0.42185783], train_acc=[0.8864 0.8572]
28/100: train_loss=[0.38087487 0.43405727], train_acc=[0.87795 0.85645]
30/100: train_loss=[0.34458914 0.40184352], train_acc=[0.8895  0.86555]
32/100: train_loss=[0.60679114 0.625689  ], train_acc=[0.80395 0.79185]
34/100: train_loss=[0.39378744 0.44014895], train_acc=[0.87535 0.85515]
36/100: train_loss=[0.36231554 0.41407648], train_acc=[0.8839  0.86305]
38/100: train_loss=[0.35377043 0.4089715 ], train_acc=[0.8863  0.86355]
40/100: train_loss=[0.36129987 0.410871  ], train_acc=[0.8848 0.8633]
42/100: train_loss=[0.33906555 0.39551738], train_acc=[0.8918  0.86915]
44/100: train_loss=[0.3339578 0.3934917], train_acc=[0.8929 0.8696]
46/100: train_loss=[0.3247384  0.38530254], train_acc=[0.8967 0.8712]
48/100: train_loss=[0.33015874 0.3792244 ], train_acc=[0.89465 0.874  ]
50/100: train_loss=[0.31897315 0.37779665], train_acc=[0.89755 0.87525]
52/100: train_loss=[0.32440272 0.373847  ], train_acc=[0.89545 0.87405]
54/100: train_loss=[0.3150409 0.3703209], train_acc=[0.8998  0.87785]
56/100: train_loss=[0.32074037 0.3731124 ], train_acc=[0.89765 0.8762 ]
58/100: train_loss=[0.45724034 0.4646171 ], train_acc=[0.8544 0.8429]
60/100: train_loss=[0.3213319  0.36774364], train_acc=[0.8969  0.87785]
62/100: train_loss=[0.30655193 0.3651786 ], train_acc=[0.90135 0.8781 ]
64/100: train_loss=[0.310135   0.38241416], train_acc=[0.89975 0.873  ]
66/100: train_loss=[0.30743808 0.36724666], train_acc=[0.90075 0.87705]
68/100: train_loss=[0.30345798 0.35477853], train_acc=[0.90115 0.882  ]
70/100: train_loss=[0.31006926 0.3645059 ], train_acc=[0.8997  0.87865]
72/100: train_loss=[0.29861563 0.36795616], train_acc=[0.9035  0.87785]
74/100: train_loss=[0.30197656 0.3654059 ], train_acc=[0.9026 0.8777]
76/100: train_loss=[0.30421153 0.3577894 ], train_acc=[0.90225 0.88065]
78/100: train_loss=[0.30082965 0.3534416 ], train_acc=[0.90335 0.8842 ]
80/100: train_loss=[0.29381052 0.35150248], train_acc=[0.9042  0.88355]
82/100: train_loss=[0.29867873 0.35716698], train_acc=[0.9036 0.8809]
84/100: train_loss=[0.29347494 0.35978442], train_acc=[0.90435 0.8815 ]
86/100: train_loss=[0.29204682 0.35491332], train_acc=[0.9046  0.88365]
88/100: train_loss=[0.28954205 0.36096606], train_acc=[0.9069  0.88105]
90/100: train_loss=[0.29279366 0.3465103 ], train_acc=[0.90485 0.8866 ]
92/100: train_loss=[0.29351598 0.34180513], train_acc=[0.90435 0.88705]
94/100: train_loss=[0.29061636 0.34009513], train_acc=[0.9057 0.8885]
96/100: train_loss=[0.28684068 0.34760028], train_acc=[0.9074 0.8846]
98/100: train_loss=[0.2896241  0.36735895], train_acc=[0.9052 0.8756]
100/100: train_loss=[0.28833055 0.36590308], train_acc=[0.90565 0.8798 ]
**** Time taken for mnist_1 = 2929.3107104301453
loading dataset mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[0.9938265 1.2757181], train_acc=[0.67025 0.57415]
2/100: train_loss=[0.7800227 0.9420544], train_acc=[0.7452 0.6851]
4/100: train_loss=[0.62070656 0.71414226], train_acc=[0.7975 0.7619]
6/100: train_loss=[0.56748456 0.6275827 ], train_acc=[0.81485 0.78975]
8/100: train_loss=[0.51851034 0.57754016], train_acc=[0.83195 0.8085 ]
10/100: train_loss=[0.4934609 0.5414138], train_acc=[0.839   0.82275]
12/100: train_loss=[0.48255983 0.5561907 ], train_acc=[0.8421  0.81135]
14/100: train_loss=[0.45284605 0.5020479 ], train_acc=[0.854   0.83215]
16/100: train_loss=[0.4324368 0.4935537], train_acc=[0.8586 0.834 ]
18/100: train_loss=[0.40130198 0.48448715], train_acc=[0.8703 0.8413]
20/100: train_loss=[0.39947385 0.47813258], train_acc=[0.8679  0.84205]
22/100: train_loss=[0.39118308 0.4593122 ], train_acc=[0.8751 0.8475]
24/100: train_loss=[0.39648396 0.47048008], train_acc=[0.87275 0.84465]
26/100: train_loss=[0.37060025 0.44387892], train_acc=[0.8774  0.85335]
28/100: train_loss=[0.3699185  0.46201283], train_acc=[0.8799 0.8463]
30/100: train_loss=[0.35901618 0.4376427 ], train_acc=[0.8838  0.85535]
32/100: train_loss=[0.3784879  0.44924816], train_acc=[0.8778  0.84945]
34/100: train_loss=[0.34348142 0.42116672], train_acc=[0.88925 0.85865]
36/100: train_loss=[0.35795832 0.45170343], train_acc=[0.88245 0.84865]
38/100: train_loss=[0.34033296 0.4109782 ], train_acc=[0.88755 0.86275]
40/100: train_loss=[0.3385946  0.40516192], train_acc=[0.8878 0.8646]
42/100: train_loss=[0.33366558 0.40630472], train_acc=[0.8903 0.8645]
44/100: train_loss=[0.33817634 0.39669177], train_acc=[0.8882 0.868 ]
46/100: train_loss=[0.3421218  0.40636918], train_acc=[0.8876  0.86715]
48/100: train_loss=[0.32597676 0.39400405], train_acc=[0.8929  0.86845]
50/100: train_loss=[0.32887053 0.4137215 ], train_acc=[0.89235 0.8629 ]
52/100: train_loss=[0.31981364 0.39511904], train_acc=[0.8946 0.8681]
54/100: train_loss=[0.32185295 0.38457447], train_acc=[0.8952 0.8718]
56/100: train_loss=[0.31831917 0.39676675], train_acc=[0.8963  0.86855]
58/100: train_loss=[0.32812193 0.41110656], train_acc=[0.89255 0.86275]
60/100: train_loss=[0.32147446 0.38294438], train_acc=[0.8952 0.873 ]
62/100: train_loss=[0.30615535 0.37462094], train_acc=[0.9002  0.87485]
64/100: train_loss=[0.3318406  0.39091343], train_acc=[0.8909 0.8724]
66/100: train_loss=[0.35787854 0.44214612], train_acc=[0.8845  0.85375]
68/100: train_loss=[0.3024606  0.37544763], train_acc=[0.90175 0.8766 ]
70/100: train_loss=[0.32308736 0.37793866], train_acc=[0.894   0.87605]
72/100: train_loss=[0.30389145 0.36111015], train_acc=[0.9016 0.8818]
74/100: train_loss=[0.3032403  0.40138438], train_acc=[0.9006 0.8672]
76/100: train_loss=[0.29402018 0.36199504], train_acc=[0.9055 0.8811]
78/100: train_loss=[0.31585497 0.43324652], train_acc=[0.89705 0.8586 ]
80/100: train_loss=[0.30050504 0.35797337], train_acc=[0.9034  0.88215]
82/100: train_loss=[0.2973448  0.37419003], train_acc=[0.90375 0.8781 ]
84/100: train_loss=[0.32346508 0.37123346], train_acc=[0.8953  0.87675]
86/100: train_loss=[0.2962835 0.3534842], train_acc=[0.9045 0.8833]
88/100: train_loss=[0.31506702 0.35953456], train_acc=[0.89775 0.8806 ]
90/100: train_loss=[0.3291106  0.36031014], train_acc=[0.89485 0.88035]
92/100: train_loss=[0.31247398 0.35056305], train_acc=[0.8967 0.8842]
94/100: train_loss=[0.29213184 0.36568543], train_acc=[0.9064  0.87965]
96/100: train_loss=[0.34834248 0.3602183 ], train_acc=[0.8913  0.88175]
98/100: train_loss=[0.30032122 0.36130866], train_acc=[0.90385 0.87935]
100/100: train_loss=[0.2902405 0.3527884], train_acc=[0.9065 0.8856]
**** Time taken for mnist_2 = 2574.0498507022858
loading dataset mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.0868636 1.4991158], train_acc=[0.6423  0.49325]
2/100: train_loss=[0.8403498 1.1120374], train_acc=[0.72495 0.6298 ]
4/100: train_loss=[0.66410786 0.88522846], train_acc=[0.78135 0.70295]
6/100: train_loss=[0.58101773 0.7586889 ], train_acc=[0.80845 0.74455]
8/100: train_loss=[0.52733976 0.66238403], train_acc=[0.8262  0.77945]
10/100: train_loss=[0.4894099 0.6218679], train_acc=[0.8386  0.79055]
12/100: train_loss=[0.46401075 0.58698976], train_acc=[0.8496 0.8038]
14/100: train_loss=[0.44336325 0.58178663], train_acc=[0.8548  0.80535]
16/100: train_loss=[0.43672866 0.54951245], train_acc=[0.8575  0.81555]
18/100: train_loss=[0.4186918 0.5286165], train_acc=[0.86205 0.82445]
20/100: train_loss=[0.40965027 0.5314895 ], train_acc=[0.86735 0.82475]
22/100: train_loss=[0.4039021  0.51076424], train_acc=[0.86655 0.82845]
24/100: train_loss=[0.41154245 0.4891716 ], train_acc=[0.8647 0.8358]
26/100: train_loss=[0.3826528  0.48436558], train_acc=[0.87475 0.83905]
28/100: train_loss=[0.37849662 0.48226583], train_acc=[0.87735 0.8379 ]
30/100: train_loss=[0.3733051  0.47180766], train_acc=[0.8781 0.8448]
32/100: train_loss=[0.3929344  0.48500434], train_acc=[0.8728  0.83955]
34/100: train_loss=[0.39497086 0.45964998], train_acc=[0.8689  0.84835]
36/100: train_loss=[0.37202892 0.46333927], train_acc=[0.877   0.84855]
38/100: train_loss=[0.35385275 0.44863266], train_acc=[0.88315 0.85095]
40/100: train_loss=[0.35875532 0.4394133 ], train_acc=[0.8847  0.85595]
42/100: train_loss=[0.38674176 0.45519233], train_acc=[0.87305 0.8513 ]
44/100: train_loss=[0.34253407 0.42982635], train_acc=[0.88735 0.86035]
46/100: train_loss=[0.33937368 0.424419  ], train_acc=[0.88805 0.86285]
48/100: train_loss=[0.34992167 0.427697  ], train_acc=[0.8858  0.86095]
50/100: train_loss=[0.34522003 0.43520874], train_acc=[0.8862 0.8574]
52/100: train_loss=[0.33864167 0.42163926], train_acc=[0.8887 0.8634]
54/100: train_loss=[0.32547045 0.40577158], train_acc=[0.89375 0.8665 ]
56/100: train_loss=[0.32560575 0.40605536], train_acc=[0.89465 0.86775]
58/100: train_loss=[0.32420167 0.40882123], train_acc=[0.89475 0.86585]
60/100: train_loss=[0.31926483 0.4032615 ], train_acc=[0.89445 0.86795]
62/100: train_loss=[0.3159705  0.40709913], train_acc=[0.8958 0.8681]
64/100: train_loss=[0.3176921 0.3972947], train_acc=[0.8961 0.8701]
66/100: train_loss=[0.31880167 0.40059957], train_acc=[0.8958  0.87165]
68/100: train_loss=[0.32053965 0.39408815], train_acc=[0.89495 0.87075]
70/100: train_loss=[0.31522    0.39241993], train_acc=[0.89665 0.87205]
72/100: train_loss=[0.3065806 0.3849238], train_acc=[0.89945 0.8734 ]
74/100: train_loss=[0.30987477 0.40063274], train_acc=[0.89815 0.8696 ]
76/100: train_loss=[0.30849043 0.38396218], train_acc=[0.89825 0.8758 ]
78/100: train_loss=[0.3063943 0.387569 ], train_acc=[0.9003 0.8742]
80/100: train_loss=[0.30189228 0.38457322], train_acc=[0.89995 0.87605]
82/100: train_loss=[0.320333  0.4104215], train_acc=[0.89435 0.86575]
84/100: train_loss=[0.30455568 0.3908524 ], train_acc=[0.90085 0.8739 ]
86/100: train_loss=[0.29888922 0.37533453], train_acc=[0.902  0.8765]
88/100: train_loss=[0.30196372 0.38406765], train_acc=[0.90055 0.87495]
90/100: train_loss=[0.2938045  0.37667057], train_acc=[0.9044 0.8776]
92/100: train_loss=[0.30128464 0.38376686], train_acc=[0.90105 0.87455]
94/100: train_loss=[0.29481643 0.36536202], train_acc=[0.90295 0.8831 ]
96/100: train_loss=[0.2937589  0.36815023], train_acc=[0.9048  0.88065]
98/100: train_loss=[0.3074965 0.3867766], train_acc=[0.89825 0.87315]
100/100: train_loss=[0.2866793  0.36321545], train_acc=[0.9062  0.88245]
**** Time taken for mnist_3 = 2469.722951889038
loading dataset mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[0.9611674 1.1525238], train_acc=[0.68745 0.61695]
2/100: train_loss=[0.7257235 0.8619358], train_acc=[0.7631  0.71325]
4/100: train_loss=[0.57330424 0.71530694], train_acc=[0.81615 0.7636 ]
6/100: train_loss=[0.5244571 0.657139 ], train_acc=[0.83005 0.77895]
8/100: train_loss=[0.49152133 0.6055035 ], train_acc=[0.8414 0.8011]
10/100: train_loss=[0.44250828 0.5675527 ], train_acc=[0.8568  0.81135]
12/100: train_loss=[0.42498848 0.5484177 ], train_acc=[0.86235 0.8176 ]
14/100: train_loss=[0.43790185 0.5547545 ], train_acc=[0.8592 0.8176]
16/100: train_loss=[0.4084517  0.50876784], train_acc=[0.86915 0.8328 ]
18/100: train_loss=[0.4107619 0.511578 ], train_acc=[0.869   0.83305]
20/100: train_loss=[0.3921266  0.49447352], train_acc=[0.8733  0.83735]
22/100: train_loss=[0.39681375 0.48285332], train_acc=[0.87325 0.84175]
24/100: train_loss=[0.36533448 0.46621093], train_acc=[0.881  0.8475]
26/100: train_loss=[0.3576291 0.46364  ], train_acc=[0.8827 0.8482]
28/100: train_loss=[0.3682    0.4477488], train_acc=[0.87815 0.85185]
30/100: train_loss=[0.35223934 0.43880194], train_acc=[0.88495 0.8576 ]
32/100: train_loss=[0.34713516 0.42985737], train_acc=[0.8865  0.85855]
34/100: train_loss=[0.34404093 0.44002384], train_acc=[0.8886 0.855 ]
36/100: train_loss=[0.34778887 0.44031233], train_acc=[0.8873 0.8538]
38/100: train_loss=[0.33866835 0.42655665], train_acc=[0.8892 0.8578]
40/100: train_loss=[0.3289134  0.41773224], train_acc=[0.89335 0.8616 ]
42/100: train_loss=[0.33985    0.40519363], train_acc=[0.88865 0.86445]
44/100: train_loss=[0.32530466 0.39722046], train_acc=[0.8927  0.86935]
46/100: train_loss=[0.3295127  0.39756152], train_acc=[0.8908 0.8677]
48/100: train_loss=[0.33063364 0.39713433], train_acc=[0.89005 0.86675]
50/100: train_loss=[0.34740055 0.41653115], train_acc=[0.8855  0.86135]
52/100: train_loss=[0.3151919  0.38935792], train_acc=[0.89725 0.8696 ]
54/100: train_loss=[0.33100298 0.39723253], train_acc=[0.89225 0.86685]
56/100: train_loss=[0.33016452 0.38929975], train_acc=[0.89075 0.8697 ]
58/100: train_loss=[0.3139587  0.37092215], train_acc=[0.89695 0.8768 ]
60/100: train_loss=[0.31240228 0.4058774 ], train_acc=[0.8986 0.8672]
62/100: train_loss=[0.31775185 0.38220686], train_acc=[0.89485 0.87055]
64/100: train_loss=[0.3099712 0.3697988], train_acc=[0.89725 0.87625]
66/100: train_loss=[0.30943552 0.36486053], train_acc=[0.89915 0.879  ]
68/100: train_loss=[0.30775607 0.36754668], train_acc=[0.8978  0.87725]
70/100: train_loss=[0.30892774 0.35909453], train_acc=[0.8981  0.88105]
72/100: train_loss=[0.29493368 0.3574333 ], train_acc=[0.9031  0.88005]
74/100: train_loss=[0.31023988 0.36943674], train_acc=[0.89655 0.8767 ]
76/100: train_loss=[0.30201888 0.3572788 ], train_acc=[0.90095 0.88175]
78/100: train_loss=[0.29321057 0.35454202], train_acc=[0.9038 0.8822]
80/100: train_loss=[0.30279374 0.35933593], train_acc=[0.9001 0.8796]
82/100: train_loss=[0.29606605 0.36204222], train_acc=[0.90395 0.8805 ]
84/100: train_loss=[0.30255583 0.3567368 ], train_acc=[0.901   0.88105]
86/100: train_loss=[0.28847942 0.34561986], train_acc=[0.9042  0.88325]
88/100: train_loss=[0.30041346 0.361929  ], train_acc=[0.8996 0.8789]
90/100: train_loss=[0.28851387 0.34004173], train_acc=[0.90485 0.88735]
92/100: train_loss=[0.29167017 0.3434044 ], train_acc=[0.90445 0.8857 ]
94/100: train_loss=[0.29768738 0.34136394], train_acc=[0.901  0.8863]
96/100: train_loss=[0.28268299 0.34242043], train_acc=[0.9068 0.885 ]
98/100: train_loss=[0.29160205 0.34734586], train_acc=[0.90335 0.886  ]
100/100: train_loss=[0.29225996 0.33330575], train_acc=[0.9022  0.88865]
**** Time taken for mnist_4 = 2363.979596853256
**** Time taken for mnist = 14371.63483285904
loading dataset fashion
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.1183136 1.152781 ], train_acc=[0.5924 0.5871]
2/100: train_loss=[0.94527334 0.9591785 ], train_acc=[0.64915 0.65785]
4/100: train_loss=[0.8603877 0.84179  ], train_acc=[0.67825 0.69295]
6/100: train_loss=[0.7873604 0.757136 ], train_acc=[0.71025 0.72225]
8/100: train_loss=[0.74091893 0.7289109 ], train_acc=[0.7243 0.7318]
10/100: train_loss=[0.71709204 0.6978822 ], train_acc=[0.7321 0.7441]
12/100: train_loss=[0.6941497  0.68138707], train_acc=[0.7423  0.74995]
14/100: train_loss=[0.6679191  0.66682404], train_acc=[0.74915 0.75225]
16/100: train_loss=[0.67418075 0.6580851 ], train_acc=[0.74465 0.75805]
18/100: train_loss=[0.63929695 0.63615084], train_acc=[0.76075 0.7654 ]
20/100: train_loss=[0.63785356 0.6509345 ], train_acc=[0.761  0.7625]
22/100: train_loss=[0.6280698 0.6348265], train_acc=[0.76705 0.76565]
24/100: train_loss=[0.630177   0.62486947], train_acc=[0.76495 0.7718 ]
26/100: train_loss=[0.61786115 0.6140894 ], train_acc=[0.7727 0.775 ]
28/100: train_loss=[0.59843135 0.60331506], train_acc=[0.77785 0.7812 ]
30/100: train_loss=[0.5929131 0.6088056], train_acc=[0.7837  0.77885]
32/100: train_loss=[0.58661187 0.5881599 ], train_acc=[0.7843 0.7866]
34/100: train_loss=[0.61962265 0.5972103 ], train_acc=[0.76965 0.7821 ]
36/100: train_loss=[0.5874188 0.5879436], train_acc=[0.7827  0.78595]
38/100: train_loss=[0.58207196 0.57927996], train_acc=[0.78375 0.7906 ]
40/100: train_loss=[0.57776034 0.57093686], train_acc=[0.786  0.7928]
42/100: train_loss=[0.58136576 0.576992  ], train_acc=[0.7882 0.7898]
44/100: train_loss=[0.57371503 0.56598485], train_acc=[0.7879 0.7944]
46/100: train_loss=[0.5679506 0.5629712], train_acc=[0.79075 0.7949 ]
48/100: train_loss=[0.562096   0.55692726], train_acc=[0.79085 0.79915]
50/100: train_loss=[0.5551081 0.5662574], train_acc=[0.7956  0.79165]
52/100: train_loss=[0.5698791  0.56012625], train_acc=[0.7841 0.7932]
54/100: train_loss=[0.5615309 0.5517639], train_acc=[0.79115 0.799  ]
56/100: train_loss=[0.5537138 0.5576897], train_acc=[0.7929  0.79685]
58/100: train_loss=[0.5512715 0.5549725], train_acc=[0.79815 0.79735]
60/100: train_loss=[0.5507616 0.5602536], train_acc=[0.7989  0.79915]
62/100: train_loss=[0.54184514 0.55335736], train_acc=[0.79925 0.7988 ]
64/100: train_loss=[0.56337756 0.5503545 ], train_acc=[0.79595 0.7996 ]
66/100: train_loss=[0.5439641 0.5485249], train_acc=[0.80065 0.80195]
68/100: train_loss=[0.54471684 0.5413538 ], train_acc=[0.7995 0.8044]
70/100: train_loss=[0.54570585 0.54881   ], train_acc=[0.7978  0.79965]
72/100: train_loss=[0.54042655 0.54357344], train_acc=[0.80025 0.8044 ]
74/100: train_loss=[0.53347486 0.5443843 ], train_acc=[0.8018 0.8043]
76/100: train_loss=[0.53532356 0.5464609 ], train_acc=[0.8003  0.80165]
78/100: train_loss=[0.5427845  0.55001837], train_acc=[0.7997  0.80015]
80/100: train_loss=[0.5392562  0.54245055], train_acc=[0.7997 0.8052]
82/100: train_loss=[0.54076356 0.5420153 ], train_acc=[0.80205 0.8057 ]
84/100: train_loss=[0.54121923 0.55034274], train_acc=[0.7995  0.80035]
86/100: train_loss=[0.53664774 0.5402987 ], train_acc=[0.8009  0.80545]
88/100: train_loss=[0.5333084 0.5366877], train_acc=[0.8052  0.80485]
90/100: train_loss=[0.5389276 0.5389204], train_acc=[0.8026  0.80875]
92/100: train_loss=[0.53423506 0.54831433], train_acc=[0.802  0.7983]
94/100: train_loss=[0.531336  0.5379544], train_acc=[0.80445 0.80315]
96/100: train_loss=[0.5334433  0.53805614], train_acc=[0.80265 0.8067 ]
98/100: train_loss=[0.53180027 0.54188794], train_acc=[0.8044  0.80565]
100/100: train_loss=[0.5363379 0.5336655], train_acc=[0.8061  0.80645]
**** Time taken for fashion_0 = 2366.4652264118195
loading dataset fashion
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.2347322 1.1987714], train_acc=[0.5544 0.5555]
2/100: train_loss=[0.9896296 1.0319988], train_acc=[0.6301 0.6145]
4/100: train_loss=[0.8389243  0.85556227], train_acc=[0.6857  0.68365]
6/100: train_loss=[0.78453374 0.79606867], train_acc=[0.707  0.7077]
8/100: train_loss=[0.724915  0.7664379], train_acc=[0.7295 0.7193]
10/100: train_loss=[0.72556365 0.7327751 ], train_acc=[0.72355 0.7293 ]
12/100: train_loss=[0.680779  0.6945207], train_acc=[0.74855 0.74425]
14/100: train_loss=[0.668616   0.68458885], train_acc=[0.74995 0.7483 ]
16/100: train_loss=[0.65350455 0.67486775], train_acc=[0.75795 0.7496 ]
18/100: train_loss=[0.62284756 0.652243  ], train_acc=[0.7671 0.7598]
20/100: train_loss=[0.6162142 0.6438515], train_acc=[0.7733  0.76365]
22/100: train_loss=[0.6124379  0.63671803], train_acc=[0.7757  0.76325]
24/100: train_loss=[0.59664446 0.6295316 ], train_acc=[0.77805 0.76805]
26/100: train_loss=[0.598763   0.63746333], train_acc=[0.78115 0.76215]
28/100: train_loss=[0.7707143 0.7198173], train_acc=[0.70315 0.7342 ]
30/100: train_loss=[0.5932932 0.6145912], train_acc=[0.78235 0.77565]
32/100: train_loss=[0.5878573 0.6310141], train_acc=[0.7816 0.7623]
34/100: train_loss=[0.6215386 0.6352676], train_acc=[0.76535 0.76745]
36/100: train_loss=[0.567101   0.60614264], train_acc=[0.7911  0.77565]
38/100: train_loss=[0.5641674 0.5957234], train_acc=[0.79235 0.78125]
40/100: train_loss=[0.562916  0.6103089], train_acc=[0.79255 0.77605]
42/100: train_loss=[0.55529565 0.5948894 ], train_acc=[0.79545 0.779  ]
44/100: train_loss=[0.5616643 0.594811 ], train_acc=[0.7931 0.7819]
46/100: train_loss=[0.56113976 0.59838545], train_acc=[0.7935  0.78195]
48/100: train_loss=[0.55830765 0.5885644 ], train_acc=[0.79365 0.7876 ]
50/100: train_loss=[0.54398215 0.5771212 ], train_acc=[0.7989 0.7861]
52/100: train_loss=[0.5527015 0.578239 ], train_acc=[0.7971  0.78555]
54/100: train_loss=[0.5403568 0.5839921], train_acc=[0.8031  0.78065]
56/100: train_loss=[0.5494702 0.5813229], train_acc=[0.7991 0.7855]
58/100: train_loss=[0.5427688  0.57977045], train_acc=[0.8021  0.78655]
60/100: train_loss=[0.5358469 0.5707903], train_acc=[0.8024  0.79455]
62/100: train_loss=[0.5361442 0.5866083], train_acc=[0.80315 0.78375]
64/100: train_loss=[0.5415317 0.5671292], train_acc=[0.801 0.791]
66/100: train_loss=[0.5394839  0.57694745], train_acc=[0.8015 0.7883]
68/100: train_loss=[0.532742   0.56226087], train_acc=[0.80405 0.7955 ]
70/100: train_loss=[0.5315464  0.58059895], train_acc=[0.804   0.79095]
72/100: train_loss=[0.52727675 0.5645118 ], train_acc=[0.8061  0.79645]
74/100: train_loss=[0.5384851 0.5622965], train_acc=[0.80315 0.79605]
76/100: train_loss=[0.52525514 0.5613077 ], train_acc=[0.80585 0.79235]
78/100: train_loss=[0.5227968 0.5528172], train_acc=[0.8091  0.79815]
80/100: train_loss=[0.53696626 0.5655485 ], train_acc=[0.80205 0.7931 ]
82/100: train_loss=[0.52186036 0.58149356], train_acc=[0.80925 0.7922 ]
84/100: train_loss=[0.5248875 0.5635016], train_acc=[0.8072 0.7949]
86/100: train_loss=[0.52531433 0.5486913 ], train_acc=[0.8082 0.7983]
88/100: train_loss=[0.530963   0.55746126], train_acc=[0.8053  0.79705]
90/100: train_loss=[0.5253218 0.5594088], train_acc=[0.8064 0.7986]
92/100: train_loss=[0.52855897 0.5528096 ], train_acc=[0.8035  0.79955]
94/100: train_loss=[0.5264362 0.5526936], train_acc=[0.80635 0.7956 ]
96/100: train_loss=[0.5292034 0.5548219], train_acc=[0.8051 0.7968]
98/100: train_loss=[0.5163278 0.5466816], train_acc=[0.8106  0.80425]
100/100: train_loss=[0.518194  0.5552788], train_acc=[0.81165 0.7983 ]
**** Time taken for fashion_1 = 2365.857157230377
loading dataset fashion
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.1843326 1.2277656], train_acc=[0.5493 0.5523]
2/100: train_loss=[1.0078402 1.073057 ], train_acc=[0.61745 0.60655]
4/100: train_loss=[0.8975993 0.8989986], train_acc=[0.66095 0.671  ]
6/100: train_loss=[0.8649215  0.94260687], train_acc=[0.66865 0.6579 ]
8/100: train_loss=[0.7957784 0.7820813], train_acc=[0.6953  0.71065]
10/100: train_loss=[0.75176495 0.75753886], train_acc=[0.7189 0.72  ]
12/100: train_loss=[0.7264134 0.7071561], train_acc=[0.72445 0.73965]
14/100: train_loss=[0.73885155 0.7566466 ], train_acc=[0.7212  0.71545]
16/100: train_loss=[0.69512916 0.6804837 ], train_acc=[0.74125 0.7482 ]
18/100: train_loss=[0.6903054  0.67550814], train_acc=[0.74235 0.7512 ]
20/100: train_loss=[0.6694452 0.6616935], train_acc=[0.7509  0.75325]
22/100: train_loss=[0.6654174  0.65617746], train_acc=[0.7499 0.7553]
24/100: train_loss=[0.65300107 0.64060193], train_acc=[0.75565 0.76385]
26/100: train_loss=[0.6873805 0.6953896], train_acc=[0.7395  0.74515]
28/100: train_loss=[0.6267358 0.6354745], train_acc=[0.7644  0.76215]
30/100: train_loss=[0.621442  0.6275979], train_acc=[0.7683 0.7674]
32/100: train_loss=[0.61534643 0.6247435 ], train_acc=[0.76975 0.7649 ]
34/100: train_loss=[0.61656934 0.61461157], train_acc=[0.76895 0.76905]
36/100: train_loss=[0.61316437 0.61460745], train_acc=[0.771   0.77385]
38/100: train_loss=[0.60551476 0.64940774], train_acc=[0.7752  0.75555]
40/100: train_loss=[0.6086171 0.6097101], train_acc=[0.7695 0.7728]
42/100: train_loss=[0.6188157  0.62015486], train_acc=[0.76945 0.76515]
44/100: train_loss=[0.5969687 0.5983278], train_acc=[0.7772 0.7773]
46/100: train_loss=[0.59901994 0.60767156], train_acc=[0.77755 0.7754 ]
48/100: train_loss=[0.5900028 0.5935459], train_acc=[0.77715 0.77985]
50/100: train_loss=[0.58886105 0.5910143 ], train_acc=[0.77855 0.78445]
52/100: train_loss=[0.5967834 0.5948136], train_acc=[0.7778  0.78015]
54/100: train_loss=[0.59361744 0.5999316 ], train_acc=[0.778   0.78335]
56/100: train_loss=[0.59965783 0.5945949 ], train_acc=[0.77595 0.78025]
58/100: train_loss=[0.5921629  0.58279353], train_acc=[0.775   0.78795]
60/100: train_loss=[0.58537334 0.57886297], train_acc=[0.78   0.7882]
62/100: train_loss=[0.58006996 0.5775578 ], train_acc=[0.78465 0.7886 ]
64/100: train_loss=[0.5690088 0.5896688], train_acc=[0.7876  0.78075]
66/100: train_loss=[0.57622266 0.57009375], train_acc=[0.78465 0.78895]
68/100: train_loss=[0.5784853 0.5775786], train_acc=[0.78485 0.78605]
70/100: train_loss=[0.5673995  0.56492585], train_acc=[0.7899  0.79355]
72/100: train_loss=[0.5784005 0.5615428], train_acc=[0.7877  0.79765]
74/100: train_loss=[0.57642174 0.5689797 ], train_acc=[0.78685 0.7934 ]
76/100: train_loss=[0.5671748 0.5746449], train_acc=[0.78925 0.7874 ]
78/100: train_loss=[0.5690299  0.56962323], train_acc=[0.78665 0.79195]
80/100: train_loss=[0.6138671 0.6391089], train_acc=[0.77495 0.75885]
82/100: train_loss=[0.56119907 0.55997306], train_acc=[0.7918  0.79795]
84/100: train_loss=[0.5565522  0.55315286], train_acc=[0.7948  0.79835]
86/100: train_loss=[0.5539626 0.5590464], train_acc=[0.79535 0.79255]
88/100: train_loss=[0.55633086 0.5533519 ], train_acc=[0.79375 0.799  ]
90/100: train_loss=[0.56200624 0.5600732 ], train_acc=[0.7926  0.79435]
92/100: train_loss=[0.57650644 0.5564717 ], train_acc=[0.78865 0.7963 ]
94/100: train_loss=[0.5495341  0.54937375], train_acc=[0.7976 0.7989]
96/100: train_loss=[0.55074203 0.5604606 ], train_acc=[0.7982 0.7945]
98/100: train_loss=[0.5550283  0.56162477], train_acc=[0.7941 0.7946]
100/100: train_loss=[0.54569924 0.55088073], train_acc=[0.79835 0.79915]
**** Time taken for fashion_2 = 2368.5363008975983
loading dataset fashion
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.0638866 1.1440307], train_acc=[0.6091  0.57555]
2/100: train_loss=[0.96429425 1.005464  ], train_acc=[0.63955 0.61935]
4/100: train_loss=[0.86175925 0.8664457 ], train_acc=[0.6815  0.68435]
6/100: train_loss=[0.8008968 0.7864527], train_acc=[0.7057  0.71075]
8/100: train_loss=[0.74881756 0.75095665], train_acc=[0.72375 0.72435]
10/100: train_loss=[0.7138689  0.72069454], train_acc=[0.73735 0.7373 ]
12/100: train_loss=[0.6864916  0.68814266], train_acc=[0.74595 0.7524 ]
14/100: train_loss=[0.6596854  0.68228364], train_acc=[0.76165 0.7459 ]
16/100: train_loss=[0.66674304 0.65824187], train_acc=[0.75775 0.7627 ]
18/100: train_loss=[0.66529405 0.6410068 ], train_acc=[0.7577  0.76575]
20/100: train_loss=[0.634223   0.64300287], train_acc=[0.76825 0.7652 ]
22/100: train_loss=[0.61807156 0.62155145], train_acc=[0.77255 0.77225]
24/100: train_loss=[0.61886907 0.61521506], train_acc=[0.7738  0.77675]
26/100: train_loss=[0.6094065  0.61985403], train_acc=[0.77505 0.7767 ]
28/100: train_loss=[0.6074836  0.61957437], train_acc=[0.77785 0.76915]
30/100: train_loss=[0.5924843 0.6116467], train_acc=[0.7832  0.77765]
32/100: train_loss=[0.58450454 0.6009912 ], train_acc=[0.7843 0.7812]
34/100: train_loss=[0.5897409 0.5931022], train_acc=[0.78365 0.7822 ]
36/100: train_loss=[0.58358485 0.5802708 ], train_acc=[0.78265 0.7875 ]
38/100: train_loss=[0.57377774 0.58470356], train_acc=[0.7901  0.78885]
40/100: train_loss=[0.5695936  0.58081806], train_acc=[0.791  0.7894]
42/100: train_loss=[0.56637454 0.5784207 ], train_acc=[0.79055 0.7893 ]
44/100: train_loss=[0.5651551 0.5746833], train_acc=[0.79335 0.7882 ]
46/100: train_loss=[0.55959517 0.5857044 ], train_acc=[0.796  0.7847]
48/100: train_loss=[0.5518246 0.5682427], train_acc=[0.7991  0.79355]
50/100: train_loss=[0.5593581 0.5619513], train_acc=[0.7957 0.7929]
52/100: train_loss=[0.56536657 0.5819062 ], train_acc=[0.7927  0.79115]
54/100: train_loss=[0.5515777 0.5646552], train_acc=[0.79775 0.7906 ]
56/100: train_loss=[0.54187787 0.562462  ], train_acc=[0.80055 0.79465]
58/100: train_loss=[0.54337233 0.5555951 ], train_acc=[0.7996  0.79735]
60/100: train_loss=[0.54628766 0.55155915], train_acc=[0.80135 0.79995]
62/100: train_loss=[0.5559074  0.55882865], train_acc=[0.79435 0.7972 ]
64/100: train_loss=[0.53606063 0.5562395 ], train_acc=[0.8038 0.7941]
66/100: train_loss=[0.5521914 0.5521713], train_acc=[0.7969 0.7974]
68/100: train_loss=[0.53150564 0.5592105 ], train_acc=[0.8058  0.79575]
70/100: train_loss=[0.54157495 0.5550918 ], train_acc=[0.8004  0.79625]
72/100: train_loss=[0.53249836 0.5471451 ], train_acc=[0.8039  0.79855]
74/100: train_loss=[0.5321909  0.55343664], train_acc=[0.8043  0.79755]
76/100: train_loss=[0.5287495 0.5383585], train_acc=[0.8087  0.80265]
78/100: train_loss=[0.5265675 0.5366161], train_acc=[0.80975 0.8037 ]
80/100: train_loss=[0.5264371 0.5378499], train_acc=[0.8066  0.80315]
82/100: train_loss=[0.5268866 0.5344506], train_acc=[0.80645 0.8032 ]
84/100: train_loss=[0.5303769 0.5386962], train_acc=[0.8074  0.80425]
86/100: train_loss=[0.52392584 0.54985434], train_acc=[0.80685 0.7985 ]
88/100: train_loss=[0.5270749  0.54447705], train_acc=[0.8079 0.798 ]
90/100: train_loss=[0.5218587 0.5511275], train_acc=[0.806  0.7959]
92/100: train_loss=[0.5334765 0.5341188], train_acc=[0.8044  0.80605]
94/100: train_loss=[0.53856957 0.55586845], train_acc=[0.80195 0.7982 ]
96/100: train_loss=[0.5277981 0.5408956], train_acc=[0.80825 0.80115]
98/100: train_loss=[0.5369176  0.53676575], train_acc=[0.8035  0.80385]
100/100: train_loss=[0.52129596 0.5375104 ], train_acc=[0.80925 0.80335]
**** Time taken for fashion_3 = 2367.74373459816
loading dataset fashion
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.1292    1.2240534], train_acc=[0.57   0.5408]
2/100: train_loss=[0.94905835 1.0063502 ], train_acc=[0.64085 0.62735]
4/100: train_loss=[0.83375597 0.89253974], train_acc=[0.68595 0.66185]
6/100: train_loss=[0.77797586 0.810294  ], train_acc=[0.71045 0.7009 ]
8/100: train_loss=[0.75861686 0.8715692 ], train_acc=[0.71585 0.66335]
10/100: train_loss=[0.72957814 0.76994216], train_acc=[0.72675 0.71315]
12/100: train_loss=[0.71945184 0.76120377], train_acc=[0.7337  0.72685]
14/100: train_loss=[0.6638688  0.72962886], train_acc=[0.7545  0.72815]
16/100: train_loss=[0.6477393  0.68085265], train_acc=[0.75935 0.74865]
18/100: train_loss=[0.6714557 0.6695694], train_acc=[0.74365 0.7502 ]
20/100: train_loss=[0.63549334 0.6566744 ], train_acc=[0.76395 0.75435]
22/100: train_loss=[0.63590497 0.70174974], train_acc=[0.75755 0.74045]
24/100: train_loss=[0.6522725 0.6402394], train_acc=[0.7535 0.7624]
26/100: train_loss=[0.60564744 0.6323011 ], train_acc=[0.7744  0.76675]
28/100: train_loss=[0.605181   0.63431734], train_acc=[0.7724  0.76745]
30/100: train_loss=[0.59849584 0.6390271 ], train_acc=[0.77545 0.76395]
32/100: train_loss=[0.6084881  0.62925047], train_acc=[0.77485 0.7644 ]
34/100: train_loss=[0.6162084  0.61303085], train_acc=[0.76765 0.77085]
36/100: train_loss=[0.59231865 0.62095165], train_acc=[0.78195 0.76685]
38/100: train_loss=[0.59047544 0.62062323], train_acc=[0.78025 0.7669 ]
40/100: train_loss=[0.61187154 0.6093133 ], train_acc=[0.7725  0.77525]
42/100: train_loss=[0.5734588  0.61445916], train_acc=[0.7868 0.7718]
44/100: train_loss=[0.57320964 0.6059129 ], train_acc=[0.79015 0.77565]
46/100: train_loss=[0.5646251 0.5882244], train_acc=[0.79025 0.78335]
48/100: train_loss=[0.5791381 0.6060477], train_acc=[0.7872 0.7748]
50/100: train_loss=[0.5754408 0.5941202], train_acc=[0.7878 0.7804]
52/100: train_loss=[0.5742408  0.58719003], train_acc=[0.78775 0.78585]
54/100: train_loss=[0.5625102  0.58749956], train_acc=[0.79245 0.78075]
56/100: train_loss=[0.5693847 0.5881057], train_acc=[0.78635 0.7792 ]
58/100: train_loss=[0.55626684 0.5807003 ], train_acc=[0.795  0.7849]
60/100: train_loss=[0.563518   0.57956463], train_acc=[0.7943 0.7878]
62/100: train_loss=[0.5589423  0.58723074], train_acc=[0.79305 0.7862 ]
64/100: train_loss=[0.565938   0.58264583], train_acc=[0.78885 0.7813 ]
66/100: train_loss=[0.5481747 0.5755709], train_acc=[0.79795 0.7882 ]
68/100: train_loss=[0.56628084 0.58560914], train_acc=[0.78555 0.7808 ]
70/100: train_loss=[0.5653712 0.5881532], train_acc=[0.79535 0.78405]
72/100: train_loss=[0.5597207 0.5817674], train_acc=[0.79455 0.784  ]
74/100: train_loss=[0.5610664 0.5849602], train_acc=[0.79135 0.78475]
76/100: train_loss=[0.5457977 0.5854245], train_acc=[0.7974  0.78245]
78/100: train_loss=[0.54604256 0.56474054], train_acc=[0.80015 0.7878 ]
80/100: train_loss=[0.55011934 0.57434887], train_acc=[0.7935 0.7825]
82/100: train_loss=[0.54963326 0.56595856], train_acc=[0.7978 0.7928]
84/100: train_loss=[0.5363283  0.56735885], train_acc=[0.8035  0.79165]
86/100: train_loss=[0.53813    0.58271396], train_acc=[0.80245 0.78315]
88/100: train_loss=[0.55072325 0.575414  ], train_acc=[0.7998  0.78855]
90/100: train_loss=[0.5382382 0.571102 ], train_acc=[0.8011  0.78765]
92/100: train_loss=[0.5468662 0.5650055], train_acc=[0.79755 0.78925]
94/100: train_loss=[0.54366916 0.5661455 ], train_acc=[0.80035 0.7915 ]
96/100: train_loss=[0.5410991 0.5657209], train_acc=[0.80055 0.78865]
98/100: train_loss=[0.54609835 0.5816381 ], train_acc=[0.79695 0.78365]
100/100: train_loss=[0.53454113 0.57014763], train_acc=[0.801   0.78795]
**** Time taken for fashion_4 = 2367.6393010616302
**** Time taken for fashion = 11836.28436422348
loading dataset fashion_and_mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.0770065  0.97683585], train_acc=[0.6411  0.63675]
2/100: train_loss=[0.8186703 0.8394951], train_acc=[0.7284  0.69505]
4/100: train_loss=[0.6220063 0.7457792], train_acc=[0.7946  0.72355]
6/100: train_loss=[0.63599   0.7556156], train_acc=[0.7913 0.7191]
8/100: train_loss=[0.44176435 0.6810361 ], train_acc=[0.857  0.7461]
10/100: train_loss=[0.41523057 0.64173716], train_acc=[0.86575 0.75815]
12/100: train_loss=[0.39054993 0.63103867], train_acc=[0.8745 0.7656]
14/100: train_loss=[0.38732347 0.6164914 ], train_acc=[0.8771 0.7748]
16/100: train_loss=[0.34802765 0.58813417], train_acc=[0.89045 0.7835 ]
18/100: train_loss=[0.33622795 0.5787071 ], train_acc=[0.89185 0.7869 ]
20/100: train_loss=[0.32911158 0.58126044], train_acc=[0.89615 0.7894 ]
22/100: train_loss=[0.3005399 0.5637999], train_acc=[0.90665 0.7945 ]
24/100: train_loss=[0.29180837 0.5604413 ], train_acc=[0.9075 0.7941]
26/100: train_loss=[0.30207598 0.5506741 ], train_acc=[0.90285 0.79915]
28/100: train_loss=[0.28113565 0.56427735], train_acc=[0.911   0.79285]
30/100: train_loss=[0.2722209  0.54482645], train_acc=[0.9125 0.8021]
32/100: train_loss=[0.26851875 0.5259562 ], train_acc=[0.91365 0.8072 ]
34/100: train_loss=[0.28853047 0.5328989 ], train_acc=[0.90865 0.80625]
36/100: train_loss=[0.2572029 0.537332 ], train_acc=[0.916  0.8015]
38/100: train_loss=[0.2526086 0.5139443], train_acc=[0.9194  0.81255]
40/100: train_loss=[0.28148776 0.5877534 ], train_acc=[0.90835 0.7832 ]
42/100: train_loss=[0.24115421 0.51158404], train_acc=[0.92185 0.8132 ]
44/100: train_loss=[0.23347706 0.5052734 ], train_acc=[0.92555 0.81805]
46/100: train_loss=[0.2477222 0.5332633], train_acc=[0.91965 0.8056 ]
48/100: train_loss=[0.23605627 0.5087183 ], train_acc=[0.92425 0.8149 ]
50/100: train_loss=[0.23063114 0.5085096 ], train_acc=[0.9266  0.81045]
52/100: train_loss=[0.22407745 0.5024077 ], train_acc=[0.92845 0.81185]
54/100: train_loss=[0.22757238 0.500239  ], train_acc=[0.9261 0.8151]
56/100: train_loss=[0.2216898 0.5077331], train_acc=[0.9297 0.8091]
58/100: train_loss=[0.22178446 0.479481  ], train_acc=[0.9289 0.8245]
60/100: train_loss=[0.22222741 0.4817644 ], train_acc=[0.92915 0.82265]
62/100: train_loss=[0.211701   0.49533805], train_acc=[0.93125 0.81505]
64/100: train_loss=[0.21025237 0.48417926], train_acc=[0.933   0.82275]
66/100: train_loss=[0.22176248 0.4831183 ], train_acc=[0.92915 0.82255]
68/100: train_loss=[0.22313292 0.51278293], train_acc=[0.92925 0.81045]
70/100: train_loss=[0.21204391 0.48032835], train_acc=[0.9336 0.8242]
72/100: train_loss=[0.2610429 0.5082511], train_acc=[0.9175 0.8102]
74/100: train_loss=[0.2142751 0.4693199], train_acc=[0.93105 0.82745]
76/100: train_loss=[0.22630538 0.47840255], train_acc=[0.92635 0.82205]
78/100: train_loss=[0.21850978 0.4704132 ], train_acc=[0.93135 0.8271 ]
80/100: train_loss=[0.21370684 0.48014322], train_acc=[0.9317 0.8252]
82/100: train_loss=[0.20300947 0.47488597], train_acc=[0.93555 0.8243 ]
84/100: train_loss=[0.20968802 0.48914173], train_acc=[0.9339  0.81985]
86/100: train_loss=[0.20352529 0.48044193], train_acc=[0.93585 0.8246 ]
88/100: train_loss=[0.20622498 0.46874896], train_acc=[0.935   0.82945]
90/100: train_loss=[0.20778434 0.46481618], train_acc=[0.9334 0.83  ]
92/100: train_loss=[0.23761044 0.46740684], train_acc=[0.92365 0.8278 ]
94/100: train_loss=[0.20932312 0.45769483], train_acc=[0.9322 0.8315]
96/100: train_loss=[0.20678774 0.4635767 ], train_acc=[0.9345 0.8306]
98/100: train_loss=[0.22188567 0.461465  ], train_acc=[0.9291 0.8321]
100/100: train_loss=[0.2001922 0.4767252], train_acc=[0.93775 0.8262 ]
**** Time taken for fashion_and_mnist_0 = 2367.576138496399
loading dataset fashion_and_mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.4822433 1.1009587], train_acc=[0.49475 0.58695]
2/100: train_loss=[1.1246194 0.9005007], train_acc=[0.62195 0.6661 ]
4/100: train_loss=[0.7882512  0.78879875], train_acc=[0.73425 0.7149 ]
6/100: train_loss=[0.69745743 0.7290743 ], train_acc=[0.76595 0.7308 ]
8/100: train_loss=[0.7741661 0.7556726], train_acc=[0.74015 0.72185]
10/100: train_loss=[0.51188195 0.67963713], train_acc=[0.8319 0.7495]
12/100: train_loss=[0.47034803 0.63463306], train_acc=[0.84545 0.76625]
14/100: train_loss=[0.4481116 0.6377932], train_acc=[0.85355 0.7668 ]
16/100: train_loss=[0.41829967 0.60121447], train_acc=[0.86505 0.77975]
18/100: train_loss=[0.42590573 0.5954924 ], train_acc=[0.8609  0.78155]
20/100: train_loss=[0.43807134 0.6073869 ], train_acc=[0.85505 0.77635]
22/100: train_loss=[0.3669403  0.58273464], train_acc=[0.88255 0.78475]
24/100: train_loss=[0.37331668 0.5809949 ], train_acc=[0.8777  0.78625]
26/100: train_loss=[0.52065915 0.57945   ], train_acc=[0.83455 0.7847 ]
28/100: train_loss=[0.33529264 0.55765325], train_acc=[0.893  0.7964]
30/100: train_loss=[0.34135425 0.5567105 ], train_acc=[0.88845 0.7954 ]
32/100: train_loss=[0.3534503 0.5777989], train_acc=[0.8851  0.78565]
34/100: train_loss=[0.32931587 0.56067157], train_acc=[0.8938 0.7927]
36/100: train_loss=[0.35412627 0.55553687], train_acc=[0.8846 0.7964]
38/100: train_loss=[0.3361141  0.53893554], train_acc=[0.89125 0.8041 ]
40/100: train_loss=[0.3302468 0.5298424], train_acc=[0.89065 0.80605]
42/100: train_loss=[0.34209594 0.5344708 ], train_acc=[0.88775 0.8053 ]
44/100: train_loss=[0.31161693 0.5300178 ], train_acc=[0.89835 0.8061 ]
46/100: train_loss=[0.3189903  0.54710615], train_acc=[0.8946 0.8   ]
48/100: train_loss=[0.2891019  0.51510304], train_acc=[0.90645 0.8142 ]
50/100: train_loss=[0.330821   0.51630974], train_acc=[0.89285 0.8127 ]
52/100: train_loss=[0.2997937  0.50904936], train_acc=[0.9023  0.81555]
54/100: train_loss=[0.2884424 0.5223682], train_acc=[0.90645 0.80715]
56/100: train_loss=[0.27520138 0.50468594], train_acc=[0.91165 0.8148 ]
58/100: train_loss=[0.28161538 0.5253926 ], train_acc=[0.9081  0.81005]
60/100: train_loss=[0.27540356 0.5374796 ], train_acc=[0.91205 0.80355]
62/100: train_loss=[0.270536   0.51136184], train_acc=[0.91175 0.81285]
64/100: train_loss=[0.27594757 0.5038228 ], train_acc=[0.9099  0.81695]
66/100: train_loss=[0.29850924 0.5097831 ], train_acc=[0.9033 0.813 ]
68/100: train_loss=[0.26095006 0.5012248 ], train_acc=[0.91705 0.8177 ]
70/100: train_loss=[0.28469414 0.49644178], train_acc=[0.9059  0.81765]
72/100: train_loss=[0.26825017 0.50220156], train_acc=[0.9126 0.8176]
74/100: train_loss=[0.25971627 0.5030844 ], train_acc=[0.91675 0.81695]
76/100: train_loss=[0.26966384 0.5413301 ], train_acc=[0.9128 0.7997]
78/100: train_loss=[0.25736827 0.5045024 ], train_acc=[0.9184 0.8153]
80/100: train_loss=[0.40351993 0.5770483 ], train_acc=[0.87245 0.77875]
82/100: train_loss=[0.25131023 0.50198346], train_acc=[0.91945 0.8147 ]
84/100: train_loss=[0.26810703 0.49028668], train_acc=[0.91255 0.81975]
86/100: train_loss=[0.24922007 0.49062726], train_acc=[0.9212 0.8215]
88/100: train_loss=[0.24399278 0.48496905], train_acc=[0.92175 0.82415]
90/100: train_loss=[0.26250923 0.4923894 ], train_acc=[0.9157 0.8209]
92/100: train_loss=[0.24039263 0.48648068], train_acc=[0.9237 0.8228]
94/100: train_loss=[0.2494419 0.4856681], train_acc=[0.92005 0.82095]
96/100: train_loss=[0.25795057 0.49515826], train_acc=[0.91655 0.81905]
98/100: train_loss=[0.27188602 0.49625224], train_acc=[0.91315 0.81945]
100/100: train_loss=[0.27298945 0.49591562], train_acc=[0.9121 0.8159]
**** Time taken for fashion_and_mnist_1 = 2231.007812976837
loading dataset fashion_and_mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.1296122 0.976342 ], train_acc=[0.6178  0.64125]
2/100: train_loss=[0.87092555 0.84000194], train_acc=[0.71085 0.68505]
4/100: train_loss=[0.657713  0.7209293], train_acc=[0.78465 0.73055]
6/100: train_loss=[0.6083027 0.6733256], train_acc=[0.79915 0.74975]
8/100: train_loss=[0.5164122  0.65671945], train_acc=[0.8324 0.7531]
10/100: train_loss=[0.47165033 0.64925057], train_acc=[0.84835 0.76135]
12/100: train_loss=[0.45382783 0.6144487 ], train_acc=[0.85165 0.774  ]
14/100: train_loss=[0.4089676  0.60957193], train_acc=[0.8688 0.7775]
16/100: train_loss=[0.3899086  0.58529174], train_acc=[0.87325 0.78445]
18/100: train_loss=[0.40998778 0.58213335], train_acc=[0.8652  0.78695]
20/100: train_loss=[0.3658479  0.57195014], train_acc=[0.8824  0.78665]
22/100: train_loss=[0.35563633 0.5652213 ], train_acc=[0.884  0.7907]
24/100: train_loss=[0.3501594  0.55834323], train_acc=[0.88825 0.79595]
26/100: train_loss=[0.35146812 0.5534876 ], train_acc=[0.8869 0.7971]
28/100: train_loss=[0.33114624 0.548854  ], train_acc=[0.8942 0.7963]
30/100: train_loss=[0.32193252 0.54134506], train_acc=[0.8974  0.80195]
32/100: train_loss=[0.32747188 0.5307727 ], train_acc=[0.89395 0.80685]
34/100: train_loss=[0.34282288 0.53529716], train_acc=[0.89095 0.80195]
36/100: train_loss=[0.35458547 0.5292347 ], train_acc=[0.88425 0.80625]
38/100: train_loss=[0.31894037 0.5357893 ], train_acc=[0.8959  0.80485]
40/100: train_loss=[0.30872056 0.5142477 ], train_acc=[0.9005 0.8105]
42/100: train_loss=[0.31149268 0.5171077 ], train_acc=[0.9012  0.81135]
44/100: train_loss=[0.30755562 0.51124537], train_acc=[0.901  0.8137]
46/100: train_loss=[0.30530316 0.52320343], train_acc=[0.90195 0.80555]
48/100: train_loss=[0.2846031 0.5083879], train_acc=[0.91035 0.81475]
50/100: train_loss=[0.2850313  0.50010675], train_acc=[0.90875 0.8148 ]
52/100: train_loss=[0.28308314 0.5055012 ], train_acc=[0.911   0.81395]
54/100: train_loss=[0.27556187 0.504548  ], train_acc=[0.9151  0.81355]
56/100: train_loss=[0.2714406  0.49728572], train_acc=[0.9145  0.81735]
58/100: train_loss=[0.27561873 0.503071  ], train_acc=[0.91515 0.81515]
60/100: train_loss=[0.26774696 0.50895035], train_acc=[0.91575 0.8129 ]
62/100: train_loss=[0.28735602 0.50269896], train_acc=[0.90805 0.8137 ]
64/100: train_loss=[0.2878928 0.4923372], train_acc=[0.9097  0.81825]
66/100: train_loss=[0.27172825 0.4919483 ], train_acc=[0.91465 0.8208 ]
68/100: train_loss=[0.26717818 0.48563662], train_acc=[0.9162  0.82205]
70/100: train_loss=[0.26632088 0.48594686], train_acc=[0.91745 0.8188 ]
72/100: train_loss=[0.260802  0.5008098], train_acc=[0.91845 0.8139 ]
74/100: train_loss=[0.27541035 0.51188904], train_acc=[0.91365 0.80965]
76/100: train_loss=[0.27987912 0.49770144], train_acc=[0.9111  0.81885]
78/100: train_loss=[0.28880838 0.50441843], train_acc=[0.9088  0.81735]
80/100: train_loss=[0.30329958 0.51471037], train_acc=[0.90445 0.80815]
82/100: train_loss=[0.2730765 0.4810678], train_acc=[0.91435 0.824  ]
84/100: train_loss=[0.2897365  0.49934158], train_acc=[0.9064 0.8186]
86/100: train_loss=[0.25542524 0.48402768], train_acc=[0.9214 0.8213]
88/100: train_loss=[0.27228752 0.4771779 ], train_acc=[0.9143  0.82475]
90/100: train_loss=[0.25283113 0.47829172], train_acc=[0.92105 0.8238 ]
92/100: train_loss=[0.2555478  0.47919968], train_acc=[0.92085 0.8253 ]
94/100: train_loss=[0.2572521  0.47652924], train_acc=[0.9201  0.82495]
96/100: train_loss=[0.26117066 0.47853318], train_acc=[0.919  0.8262]
98/100: train_loss=[0.25146365 0.4746328 ], train_acc=[0.92155 0.82555]
100/100: train_loss=[0.30160317 0.5183293 ], train_acc=[0.9049 0.8116]
**** Time taken for fashion_and_mnist_2 = 2092.259994029999
loading dataset fashion_and_mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.2089243 1.0003537], train_acc=[0.59565 0.64685]
2/100: train_loss=[0.891671   0.85067356], train_acc=[0.70265 0.696  ]
4/100: train_loss=[0.71456623 0.7318026 ], train_acc=[0.7642  0.72885]
6/100: train_loss=[0.5507858  0.66960645], train_acc=[0.81915 0.75615]
8/100: train_loss=[0.53325653 0.63565695], train_acc=[0.8244  0.76615]
10/100: train_loss=[0.49134934 0.61555344], train_acc=[0.8412 0.7721]
12/100: train_loss=[0.42218718 0.6253777 ], train_acc=[0.8635  0.76665]
14/100: train_loss=[0.4133595  0.60543233], train_acc=[0.86405 0.77645]
16/100: train_loss=[0.37417868 0.5698793 ], train_acc=[0.8819 0.7871]
18/100: train_loss=[0.36430252 0.56442964], train_acc=[0.882 0.79 ]
20/100: train_loss=[0.3517012 0.5510772], train_acc=[0.88815 0.7951 ]
22/100: train_loss=[0.3321696 0.6081823], train_acc=[0.89465 0.7735 ]
24/100: train_loss=[0.3220045 0.5554106], train_acc=[0.89635 0.7966 ]
26/100: train_loss=[0.3395233  0.57839453], train_acc=[0.89085 0.78885]
28/100: train_loss=[0.31556445 0.52853435], train_acc=[0.9015 0.8039]
30/100: train_loss=[0.33611867 0.56721413], train_acc=[0.8916 0.7837]
32/100: train_loss=[0.29212683 0.5450698 ], train_acc=[0.9049 0.7946]
34/100: train_loss=[0.28596133 0.53414536], train_acc=[0.90785 0.79945]
36/100: train_loss=[0.2800996 0.5259465], train_acc=[0.90835 0.80305]
38/100: train_loss=[0.2857752  0.54134095], train_acc=[0.90755 0.8025 ]
40/100: train_loss=[0.27551585 0.5130203 ], train_acc=[0.91305 0.81305]
42/100: train_loss=[0.2761969  0.55853623], train_acc=[0.91265 0.7869 ]
44/100: train_loss=[0.27174366 0.53439546], train_acc=[0.9127 0.8053]
46/100: train_loss=[0.26505238 0.5212399 ], train_acc=[0.91575 0.8114 ]
48/100: train_loss=[0.26130673 0.50487816], train_acc=[0.9148  0.81685]
50/100: train_loss=[0.2589206 0.5017203], train_acc=[0.9186 0.8175]
52/100: train_loss=[0.25562358 0.50416875], train_acc=[0.9185 0.8167]
54/100: train_loss=[0.25159976 0.5044143 ], train_acc=[0.92045 0.81555]
56/100: train_loss=[0.25831857 0.49796295], train_acc=[0.91785 0.82065]
58/100: train_loss=[0.24169159 0.5049569 ], train_acc=[0.92325 0.81985]
60/100: train_loss=[0.2447927 0.4919281], train_acc=[0.9212  0.82085]
62/100: train_loss=[0.24941151 0.5276947 ], train_acc=[0.9198  0.80785]
64/100: train_loss=[0.24139608 0.4902482 ], train_acc=[0.92435 0.8241 ]
66/100: train_loss=[0.23696123 0.48808697], train_acc=[0.92445 0.82515]
68/100: train_loss=[0.23655088 0.50939053], train_acc=[0.92615 0.81725]
70/100: train_loss=[0.25673088 0.51831925], train_acc=[0.9171 0.8157]
72/100: train_loss=[0.24931838 0.49443775], train_acc=[0.9218  0.82065]
74/100: train_loss=[0.23981771 0.49745822], train_acc=[0.92535 0.82375]
76/100: train_loss=[0.24552402 0.48465335], train_acc=[0.9222 0.8254]
78/100: train_loss=[0.2331934  0.48706067], train_acc=[0.9256 0.8236]
80/100: train_loss=[0.22954845 0.4873867 ], train_acc=[0.9272 0.8251]
82/100: train_loss=[0.25308844 0.5052996 ], train_acc=[0.91965 0.8144 ]
84/100: train_loss=[0.2259929  0.48088485], train_acc=[0.9295 0.8243]
86/100: train_loss=[0.23252845 0.47742406], train_acc=[0.9247 0.8268]
88/100: train_loss=[0.23778076 0.4733351 ], train_acc=[0.9253 0.8292]
90/100: train_loss=[0.22747935 0.48764113], train_acc=[0.9292 0.8294]
92/100: train_loss=[0.2387056 0.4941405], train_acc=[0.92435 0.82065]
94/100: train_loss=[0.22551376 0.47829694], train_acc=[0.92775 0.8267 ]
96/100: train_loss=[0.23907547 0.48753464], train_acc=[0.9246 0.8262]
98/100: train_loss=[0.23545654 0.47402588], train_acc=[0.9268 0.8312]
100/100: train_loss=[0.23576732 0.47747472], train_acc=[0.9263  0.82825]
**** Time taken for fashion_and_mnist_3 = 2091.41015124321
loading dataset fashion_and_mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.3921291 1.1677758], train_acc=[0.52135 0.56745]
2/100: train_loss=[0.98211664 0.86056936], train_acc=[0.6648 0.6868]
4/100: train_loss=[0.6454989 0.7535053], train_acc=[0.78905 0.71575]
6/100: train_loss=[0.57756275 0.70207876], train_acc=[0.8094  0.74015]
8/100: train_loss=[0.70337087 0.73057526], train_acc=[0.7684 0.7283]
10/100: train_loss=[0.46910185 0.6598953 ], train_acc=[0.8448 0.7541]
12/100: train_loss=[0.4144139 0.6206247], train_acc=[0.86585 0.76735]
14/100: train_loss=[0.3830348 0.611261 ], train_acc=[0.87755 0.77245]
16/100: train_loss=[0.36555338 0.6000805 ], train_acc=[0.88235 0.77475]
18/100: train_loss=[0.34742653 0.58338535], train_acc=[0.88875 0.7824 ]
20/100: train_loss=[0.35388812 0.57522374], train_acc=[0.88425 0.78755]
22/100: train_loss=[0.3284622 0.5673891], train_acc=[0.89315 0.7931 ]
24/100: train_loss=[0.319251  0.5746493], train_acc=[0.89925 0.78815]
26/100: train_loss=[0.3168197  0.56303245], train_acc=[0.898   0.79315]
28/100: train_loss=[0.3307025  0.57503057], train_acc=[0.894  0.7861]
30/100: train_loss=[0.34140486 0.5524358 ], train_acc=[0.8902 0.7976]
32/100: train_loss=[0.29091096 0.54062706], train_acc=[0.9096  0.80095]
34/100: train_loss=[0.31004205 0.5462021 ], train_acc=[0.90325 0.7964 ]
36/100: train_loss=[0.30645248 0.55356765], train_acc=[0.9025 0.798 ]
38/100: train_loss=[0.29861653 0.5367493 ], train_acc=[0.9037  0.80045]
40/100: train_loss=[0.28673005 0.5490262 ], train_acc=[0.91   0.8001]
42/100: train_loss=[0.28762576 0.5376298 ], train_acc=[0.90655 0.80065]
44/100: train_loss=[0.29119977 0.5380714 ], train_acc=[0.9091 0.801 ]
46/100: train_loss=[0.26705223 0.5255956 ], train_acc=[0.91765 0.80285]
48/100: train_loss=[0.2708124 0.5312785], train_acc=[0.91315 0.80475]
50/100: train_loss=[0.26982674 0.5200014 ], train_acc=[0.91445 0.80955]
52/100: train_loss=[0.25818563 0.51954365], train_acc=[0.9183 0.811 ]
54/100: train_loss=[0.25799754 0.55190927], train_acc=[0.91795 0.7992 ]
56/100: train_loss=[0.26227978 0.518929  ], train_acc=[0.91455 0.8094 ]
58/100: train_loss=[0.26274443 0.5176045 ], train_acc=[0.9156  0.80915]
60/100: train_loss=[0.25017124 0.512753  ], train_acc=[0.9209 0.8122]
62/100: train_loss=[0.25692204 0.5355575 ], train_acc=[0.9177  0.80295]
64/100: train_loss=[0.2532637  0.53038424], train_acc=[0.9191  0.80495]
66/100: train_loss=[0.26045552 0.51248604], train_acc=[0.9158 0.8123]
68/100: train_loss=[0.24936154 0.5012291 ], train_acc=[0.9205 0.8155]
70/100: train_loss=[0.24316566 0.51029235], train_acc=[0.9241 0.8105]
72/100: train_loss=[0.2414534 0.4965279], train_acc=[0.923   0.82065]
74/100: train_loss=[0.24501248 0.50989103], train_acc=[0.92285 0.81225]
76/100: train_loss=[0.24494493 0.5313404 ], train_acc=[0.92085 0.80115]
78/100: train_loss=[0.24080929 0.5027614 ], train_acc=[0.92355 0.81425]
80/100: train_loss=[0.2459447 0.5254287], train_acc=[0.9223  0.80835]
82/100: train_loss=[0.2432548  0.49672914], train_acc=[0.9238 0.8195]
84/100: train_loss=[0.2518404  0.49498254], train_acc=[0.92055 0.8154 ]
86/100: train_loss=[0.2580984 0.5016437], train_acc=[0.9183 0.8152]
88/100: train_loss=[0.23927023 0.49675307], train_acc=[0.92335 0.81675]
90/100: train_loss=[0.26959515 0.49520147], train_acc=[0.91425 0.8189 ]
92/100: train_loss=[0.23981152 0.5018412 ], train_acc=[0.92485 0.81505]
94/100: train_loss=[0.2337267 0.4876375], train_acc=[0.9256  0.82015]
96/100: train_loss=[0.23429859 0.48789164], train_acc=[0.9257  0.82255]
98/100: train_loss=[0.24053681 0.51417917], train_acc=[0.92315 0.81325]
100/100: train_loss=[0.23480298 0.50566685], train_acc=[0.9249 0.817 ]
**** Time taken for fashion_and_mnist_4 = 2093.399589538574
**** Time taken for fashion_and_mnist = 10875.707971334457
