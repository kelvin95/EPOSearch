Script started.
loading dataset mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[0.94325435 1.0912203 ], train_acc=[0.68815 0.62585]
2/100: train_loss=[0.73082525 0.8496484 ], train_acc=[0.7596  0.71215]
4/100: train_loss=[0.56949866 0.6735678 ], train_acc=[0.81605 0.77275]
6/100: train_loss=[0.51054233 0.62518215], train_acc=[0.83195 0.7887 ]
8/100: train_loss=[0.4644786 0.5630751], train_acc=[0.84765 0.81065]
10/100: train_loss=[0.42447653 0.54358095], train_acc=[0.86095 0.81825]
12/100: train_loss=[0.4225681  0.50014246], train_acc=[0.86    0.83365]
14/100: train_loss=[0.38660088 0.47053656], train_acc=[0.8742 0.8434]
16/100: train_loss=[0.38736483 0.46735528], train_acc=[0.8726  0.84415]
18/100: train_loss=[0.36903545 0.44837007], train_acc=[0.87975 0.85105]
20/100: train_loss=[0.37663102 0.42971745], train_acc=[0.8775 0.8577]
22/100: train_loss=[0.36424723 0.45647547], train_acc=[0.88295 0.84665]
24/100: train_loss=[0.34314314 0.41271615], train_acc=[0.8883 0.8631]
26/100: train_loss=[0.33019942 0.40376815], train_acc=[0.8913 0.8654]
28/100: train_loss=[0.3314999 0.4084025], train_acc=[0.8928 0.8654]
30/100: train_loss=[0.33293274 0.38626558], train_acc=[0.8915  0.87205]
32/100: train_loss=[0.31949177 0.3803545 ], train_acc=[0.89475 0.874  ]
34/100: train_loss=[0.3173835  0.38943073], train_acc=[0.8952 0.872 ]
36/100: train_loss=[0.33077237 0.506975  ], train_acc=[0.8917  0.83315]
38/100: train_loss=[0.30784705 0.3685023 ], train_acc=[0.8997 0.8785]
40/100: train_loss=[0.34596416 0.42765206], train_acc=[0.8895 0.857 ]
42/100: train_loss=[0.2996046 0.3823275], train_acc=[0.9027 0.8732]
44/100: train_loss=[0.31285655 0.37099144], train_acc=[0.89805 0.8766 ]
46/100: train_loss=[0.3020575  0.36966744], train_acc=[0.90105 0.8779 ]
48/100: train_loss=[0.29800275 0.36447638], train_acc=[0.90245 0.87965]
50/100: train_loss=[0.29462042 0.3714184 ], train_acc=[0.9049  0.87815]
52/100: train_loss=[0.31137478 0.3621049 ], train_acc=[0.89765 0.87925]
54/100: train_loss=[0.29044583 0.3643861 ], train_acc=[0.9051  0.88005]
56/100: train_loss=[0.29595593 0.35525098], train_acc=[0.9035  0.88075]
58/100: train_loss=[0.28747687 0.35975155], train_acc=[0.9068  0.88255]
60/100: train_loss=[0.2966296 0.3582328], train_acc=[0.90305 0.8835 ]
62/100: train_loss=[0.3015215  0.35449463], train_acc=[0.90525 0.88375]
64/100: train_loss=[0.2826848  0.35599437], train_acc=[0.90875 0.88305]
66/100: train_loss=[0.2973918  0.36665323], train_acc=[0.9044  0.88025]
68/100: train_loss=[0.28803587 0.35198808], train_acc=[0.9079 0.8851]
70/100: train_loss=[0.28687176 0.35685575], train_acc=[0.90775 0.88305]
72/100: train_loss=[0.28161308 0.34757027], train_acc=[0.91005 0.88625]
74/100: train_loss=[0.28809157 0.36145678], train_acc=[0.90655 0.88095]
76/100: train_loss=[0.28946155 0.3546397 ], train_acc=[0.9066  0.88265]
78/100: train_loss=[0.2931531  0.35298738], train_acc=[0.905  0.8827]
80/100: train_loss=[0.2802341 0.3462993], train_acc=[0.9101  0.88665]
82/100: train_loss=[0.27951655 0.34700185], train_acc=[0.91025 0.88625]
84/100: train_loss=[0.27664563 0.35076678], train_acc=[0.91105 0.8842 ]
86/100: train_loss=[0.28733352 0.36153015], train_acc=[0.90775 0.8834 ]
88/100: train_loss=[0.2825174  0.34212634], train_acc=[0.9099  0.88795]
90/100: train_loss=[0.27988765 0.33995685], train_acc=[0.9113 0.89  ]
92/100: train_loss=[0.28290647 0.3421834 ], train_acc=[0.90955 0.8886 ]
94/100: train_loss=[0.27573547 0.3389483 ], train_acc=[0.91115 0.88995]
96/100: train_loss=[0.27452648 0.34189925], train_acc=[0.91285 0.8889 ]
98/100: train_loss=[0.27686855 0.34781015], train_acc=[0.9128  0.88765]
100/100: train_loss=[0.28015593 0.34193948], train_acc=[0.9106  0.88855]
**** Time taken for mnist_0 = 2593.905940771103
loading dataset mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.148413  1.3500247], train_acc=[0.62195 0.54325]
2/100: train_loss=[0.86049336 0.98448944], train_acc=[0.71835 0.66905]
4/100: train_loss=[0.5958466 0.717175 ], train_acc=[0.8068 0.758 ]
6/100: train_loss=[0.518871   0.64072686], train_acc=[0.83085 0.78295]
8/100: train_loss=[0.46893543 0.5615052 ], train_acc=[0.84685 0.8115 ]
10/100: train_loss=[0.4341396 0.5413523], train_acc=[0.8578 0.8174]
12/100: train_loss=[0.40412557 0.5243434 ], train_acc=[0.8689  0.82285]
14/100: train_loss=[0.3878096 0.4848624], train_acc=[0.8738  0.83775]
16/100: train_loss=[0.3761238  0.47337782], train_acc=[0.8769  0.84765]
18/100: train_loss=[0.38684386 0.48391426], train_acc=[0.87425 0.8426 ]
20/100: train_loss=[0.35254768 0.44863257], train_acc=[0.88395 0.8514 ]
22/100: train_loss=[0.34001756 0.42433834], train_acc=[0.88765 0.8595 ]
24/100: train_loss=[0.33915165 0.41636696], train_acc=[0.8883  0.86185]
26/100: train_loss=[0.34154963 0.4086267 ], train_acc=[0.8877  0.86565]
28/100: train_loss=[0.32942328 0.4067041 ], train_acc=[0.89095 0.86525]
30/100: train_loss=[0.38511693 0.4657629 ], train_acc=[0.875  0.8451]
32/100: train_loss=[0.32108617 0.39355072], train_acc=[0.892  0.8689]
34/100: train_loss=[0.3719607  0.47750175], train_acc=[0.87615 0.8415 ]
36/100: train_loss=[0.30957758 0.41511384], train_acc=[0.89755 0.8634 ]
38/100: train_loss=[0.31861684 0.37893295], train_acc=[0.8946 0.8766]
40/100: train_loss=[0.30346134 0.37221724], train_acc=[0.89825 0.8769 ]
42/100: train_loss=[0.30901447 0.36894715], train_acc=[0.89725 0.879  ]
44/100: train_loss=[0.29878366 0.3675278 ], train_acc=[0.8999  0.87775]
46/100: train_loss=[0.29795662 0.36357027], train_acc=[0.9006  0.87955]
48/100: train_loss=[0.2910737  0.35907182], train_acc=[0.90305 0.88165]
50/100: train_loss=[0.2900143  0.35797173], train_acc=[0.9039 0.882 ]
52/100: train_loss=[0.2884264  0.35542017], train_acc=[0.90395 0.8845 ]
54/100: train_loss=[0.29011494 0.36593044], train_acc=[0.9038 0.8798]
56/100: train_loss=[0.3095296  0.36007315], train_acc=[0.8971 0.8816]
58/100: train_loss=[0.2810517  0.34948978], train_acc=[0.9068 0.8849]
60/100: train_loss=[0.28156766 0.3441375 ], train_acc=[0.9069 0.8862]
62/100: train_loss=[0.29367545 0.3490199 ], train_acc=[0.90185 0.8857 ]
64/100: train_loss=[0.28301036 0.34529653], train_acc=[0.90655 0.8866 ]
66/100: train_loss=[0.2800098  0.35631415], train_acc=[0.90705 0.88215]
68/100: train_loss=[0.2875332  0.36142233], train_acc=[0.90525 0.8819 ]
70/100: train_loss=[0.29234695 0.35188055], train_acc=[0.9023  0.88545]
72/100: train_loss=[0.2852518  0.34477258], train_acc=[0.9068  0.88775]
74/100: train_loss=[0.28906095 0.34868708], train_acc=[0.9039 0.8847]
76/100: train_loss=[0.28432462 0.33220306], train_acc=[0.90665 0.89165]
78/100: train_loss=[0.28100622 0.3406952 ], train_acc=[0.909  0.8882]
80/100: train_loss=[0.2792894 0.333293 ], train_acc=[0.9082  0.89175]
82/100: train_loss=[0.27725157 0.3599728 ], train_acc=[0.90975 0.88185]
84/100: train_loss=[0.27852887 0.34573624], train_acc=[0.9084 0.8866]
86/100: train_loss=[0.2718334  0.33127525], train_acc=[0.91025 0.8936 ]
88/100: train_loss=[0.28188077 0.34705397], train_acc=[0.90675 0.88725]
90/100: train_loss=[0.28330365 0.33088645], train_acc=[0.9108 0.8928]
92/100: train_loss=[0.28165495 0.33141753], train_acc=[0.90625 0.8938 ]
94/100: train_loss=[0.2764438  0.32650122], train_acc=[0.9084 0.8946]
96/100: train_loss=[0.27307406 0.33466446], train_acc=[0.91075 0.89045]
98/100: train_loss=[0.27610755 0.32885665], train_acc=[0.91015 0.89355]
100/100: train_loss=[0.27568394 0.32970083], train_acc=[0.909   0.89375]
**** Time taken for mnist_1 = 2581.2800023555756
loading dataset mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.0526268 1.2367812], train_acc=[0.6471 0.5931]
2/100: train_loss=[0.80693763 0.9686827 ], train_acc=[0.73625 0.6808 ]
4/100: train_loss=[0.629392  0.8102597], train_acc=[0.79125 0.7292 ]
6/100: train_loss=[0.56315047 0.64958876], train_acc=[0.8139  0.78415]
8/100: train_loss=[0.47682405 0.58467835], train_acc=[0.84455 0.8048 ]
10/100: train_loss=[0.4536072 0.5231768], train_acc=[0.85285 0.8279 ]
12/100: train_loss=[0.4155927  0.50252336], train_acc=[0.8625  0.83125]
14/100: train_loss=[0.40290603 0.4890742 ], train_acc=[0.8692  0.83845]
16/100: train_loss=[0.39779422 0.49180973], train_acc=[0.8693  0.83835]
18/100: train_loss=[0.37546209 0.44478184], train_acc=[0.87645 0.85065]
20/100: train_loss=[0.36717182 0.44537315], train_acc=[0.8798 0.8524]
22/100: train_loss=[0.36865398 0.430299  ], train_acc=[0.87885 0.85465]
24/100: train_loss=[0.3704909  0.46176642], train_acc=[0.87905 0.84875]
26/100: train_loss=[0.34969515 0.4229023 ], train_acc=[0.885   0.86035]
28/100: train_loss=[0.34059694 0.4144434 ], train_acc=[0.8877 0.8618]
30/100: train_loss=[0.32873985 0.3966498 ], train_acc=[0.8927 0.8677]
32/100: train_loss=[0.33772874 0.44147995], train_acc=[0.88805 0.8506 ]
34/100: train_loss=[0.32016823 0.38625914], train_acc=[0.89595 0.87135]
36/100: train_loss=[0.3209714  0.39146072], train_acc=[0.8935  0.86905]
38/100: train_loss=[0.3428398  0.41319025], train_acc=[0.88775 0.8595 ]
40/100: train_loss=[0.31644714 0.39513624], train_acc=[0.89585 0.8677 ]
42/100: train_loss=[0.31136736 0.37779373], train_acc=[0.89795 0.8741 ]
44/100: train_loss=[0.31106305 0.37932053], train_acc=[0.8978 0.8736]
46/100: train_loss=[0.32230583 0.37606683], train_acc=[0.8939  0.87495]
48/100: train_loss=[0.3003914  0.36771056], train_acc=[0.9007  0.87725]
50/100: train_loss=[0.3066245  0.36524242], train_acc=[0.89955 0.87755]
52/100: train_loss=[0.33114514 0.40755424], train_acc=[0.89145 0.86665]
54/100: train_loss=[0.34855103 0.45411465], train_acc=[0.88755 0.84795]
56/100: train_loss=[0.29943618 0.3597492 ], train_acc=[0.9012  0.88135]
58/100: train_loss=[0.2969813  0.36137226], train_acc=[0.90295 0.8813 ]
60/100: train_loss=[0.29859185 0.36612275], train_acc=[0.90185 0.87975]
62/100: train_loss=[0.29459405 0.3584573 ], train_acc=[0.90325 0.88145]
64/100: train_loss=[0.29041898 0.3508845 ], train_acc=[0.90375 0.88315]
66/100: train_loss=[0.2934424  0.36490294], train_acc=[0.90275 0.8801 ]
68/100: train_loss=[0.2965217  0.36646912], train_acc=[0.90315 0.8792 ]
70/100: train_loss=[0.29394573 0.3497792 ], train_acc=[0.904   0.88425]
72/100: train_loss=[0.29018855 0.36463344], train_acc=[0.9037  0.87875]
74/100: train_loss=[0.31041983 0.36305973], train_acc=[0.8971  0.88245]
76/100: train_loss=[0.29801106 0.36079964], train_acc=[0.90265 0.88035]
78/100: train_loss=[0.29484883 0.36514065], train_acc=[0.90305 0.87675]
80/100: train_loss=[0.28535378 0.34693384], train_acc=[0.90615 0.88525]
82/100: train_loss=[0.28728575 0.34953904], train_acc=[0.905  0.8837]
84/100: train_loss=[0.28264686 0.34981295], train_acc=[0.90735 0.88485]
86/100: train_loss=[0.2914136  0.35253042], train_acc=[0.9037 0.8821]
88/100: train_loss=[0.28690988 0.35364047], train_acc=[0.90535 0.88205]
90/100: train_loss=[0.30120975 0.3455894 ], train_acc=[0.90315 0.88665]
92/100: train_loss=[0.28990588 0.34635335], train_acc=[0.90555 0.8868 ]
94/100: train_loss=[0.28398955 0.33966434], train_acc=[0.9072  0.88765]
96/100: train_loss=[0.29010406 0.34657833], train_acc=[0.90435 0.8864 ]
98/100: train_loss=[0.2911124  0.35465798], train_acc=[0.905  0.8824]
100/100: train_loss=[0.30835435 0.38072506], train_acc=[0.8998  0.87145]
**** Time taken for mnist_2 = 2625.4549038410187
loading dataset mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.0770091 1.2972422], train_acc=[0.64655 0.54625]
2/100: train_loss=[0.79558015 0.95810044], train_acc=[0.7431  0.67445]
4/100: train_loss=[0.59846276 0.7341894 ], train_acc=[0.809  0.7537]
6/100: train_loss=[0.53747195 0.6251899 ], train_acc=[0.827   0.78695]
8/100: train_loss=[0.4915059 0.5948066], train_acc=[0.84165 0.79645]
10/100: train_loss=[0.43911234 0.54194355], train_acc=[0.8578  0.81855]
12/100: train_loss=[0.42650142 0.5097933 ], train_acc=[0.86265 0.8315 ]
14/100: train_loss=[0.41341272 0.48296198], train_acc=[0.8653 0.8407]
16/100: train_loss=[0.39615056 0.48315567], train_acc=[0.8697  0.84055]
18/100: train_loss=[0.3873327  0.45907366], train_acc=[0.87495 0.8496 ]
20/100: train_loss=[0.37747753 0.48465526], train_acc=[0.87905 0.83725]
22/100: train_loss=[0.37117994 0.44121814], train_acc=[0.88025 0.8536 ]
24/100: train_loss=[0.36634642 0.4240556 ], train_acc=[0.8815 0.858 ]
26/100: train_loss=[0.35649177 0.4212925 ], train_acc=[0.8847  0.86035]
28/100: train_loss=[0.45105165 0.4709252 ], train_acc=[0.85235 0.84545]
30/100: train_loss=[0.34584114 0.40063015], train_acc=[0.8888  0.86545]
32/100: train_loss=[0.38264224 0.44649667], train_acc=[0.87725 0.8517 ]
34/100: train_loss=[0.3397586  0.39359677], train_acc=[0.8883 0.8713]
36/100: train_loss=[0.3630162  0.40974963], train_acc=[0.88105 0.86165]
38/100: train_loss=[0.3352865  0.40153962], train_acc=[0.89025 0.86555]
40/100: train_loss=[0.33850732 0.3894498 ], train_acc=[0.8893 0.8701]
42/100: train_loss=[0.3370148  0.38073084], train_acc=[0.8883 0.8735]
44/100: train_loss=[0.3298267  0.38101348], train_acc=[0.89155 0.8731 ]
46/100: train_loss=[0.3302667 0.3844155], train_acc=[0.8925 0.8716]
48/100: train_loss=[0.34324333 0.3729586 ], train_acc=[0.8884 0.8752]
50/100: train_loss=[0.31884825 0.37973872], train_acc=[0.8957 0.8734]
52/100: train_loss=[0.32830906 0.36943457], train_acc=[0.89145 0.87775]
54/100: train_loss=[0.3209907  0.37121588], train_acc=[0.897   0.87715]
56/100: train_loss=[0.31673    0.37028936], train_acc=[0.89755 0.8784 ]
58/100: train_loss=[0.31982243 0.36483696], train_acc=[0.89685 0.8795 ]
60/100: train_loss=[0.30942622 0.41859418], train_acc=[0.8989  0.86005]
62/100: train_loss=[0.30727574 0.38092688], train_acc=[0.9007 0.8737]
64/100: train_loss=[0.3159158 0.3709897], train_acc=[0.89765 0.87655]
66/100: train_loss=[0.30909073 0.37476584], train_acc=[0.89865 0.8757 ]
68/100: train_loss=[0.31223434 0.373745  ], train_acc=[0.8988  0.87435]
70/100: train_loss=[0.31093752 0.38192397], train_acc=[0.8975 0.8741]
72/100: train_loss=[0.30769026 0.36167252], train_acc=[0.9002  0.88055]
74/100: train_loss=[0.29760987 0.35562006], train_acc=[0.90395 0.88105]
76/100: train_loss=[0.29793406 0.35277784], train_acc=[0.90325 0.8831 ]
78/100: train_loss=[0.29843464 0.3531059 ], train_acc=[0.90275 0.8844 ]
80/100: train_loss=[0.2973115 0.3588842], train_acc=[0.90415 0.8813 ]
82/100: train_loss=[0.29550305 0.36242226], train_acc=[0.9034  0.88145]
84/100: train_loss=[0.29901394 0.3614782 ], train_acc=[0.90335 0.8813 ]
86/100: train_loss=[0.2946374 0.3527637], train_acc=[0.9044  0.88145]
88/100: train_loss=[0.29317623 0.3572805 ], train_acc=[0.90395 0.88225]
90/100: train_loss=[0.28776467 0.3412557 ], train_acc=[0.90665 0.8878 ]
92/100: train_loss=[0.2850125  0.34753758], train_acc=[0.90635 0.88535]
94/100: train_loss=[0.2883198  0.36305964], train_acc=[0.9069  0.88045]
96/100: train_loss=[0.28588203 0.34072897], train_acc=[0.90785 0.88955]
98/100: train_loss=[0.28646514 0.34787786], train_acc=[0.9082  0.88495]
100/100: train_loss=[0.28742838 0.34298566], train_acc=[0.9079 0.8876]
**** Time taken for mnist_3 = 2622.964390039444
loading dataset mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[0.89726204 1.1344695 ], train_acc=[0.706   0.62095]
2/100: train_loss=[0.7086798 0.9204887], train_acc=[0.7669  0.69105]
4/100: train_loss=[0.5579952 0.6909425], train_acc=[0.81675 0.7676 ]
6/100: train_loss=[0.53470314 0.6438212 ], train_acc=[0.8249  0.78605]
8/100: train_loss=[0.44651902 0.5446123 ], train_acc=[0.8527 0.8143]
10/100: train_loss=[0.44350594 0.52721316], train_acc=[0.8541  0.82225]
12/100: train_loss=[0.41412625 0.4985526 ], train_acc=[0.86245 0.834  ]
14/100: train_loss=[0.3934009 0.4788646], train_acc=[0.8706  0.83915]
16/100: train_loss=[0.39654312 0.46486968], train_acc=[0.86925 0.8433 ]
18/100: train_loss=[0.38019612 0.44553173], train_acc=[0.87505 0.85105]
20/100: train_loss=[0.38650745 0.45931193], train_acc=[0.8716  0.84955]
22/100: train_loss=[0.3621326  0.43795726], train_acc=[0.8783  0.85245]
24/100: train_loss=[0.3475003 0.4192747], train_acc=[0.8849  0.85965]
26/100: train_loss=[0.34163827 0.42737463], train_acc=[0.8864  0.85645]
28/100: train_loss=[0.33420625 0.4125089 ], train_acc=[0.8877  0.86225]
30/100: train_loss=[0.33390182 0.39805475], train_acc=[0.88745 0.8662 ]
32/100: train_loss=[0.3342984 0.4038183], train_acc=[0.88895 0.86605]
34/100: train_loss=[0.32937202 0.40561977], train_acc=[0.8906 0.8651]
36/100: train_loss=[0.35357794 0.4130811 ], train_acc=[0.88475 0.8626 ]
38/100: train_loss=[0.3206351 0.3906852], train_acc=[0.89135 0.87065]
40/100: train_loss=[0.311248   0.38781136], train_acc=[0.89595 0.8704 ]
42/100: train_loss=[0.3248863  0.38882786], train_acc=[0.8918  0.87215]
44/100: train_loss=[0.31518295 0.3812919 ], train_acc=[0.89615 0.8741 ]
46/100: train_loss=[0.30761647 0.37423614], train_acc=[0.89885 0.87525]
48/100: train_loss=[0.30525497 0.37725708], train_acc=[0.8975  0.87515]
50/100: train_loss=[0.31819254 0.3725334 ], train_acc=[0.8944 0.8781]
52/100: train_loss=[0.3083757  0.38166848], train_acc=[0.89745 0.87455]
54/100: train_loss=[0.30951026 0.3744551 ], train_acc=[0.89705 0.87535]
56/100: train_loss=[0.2966855  0.37058416], train_acc=[0.90045 0.87745]
58/100: train_loss=[0.29841533 0.37421045], train_acc=[0.90175 0.87435]
60/100: train_loss=[0.30216724 0.36252773], train_acc=[0.89855 0.8807 ]
62/100: train_loss=[0.2953345 0.3661224], train_acc=[0.90015 0.8801 ]
64/100: train_loss=[0.30413094 0.36472738], train_acc=[0.8996 0.8797]
66/100: train_loss=[0.29091537 0.36634123], train_acc=[0.90275 0.88005]
68/100: train_loss=[0.28962407 0.35491484], train_acc=[0.9046  0.88225]
70/100: train_loss=[0.29658818 0.36451843], train_acc=[0.9017  0.88045]
72/100: train_loss=[0.28703567 0.3501394 ], train_acc=[0.904   0.88505]
74/100: train_loss=[0.2972116  0.35118294], train_acc=[0.90285 0.88245]
76/100: train_loss=[0.2851666  0.36452726], train_acc=[0.90555 0.88095]
78/100: train_loss=[0.28566194 0.35028493], train_acc=[0.9054  0.88315]
80/100: train_loss=[0.28540373 0.36403623], train_acc=[0.9069  0.88095]
82/100: train_loss=[0.27871218 0.3440334 ], train_acc=[0.90725 0.88615]
84/100: train_loss=[0.2902876  0.36820772], train_acc=[0.905  0.8774]
86/100: train_loss=[0.2870625 0.351417 ], train_acc=[0.90565 0.88485]
88/100: train_loss=[0.28428492 0.35282913], train_acc=[0.9073 0.8851]
90/100: train_loss=[0.28006566 0.3468596 ], train_acc=[0.9091 0.8858]
92/100: train_loss=[0.28028023 0.34365875], train_acc=[0.90785 0.88745]
94/100: train_loss=[0.27691457 0.34944856], train_acc=[0.9097 0.8855]
96/100: train_loss=[0.2762447  0.34492242], train_acc=[0.9092  0.88865]
98/100: train_loss=[0.27390718 0.35141355], train_acc=[0.9103 0.8832]
100/100: train_loss=[0.28918123 0.34719464], train_acc=[0.9049  0.88685]
**** Time taken for mnist_4 = 2646.1750345230103
**** Time taken for mnist = 13069.828683376312
loading dataset fashion
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.575781  1.6029999], train_acc=[0.4456  0.42485]
2/100: train_loss=[1.0566214 1.1065652], train_acc=[0.60295 0.5942 ]
4/100: train_loss=[0.91023856 0.95983803], train_acc=[0.666   0.64555]
6/100: train_loss=[0.8354268 0.9098564], train_acc=[0.692  0.6683]
8/100: train_loss=[0.77976656 0.8034149 ], train_acc=[0.71215 0.70655]
10/100: train_loss=[0.7604682 0.7764314], train_acc=[0.72125 0.7125 ]
12/100: train_loss=[0.72811294 0.74577457], train_acc=[0.7296 0.7255]
14/100: train_loss=[0.71135134 0.7357757 ], train_acc=[0.73825 0.7318 ]
16/100: train_loss=[0.6966885  0.74104893], train_acc=[0.74435 0.72815]
18/100: train_loss=[0.6912282  0.70257777], train_acc=[0.74625 0.74325]
20/100: train_loss=[0.6567153 0.7191722], train_acc=[0.7615 0.7346]
22/100: train_loss=[0.6887941 0.706025 ], train_acc=[0.74295 0.7394 ]
24/100: train_loss=[0.68570095 0.7667142 ], train_acc=[0.74675 0.71715]
26/100: train_loss=[0.65647566 0.68195385], train_acc=[0.75925 0.74345]
28/100: train_loss=[0.63259774 0.66237354], train_acc=[0.7704 0.7545]
30/100: train_loss=[0.6271077 0.6544024], train_acc=[0.7752  0.76065]
32/100: train_loss=[0.6165996  0.65034777], train_acc=[0.7751 0.7612]
34/100: train_loss=[0.6191101 0.6473404], train_acc=[0.77385 0.76265]
36/100: train_loss=[0.6456485  0.64906305], train_acc=[0.7631 0.7643]
38/100: train_loss=[0.6182619  0.64334756], train_acc=[0.7755 0.7627]
40/100: train_loss=[0.62893236 0.63879186], train_acc=[0.77135 0.7658 ]
42/100: train_loss=[0.60247034 0.62323165], train_acc=[0.7817  0.77115]
44/100: train_loss=[0.5948343 0.6351095], train_acc=[0.78495 0.7669 ]
46/100: train_loss=[0.60794127 0.6375052 ], train_acc=[0.77835 0.7624 ]
48/100: train_loss=[0.60764563 0.63736033], train_acc=[0.7772 0.7685]
50/100: train_loss=[0.62388426 0.6473412 ], train_acc=[0.772  0.7595]
52/100: train_loss=[0.5915528  0.61784035], train_acc=[0.7849 0.7752]
54/100: train_loss=[0.57992977 0.62377554], train_acc=[0.78895 0.7715 ]
56/100: train_loss=[0.57710344 0.6164899 ], train_acc=[0.79095 0.7723 ]
58/100: train_loss=[0.5725133 0.6148409], train_acc=[0.7915 0.7707]
60/100: train_loss=[0.5801816 0.6090853], train_acc=[0.7911  0.77705]
62/100: train_loss=[0.57759416 0.60947853], train_acc=[0.7911 0.7705]
64/100: train_loss=[0.58429176 0.59595436], train_acc=[0.78775 0.78315]
66/100: train_loss=[0.5630818 0.596612 ], train_acc=[0.7978  0.78155]
68/100: train_loss=[0.5788777  0.59735215], train_acc=[0.7893  0.78055]
70/100: train_loss=[0.58316094 0.6161125 ], train_acc=[0.7895 0.7729]
72/100: train_loss=[0.5579153  0.58997333], train_acc=[0.7974 0.7829]
74/100: train_loss=[0.5624789 0.5945458], train_acc=[0.79735 0.78195]
76/100: train_loss=[0.5727922 0.5820976], train_acc=[0.79485 0.78775]
78/100: train_loss=[0.5568736  0.59323776], train_acc=[0.79695 0.78315]
80/100: train_loss=[0.56558067 0.5887018 ], train_acc=[0.79405 0.78735]
82/100: train_loss=[0.56694835 0.5975508 ], train_acc=[0.7934 0.7818]
84/100: train_loss=[0.6000376  0.60381466], train_acc=[0.78155 0.78335]
86/100: train_loss=[0.56505173 0.60457265], train_acc=[0.79835 0.77105]
88/100: train_loss=[0.5703786  0.57957923], train_acc=[0.7921 0.7887]
90/100: train_loss=[0.55441684 0.57236725], train_acc=[0.79885 0.7913 ]
92/100: train_loss=[0.5704416 0.5787933], train_acc=[0.7891 0.7891]
94/100: train_loss=[0.5499122  0.57591236], train_acc=[0.80035 0.78975]
96/100: train_loss=[0.54335415 0.5718877 ], train_acc=[0.801  0.7923]
98/100: train_loss=[0.54524857 0.57483095], train_acc=[0.80355 0.78875]
100/100: train_loss=[0.5493478 0.5805241], train_acc=[0.80015 0.78835]
**** Time taken for fashion_0 = 2691.026032447815
loading dataset fashion
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.0584244 1.140168 ], train_acc=[0.60785 0.58975]
2/100: train_loss=[0.92870295 0.9897492 ], train_acc=[0.64405 0.63985]
4/100: train_loss=[0.8214153 0.8778254], train_acc=[0.68865 0.6735 ]
6/100: train_loss=[0.7877196 0.7980418], train_acc=[0.70575 0.7088 ]
8/100: train_loss=[0.7258262  0.73776305], train_acc=[0.72785 0.73085]
10/100: train_loss=[0.6854169 0.71052  ], train_acc=[0.7465  0.74215]
12/100: train_loss=[0.6805561 0.6869949], train_acc=[0.74625 0.7471 ]
14/100: train_loss=[0.66242594 0.6700323 ], train_acc=[0.754   0.75335]
16/100: train_loss=[0.6335941  0.65408677], train_acc=[0.7659  0.75975]
18/100: train_loss=[0.64028114 0.6458815 ], train_acc=[0.76315 0.7642 ]
20/100: train_loss=[0.62834215 0.63682455], train_acc=[0.7692 0.7663]
22/100: train_loss=[0.6302897 0.6201259], train_acc=[0.7668 0.7722]
24/100: train_loss=[0.6710818 0.6972675], train_acc=[0.75    0.73805]
26/100: train_loss=[0.59442234 0.60726047], train_acc=[0.7787 0.775 ]
28/100: train_loss=[0.6079162  0.61558616], train_acc=[0.7713 0.7713]
30/100: train_loss=[0.66529894 0.6537815 ], train_acc=[0.749   0.76325]
32/100: train_loss=[0.59237117 0.59994334], train_acc=[0.7791 0.7831]
34/100: train_loss=[0.5830042 0.5871411], train_acc=[0.7829 0.7866]
36/100: train_loss=[0.57946634 0.57827115], train_acc=[0.7881 0.7873]
38/100: train_loss=[0.57718927 0.5887703 ], train_acc=[0.78565 0.7838 ]
40/100: train_loss=[0.57383364 0.576252  ], train_acc=[0.78815 0.7883 ]
42/100: train_loss=[0.56517637 0.5758452 ], train_acc=[0.7927 0.7896]
44/100: train_loss=[0.55915546 0.5603356 ], train_acc=[0.79085 0.79725]
46/100: train_loss=[0.5716911 0.5671493], train_acc=[0.7913 0.7946]
48/100: train_loss=[0.5597591 0.5609209], train_acc=[0.7923 0.7936]
50/100: train_loss=[0.57175654 0.5681222 ], train_acc=[0.78955 0.78935]
52/100: train_loss=[0.5628945  0.55909425], train_acc=[0.79165 0.79575]
54/100: train_loss=[0.5491615 0.5506354], train_acc=[0.7997 0.7993]
56/100: train_loss=[0.5904849 0.5649232], train_acc=[0.78745 0.7941 ]
58/100: train_loss=[0.5505671 0.5567853], train_acc=[0.79895 0.7958 ]
60/100: train_loss=[0.5502623 0.5575856], train_acc=[0.79455 0.8003 ]
62/100: train_loss=[0.5465947 0.5537912], train_acc=[0.7988  0.79855]
64/100: train_loss=[0.55952126 0.55064446], train_acc=[0.79425 0.7981 ]
66/100: train_loss=[0.55047435 0.5617131 ], train_acc=[0.7954 0.7942]
68/100: train_loss=[0.5391698  0.54247725], train_acc=[0.8038  0.80105]
70/100: train_loss=[0.56536067 0.54866135], train_acc=[0.7906  0.80045]
72/100: train_loss=[0.54084307 0.53763777], train_acc=[0.80195 0.8026 ]
74/100: train_loss=[0.53787464 0.5398367 ], train_acc=[0.80175 0.80435]
76/100: train_loss=[0.53493595 0.54091656], train_acc=[0.8054  0.80375]
78/100: train_loss=[0.5414503 0.5654896], train_acc=[0.80005 0.7959 ]
80/100: train_loss=[0.52893233 0.5479446 ], train_acc=[0.805  0.8012]
82/100: train_loss=[0.55120647 0.5472264 ], train_acc=[0.79475 0.80335]
84/100: train_loss=[0.5406645  0.53844327], train_acc=[0.8021 0.8022]
86/100: train_loss=[0.53080255 0.53623164], train_acc=[0.8059  0.80345]
88/100: train_loss=[0.52850586 0.53907233], train_acc=[0.8055  0.80595]
90/100: train_loss=[0.5236597 0.5307755], train_acc=[0.8082 0.8053]
92/100: train_loss=[0.5258188 0.5318371], train_acc=[0.80635 0.80605]
94/100: train_loss=[0.5283638  0.53383505], train_acc=[0.80715 0.80525]
96/100: train_loss=[0.52682656 0.5327168 ], train_acc=[0.8082 0.8061]
98/100: train_loss=[0.533628  0.5330408], train_acc=[0.8028 0.8071]
100/100: train_loss=[0.5321993  0.55199146], train_acc=[0.80385 0.7949 ]
**** Time taken for fashion_1 = 2687.4187638759613
loading dataset fashion
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.0648555 1.1505665], train_acc=[0.60445 0.5795 ]
2/100: train_loss=[0.929436  1.0156316], train_acc=[0.65585 0.62285]
4/100: train_loss=[0.82584655 0.87948585], train_acc=[0.6973 0.673 ]
6/100: train_loss=[0.76560897 0.7935135 ], train_acc=[0.71575 0.70805]
8/100: train_loss=[0.74747807 0.7610531 ], train_acc=[0.72245 0.72305]
10/100: train_loss=[0.7206681  0.73878926], train_acc=[0.73455 0.7283 ]
12/100: train_loss=[0.66635287 0.70141685], train_acc=[0.75285 0.7387 ]
14/100: train_loss=[0.6632168 0.6796467], train_acc=[0.75235 0.75025]
16/100: train_loss=[0.6638292 0.6807112], train_acc=[0.75195 0.7451 ]
18/100: train_loss=[0.62672293 0.6584871 ], train_acc=[0.7687 0.7574]
20/100: train_loss=[0.6239393 0.6422548], train_acc=[0.77135 0.761  ]
22/100: train_loss=[0.6119564  0.64102834], train_acc=[0.7731  0.76305]
24/100: train_loss=[0.6077335 0.6474934], train_acc=[0.7732 0.7623]
26/100: train_loss=[0.5917017 0.6260078], train_acc=[0.78175 0.76985]
28/100: train_loss=[0.5900225 0.6227991], train_acc=[0.7805  0.76945]
30/100: train_loss=[0.5814626 0.607305 ], train_acc=[0.78445 0.7767 ]
32/100: train_loss=[0.5732252 0.6306471], train_acc=[0.78655 0.76255]
34/100: train_loss=[0.5794643 0.5985608], train_acc=[0.78385 0.78235]
36/100: train_loss=[0.5630434  0.59460384], train_acc=[0.7923  0.78355]
38/100: train_loss=[0.5620376 0.5910705], train_acc=[0.79055 0.78265]
40/100: train_loss=[0.5658572 0.5853955], train_acc=[0.7908 0.788 ]
42/100: train_loss=[0.55984586 0.5848241 ], train_acc=[0.79045 0.7863 ]
44/100: train_loss=[0.5525105 0.6151159], train_acc=[0.7935 0.768 ]
46/100: train_loss=[0.60533845 0.58509946], train_acc=[0.7747 0.7877]
48/100: train_loss=[0.55648375 0.5787881 ], train_acc=[0.79425 0.78955]
50/100: train_loss=[0.56282496 0.57841986], train_acc=[0.79025 0.78865]
52/100: train_loss=[0.5645775 0.5849052], train_acc=[0.79205 0.78275]
54/100: train_loss=[0.55479866 0.5784089 ], train_acc=[0.7928 0.7876]
56/100: train_loss=[0.5429311  0.57269764], train_acc=[0.7959  0.79145]
58/100: train_loss=[0.53914064 0.56697285], train_acc=[0.80165 0.7943 ]
60/100: train_loss=[0.5568121 0.5682331], train_acc=[0.79315 0.7943 ]
62/100: train_loss=[0.5361203 0.5643466], train_acc=[0.8022  0.79545]
64/100: train_loss=[0.5388387 0.567284 ], train_acc=[0.7991  0.79565]
66/100: train_loss=[0.53960496 0.5632384 ], train_acc=[0.8013 0.7966]
68/100: train_loss=[0.5362639  0.56403726], train_acc=[0.8016  0.79705]
70/100: train_loss=[0.5454534 0.5542452], train_acc=[0.795   0.80035]
72/100: train_loss=[0.53779614 0.5550486 ], train_acc=[0.8022 0.7971]
74/100: train_loss=[0.51986754 0.54987913], train_acc=[0.8071 0.8028]
76/100: train_loss=[0.53442913 0.56028694], train_acc=[0.80305 0.79755]
78/100: train_loss=[0.5290155  0.56084055], train_acc=[0.8054 0.7955]
80/100: train_loss=[0.52380913 0.54226756], train_acc=[0.80745 0.8055 ]
82/100: train_loss=[0.52304405 0.5486512 ], train_acc=[0.8068  0.80095]
84/100: train_loss=[0.52898806 0.5514062 ], train_acc=[0.80675 0.7984 ]
86/100: train_loss=[0.51998955 0.5403929 ], train_acc=[0.80705 0.8063 ]
88/100: train_loss=[0.5229205 0.5496226], train_acc=[0.8072 0.8047]
90/100: train_loss=[0.5196052 0.5392046], train_acc=[0.8092 0.806 ]
92/100: train_loss=[0.5209635  0.53890663], train_acc=[0.8066 0.8071]
94/100: train_loss=[0.5200922  0.54603714], train_acc=[0.80895 0.80375]
96/100: train_loss=[0.5876889  0.57465875], train_acc=[0.7903 0.7938]
98/100: train_loss=[0.5156653 0.5478875], train_acc=[0.8095  0.80265]
100/100: train_loss=[0.5261278 0.5478536], train_acc=[0.8096 0.8012]
**** Time taken for fashion_2 = 2663.9699170589447
loading dataset fashion
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[2.306325  2.3052576], train_acc=[0.0988  0.10165]
2/100: train_loss=[2.30591   2.3048897], train_acc=[0.09775 0.10165]
4/100: train_loss=[2.305202  2.3043804], train_acc=[0.09555 0.10165]
6/100: train_loss=[2.3044543 2.3040242], train_acc=[0.0955  0.10165]
8/100: train_loss=[2.3038511 2.3039289], train_acc=[0.09355 0.10165]
10/100: train_loss=[2.3034868 2.3037066], train_acc=[0.09325 0.1017 ]
12/100: train_loss=[2.303274  2.3034136], train_acc=[0.09385 0.1017 ]
14/100: train_loss=[2.3029287 2.3033724], train_acc=[0.0919  0.10155]
16/100: train_loss=[2.3027413 2.303241 ], train_acc=[0.09215 0.10185]
18/100: train_loss=[2.3024805 2.3031447], train_acc=[0.091   0.10175]
20/100: train_loss=[2.302427  2.3028908], train_acc=[0.09185 0.1018 ]
22/100: train_loss=[2.3023422 2.30278  ], train_acc=[0.0911  0.10075]
24/100: train_loss=[2.3022072 2.3025908], train_acc=[0.09145 0.0941 ]
26/100: train_loss=[2.3020241 2.30285  ], train_acc=[0.08735 0.09385]
28/100: train_loss=[2.3018148 2.3026447], train_acc=[0.08955 0.0944 ]
30/100: train_loss=[2.3017697 2.3026097], train_acc=[0.08925 0.09455]
32/100: train_loss=[2.3014665 2.302826 ], train_acc=[0.087  0.0944]
34/100: train_loss=[2.3013754 2.302686 ], train_acc=[0.0874 0.0944]
36/100: train_loss=[2.3010788 2.3028796], train_acc=[0.0843  0.09465]
38/100: train_loss=[2.3010902 2.3024104], train_acc=[0.08635 0.0939 ]
40/100: train_loss=[2.3010042 2.3021896], train_acc=[0.10705 0.09375]
42/100: train_loss=[2.3004358 2.3022366], train_acc=[0.10035 0.0942 ]
44/100: train_loss=[2.2994604 2.301994 ], train_acc=[0.1063 0.0946]
46/100: train_loss=[2.299404  2.3018167], train_acc=[0.1116  0.09425]
48/100: train_loss=[2.296767  2.3016288], train_acc=[0.1152  0.09445]
50/100: train_loss=[2.2845438 2.304829 ], train_acc=[0.1616  0.08505]
52/100: train_loss=[2.2574837 2.304439 ], train_acc=[0.17895 0.08735]
54/100: train_loss=[2.0494945 2.2566686], train_acc=[0.2256 0.1553]
56/100: train_loss=[1.5016198 1.9214395], train_acc=[0.4288  0.28445]
58/100: train_loss=[1.2471876 1.5976161], train_acc=[0.52135 0.4057 ]
60/100: train_loss=[1.1683304 1.413862 ], train_acc=[0.5513 0.4814]
62/100: train_loss=[1.1608198 1.2805827], train_acc=[0.5523 0.5245]
64/100: train_loss=[1.1065941 1.2042154], train_acc=[0.588   0.54995]
66/100: train_loss=[1.0157932 1.0906178], train_acc=[0.61425 0.59585]
68/100: train_loss=[0.9774739 1.0421996], train_acc=[0.6337  0.61525]
70/100: train_loss=[0.934745  1.0121527], train_acc=[0.6519 0.6203]
72/100: train_loss=[0.9050046 0.9573498], train_acc=[0.66265 0.6473 ]
74/100: train_loss=[0.97943467 1.0067472 ], train_acc=[0.6373 0.6242]
76/100: train_loss=[0.9090617  0.91276544], train_acc=[0.6648  0.65975]
78/100: train_loss=[1.0343952 1.0401129], train_acc=[0.61185 0.61385]
80/100: train_loss=[0.8308563 0.8706459], train_acc=[0.6897 0.6744]
82/100: train_loss=[0.84080833 0.8839158 ], train_acc=[0.6818  0.67675]
84/100: train_loss=[0.82172406 0.8722884 ], train_acc=[0.69215 0.6727 ]
86/100: train_loss=[0.8382076 0.8631654], train_acc=[0.68375 0.68225]
88/100: train_loss=[0.80581456 0.80997974], train_acc=[0.69525 0.70275]
90/100: train_loss=[0.81668246 0.8633388 ], train_acc=[0.6917  0.67775]
92/100: train_loss=[0.82806784 0.864241  ], train_acc=[0.6887  0.68955]
94/100: train_loss=[0.8107387  0.80613303], train_acc=[0.6964 0.7035]
96/100: train_loss=[0.79663664 0.7910191 ], train_acc=[0.7031  0.70395]
98/100: train_loss=[0.77477336 0.78292716], train_acc=[0.71525 0.7083 ]
100/100: train_loss=[0.7928685  0.80513716], train_acc=[0.70565 0.69855]
**** Time taken for fashion_3 = 2489.729991674423
loading dataset fashion
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.143294  1.1818568], train_acc=[0.5665 0.572 ]
2/100: train_loss=[1.087436  1.1665307], train_acc=[0.5968 0.5723]
4/100: train_loss=[0.82982665 0.8417915 ], train_acc=[0.69005 0.6866 ]
6/100: train_loss=[0.78895223 0.7883586 ], train_acc=[0.6986 0.7134]
8/100: train_loss=[0.73122793 0.74081564], train_acc=[0.72785 0.73145]
10/100: train_loss=[0.7380536 0.7288895], train_acc=[0.7208  0.72835]
12/100: train_loss=[0.7014092 0.6992029], train_acc=[0.7388 0.7405]
14/100: train_loss=[0.6769551 0.6857161], train_acc=[0.7519  0.74885]
16/100: train_loss=[0.664451   0.70421684], train_acc=[0.7557 0.7378]
18/100: train_loss=[0.6503441  0.68211347], train_acc=[0.7625 0.7529]
20/100: train_loss=[0.67525774 0.6598618 ], train_acc=[0.75045 0.75945]
22/100: train_loss=[0.63403624 0.6486907 ], train_acc=[0.7663 0.7612]
24/100: train_loss=[0.6559965 0.6530275], train_acc=[0.7521 0.7631]
26/100: train_loss=[0.628835   0.63645464], train_acc=[0.76755 0.76595]
28/100: train_loss=[0.6334595 0.625344 ], train_acc=[0.76255 0.7704 ]
30/100: train_loss=[0.6003991 0.6246238], train_acc=[0.7803 0.773 ]
32/100: train_loss=[0.6261889  0.61610717], train_acc=[0.76515 0.777  ]
34/100: train_loss=[0.5912782 0.615031 ], train_acc=[0.78285 0.77535]
36/100: train_loss=[0.5943338  0.62038314], train_acc=[0.7806 0.7731]
38/100: train_loss=[0.60219264 0.60375   ], train_acc=[0.7783 0.7819]
40/100: train_loss=[0.5875796 0.6014608], train_acc=[0.7859 0.7816]
42/100: train_loss=[0.583905  0.5930305], train_acc=[0.78555 0.7853 ]
44/100: train_loss=[0.5796099 0.5988048], train_acc=[0.7857 0.7844]
46/100: train_loss=[0.57982934 0.59683716], train_acc=[0.78675 0.78745]
48/100: train_loss=[0.5754862 0.6068647], train_acc=[0.7902 0.7797]
50/100: train_loss=[0.5654628 0.6012131], train_acc=[0.7937  0.77945]
52/100: train_loss=[0.57296246 0.5836513 ], train_acc=[0.79005 0.78945]
54/100: train_loss=[0.56778127 0.5900164 ], train_acc=[0.79295 0.78505]
56/100: train_loss=[0.58288914 0.5949634 ], train_acc=[0.7841 0.7843]
58/100: train_loss=[0.5600396 0.5878641], train_acc=[0.7951 0.7905]
60/100: train_loss=[0.5597095 0.57283  ], train_acc=[0.7968  0.79445]
62/100: train_loss=[0.5795679 0.6275575], train_acc=[0.7871  0.78005]
64/100: train_loss=[0.5526613 0.5705142], train_acc=[0.7981 0.7939]
66/100: train_loss=[0.5453525 0.5683377], train_acc=[0.7999 0.7955]
68/100: train_loss=[0.55799896 0.5728035 ], train_acc=[0.7938  0.79205]
70/100: train_loss=[0.5435116 0.5750251], train_acc=[0.8024  0.79485]
72/100: train_loss=[0.55474997 0.568828  ], train_acc=[0.79785 0.79595]
74/100: train_loss=[0.58322215 0.6062541 ], train_acc=[0.79045 0.7854 ]
76/100: train_loss=[0.5488614  0.57729524], train_acc=[0.80015 0.78845]
78/100: train_loss=[0.58386123 0.6267209 ], train_acc=[0.7935  0.76515]
80/100: train_loss=[0.53975254 0.5610737 ], train_acc=[0.8042  0.79675]
82/100: train_loss=[0.55306035 0.5778541 ], train_acc=[0.79845 0.78725]
84/100: train_loss=[0.5377981  0.55984044], train_acc=[0.8044 0.7988]
86/100: train_loss=[0.53449243 0.56355035], train_acc=[0.80395 0.79765]
88/100: train_loss=[0.5380299  0.56097484], train_acc=[0.8027 0.797 ]
90/100: train_loss=[0.5322773 0.5495246], train_acc=[0.80475 0.801  ]
92/100: train_loss=[0.53484815 0.54886276], train_acc=[0.8059 0.8001]
94/100: train_loss=[0.52793175 0.55750394], train_acc=[0.8083  0.79765]
96/100: train_loss=[0.55812573 0.57712084], train_acc=[0.79675 0.79135]
98/100: train_loss=[0.53009343 0.5555651 ], train_acc=[0.8043 0.7995]
100/100: train_loss=[0.5358894  0.54464555], train_acc=[0.806   0.80275]
**** Time taken for fashion_4 = 2385.0960595607758
**** Time taken for fashion = 12917.289246082306
loading dataset fashion_and_mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.2301425 0.9915706], train_acc=[0.57815 0.63725]
2/100: train_loss=[0.94514894 0.8593276 ], train_acc=[0.68365 0.67875]
4/100: train_loss=[0.7152374  0.74903256], train_acc=[0.76655 0.724  ]
6/100: train_loss=[0.6342575 0.7198502], train_acc=[0.7956 0.734 ]
8/100: train_loss=[0.5468625 0.6870517], train_acc=[0.82215 0.7482 ]
10/100: train_loss=[0.5134649  0.67723733], train_acc=[0.83325 0.75155]
12/100: train_loss=[0.52114564 0.70892215], train_acc=[0.82865 0.7365 ]
14/100: train_loss=[0.44252077 0.6221856 ], train_acc=[0.8596 0.7718]
16/100: train_loss=[0.43341318 0.6128802 ], train_acc=[0.85715 0.77095]
18/100: train_loss=[0.39883843 0.60671276], train_acc=[0.8696 0.7751]
20/100: train_loss=[0.37332407 0.58250207], train_acc=[0.87785 0.7828 ]
22/100: train_loss=[0.38690716 0.5898131 ], train_acc=[0.8743 0.782 ]
24/100: train_loss=[0.3699346 0.5773079], train_acc=[0.8791  0.78525]
26/100: train_loss=[0.34311065 0.5815426 ], train_acc=[0.89005 0.7826 ]
28/100: train_loss=[0.34337857 0.5643402 ], train_acc=[0.8888  0.79015]
30/100: train_loss=[0.4282624 0.7093191], train_acc=[0.8616 0.7415]
32/100: train_loss=[0.3285213  0.54490566], train_acc=[0.89385 0.79935]
34/100: train_loss=[0.3180978  0.54313767], train_acc=[0.89725 0.79925]
36/100: train_loss=[0.3063655 0.5374167], train_acc=[0.9011  0.80255]
38/100: train_loss=[0.302143  0.5421315], train_acc=[0.90175 0.80115]
40/100: train_loss=[0.30066335 0.55084413], train_acc=[0.90345 0.79625]
42/100: train_loss=[0.3251324 0.5429357], train_acc=[0.89315 0.80195]
44/100: train_loss=[0.29373637 0.52802193], train_acc=[0.9045 0.8047]
46/100: train_loss=[0.29313186 0.5430748 ], train_acc=[0.90485 0.79875]
48/100: train_loss=[0.28641596 0.5218099 ], train_acc=[0.90875 0.8065 ]
50/100: train_loss=[0.27801555 0.52418154], train_acc=[0.91065 0.8045 ]
52/100: train_loss=[0.27428916 0.5444594 ], train_acc=[0.91255 0.7986 ]
54/100: train_loss=[0.26859865 0.51610184], train_acc=[0.9141 0.8089]
56/100: train_loss=[0.280287   0.51139927], train_acc=[0.91115 0.8129 ]
58/100: train_loss=[0.27525568 0.51666135], train_acc=[0.9111  0.80865]
60/100: train_loss=[0.26981577 0.56216073], train_acc=[0.914  0.7879]
62/100: train_loss=[0.2584291 0.5046282], train_acc=[0.91715 0.81115]
64/100: train_loss=[0.27198163 0.5281362 ], train_acc=[0.9129 0.8031]
66/100: train_loss=[0.25967112 0.50613624], train_acc=[0.9186  0.81345]
68/100: train_loss=[0.25560352 0.52191335], train_acc=[0.91955 0.8031 ]
70/100: train_loss=[0.2616367  0.51970065], train_acc=[0.91805 0.8067 ]
72/100: train_loss=[0.3029058 0.5762471], train_acc=[0.9047  0.77675]
74/100: train_loss=[0.25647834 0.5123286 ], train_acc=[0.9171  0.81375]
76/100: train_loss=[0.2599526  0.49830997], train_acc=[0.91705 0.81745]
78/100: train_loss=[0.2514326  0.49319223], train_acc=[0.92155 0.82095]
80/100: train_loss=[0.25924966 0.49208805], train_acc=[0.91845 0.8181 ]
82/100: train_loss=[0.25589916 0.53834003], train_acc=[0.91895 0.79645]
84/100: train_loss=[0.2508155  0.49565876], train_acc=[0.9207 0.818 ]
86/100: train_loss=[0.2490733 0.5080221], train_acc=[0.9237  0.81315]
88/100: train_loss=[0.24275751 0.49861565], train_acc=[0.925   0.81795]
90/100: train_loss=[0.24881332 0.4887044 ], train_acc=[0.9237 0.822 ]
92/100: train_loss=[0.26102945 0.4959132 ], train_acc=[0.9181  0.82015]
94/100: train_loss=[0.24349484 0.48907492], train_acc=[0.9241  0.81995]
96/100: train_loss=[0.250526  0.4845307], train_acc=[0.92315 0.8233 ]
98/100: train_loss=[0.23968175 0.47837016], train_acc=[0.9253  0.82485]
100/100: train_loss=[0.24602547 0.47732124], train_acc=[0.9235  0.82635]
**** Time taken for fashion_and_mnist_0 = 2317.1287105083466
loading dataset fashion_and_mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[2.1675332 2.1461565], train_acc=[0.1721  0.17545]
2/100: train_loss=[1.7753255 1.6380966], train_acc=[0.3557  0.40215]
4/100: train_loss=[0.9926501 0.9027828], train_acc=[0.6676  0.67405]
6/100: train_loss=[0.7657089  0.82290804], train_acc=[0.75225 0.68975]
8/100: train_loss=[0.6544387 0.7692922], train_acc=[0.7896 0.7212]
10/100: train_loss=[0.637838   0.76118964], train_acc=[0.7937 0.7222]
12/100: train_loss=[0.519429   0.68399704], train_acc=[0.8326 0.7439]
14/100: train_loss=[0.49268293 0.6675718 ], train_acc=[0.84175 0.7511 ]
16/100: train_loss=[0.44511   0.6732298], train_acc=[0.8585  0.74825]
18/100: train_loss=[0.4279068  0.65338635], train_acc=[0.8612 0.757 ]
20/100: train_loss=[0.4250145 0.6365928], train_acc=[0.86425 0.76475]
22/100: train_loss=[0.39337263 0.6219723 ], train_acc=[0.87475 0.77045]
24/100: train_loss=[0.37223184 0.6209854 ], train_acc=[0.88    0.76885]
26/100: train_loss=[0.35760868 0.61288977], train_acc=[0.8845 0.7733]
28/100: train_loss=[0.34760997 0.6289517 ], train_acc=[0.8895  0.76585]
30/100: train_loss=[0.33036855 0.59737086], train_acc=[0.8942  0.78055]
32/100: train_loss=[0.32831973 0.6024555 ], train_acc=[0.89525 0.7754 ]
34/100: train_loss=[0.34466663 0.59050745], train_acc=[0.8907  0.78375]
36/100: train_loss=[0.3172953  0.60379076], train_acc=[0.8987  0.77765]
38/100: train_loss=[0.30965096 0.5852218 ], train_acc=[0.9021 0.785 ]
40/100: train_loss=[0.31360635 0.5979753 ], train_acc=[0.9012 0.7825]
42/100: train_loss=[0.332103  0.6142057], train_acc=[0.89495 0.7723 ]
44/100: train_loss=[0.35820577 0.5980247 ], train_acc=[0.88685 0.77865]
46/100: train_loss=[0.30028582 0.5752652 ], train_acc=[0.9031  0.78825]
48/100: train_loss=[0.2899305  0.57064366], train_acc=[0.9092  0.79195]
50/100: train_loss=[0.28627634 0.56984556], train_acc=[0.9093  0.79035]
52/100: train_loss=[0.2865228 0.5646814], train_acc=[0.90985 0.79275]
54/100: train_loss=[0.28340563 0.557645  ], train_acc=[0.909   0.79365]
56/100: train_loss=[0.27660987 0.5559529 ], train_acc=[0.9122 0.7981]
58/100: train_loss=[0.30916658 0.5731514 ], train_acc=[0.90265 0.7939 ]
60/100: train_loss=[0.2879964 0.5856979], train_acc=[0.90985 0.78915]
62/100: train_loss=[0.28866825 0.5775091 ], train_acc=[0.90825 0.78455]
64/100: train_loss=[0.2766462  0.56189084], train_acc=[0.913   0.79725]
66/100: train_loss=[0.27016243 0.5524096 ], train_acc=[0.9142 0.7982]
68/100: train_loss=[0.27553377 0.55228275], train_acc=[0.9134 0.7983]
70/100: train_loss=[0.2702545  0.55253416], train_acc=[0.9144 0.7991]
72/100: train_loss=[0.27227983 0.5462568 ], train_acc=[0.91495 0.80015]
74/100: train_loss=[0.2656421  0.54723805], train_acc=[0.91565 0.80185]
76/100: train_loss=[0.26002726 0.5473371 ], train_acc=[0.91755 0.79665]
78/100: train_loss=[0.26439643 0.5550734 ], train_acc=[0.91665 0.7936 ]
80/100: train_loss=[0.26957023 0.54013467], train_acc=[0.9144 0.8042]
82/100: train_loss=[0.26237231 0.545089  ], train_acc=[0.91715 0.8015 ]
84/100: train_loss=[0.25983056 0.54087746], train_acc=[0.91875 0.80215]
86/100: train_loss=[0.25688452 0.54583025], train_acc=[0.9181 0.7956]
88/100: train_loss=[0.27620423 0.5737745 ], train_acc=[0.9146  0.78505]
90/100: train_loss=[0.2608294 0.5395302], train_acc=[0.91775 0.8035 ]
92/100: train_loss=[0.25540823 0.53824955], train_acc=[0.92095 0.8005 ]
94/100: train_loss=[0.25695592 0.5346653 ], train_acc=[0.91985 0.8071 ]
96/100: train_loss=[0.2610155  0.55052626], train_acc=[0.9192 0.7985]
98/100: train_loss=[0.2559056 0.5285968], train_acc=[0.92025 0.8059 ]
100/100: train_loss=[0.25682598 0.5381395 ], train_acc=[0.9211  0.80085]
**** Time taken for fashion_and_mnist_1 = 2306.3808150291443
loading dataset fashion_and_mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.049024 0.891924], train_acc=[0.6541  0.67655]
2/100: train_loss=[0.8467397  0.80514026], train_acc=[0.7219 0.707 ]
4/100: train_loss=[0.65471596 0.70918727], train_acc=[0.7839 0.7418]
6/100: train_loss=[0.53965265 0.68297255], train_acc=[0.8252  0.74605]
8/100: train_loss=[0.4942697 0.6365336], train_acc=[0.8411  0.76945]
10/100: train_loss=[0.51354754 0.63105947], train_acc=[0.8315  0.76665]
12/100: train_loss=[0.425457   0.61229193], train_acc=[0.8618 0.7725]
14/100: train_loss=[0.40918452 0.59675425], train_acc=[0.86905 0.77675]
16/100: train_loss=[0.37894616 0.5910334 ], train_acc=[0.878  0.7797]
18/100: train_loss=[0.3653532  0.57027805], train_acc=[0.88235 0.79165]
20/100: train_loss=[0.35969427 0.55406255], train_acc=[0.88465 0.79785]
22/100: train_loss=[0.3599173  0.55782557], train_acc=[0.8836 0.795 ]
24/100: train_loss=[0.3370818 0.5438435], train_acc=[0.89375 0.80055]
26/100: train_loss=[0.3179402 0.5521265], train_acc=[0.8968  0.79825]
28/100: train_loss=[0.34364668 0.53723747], train_acc=[0.8868 0.8017]
30/100: train_loss=[0.30620667 0.5284217 ], train_acc=[0.90335 0.8073 ]
32/100: train_loss=[0.29912287 0.52605385], train_acc=[0.90695 0.8075 ]
34/100: train_loss=[0.29604813 0.5205886 ], train_acc=[0.90585 0.80665]
36/100: train_loss=[0.29474774 0.51032686], train_acc=[0.9054  0.81375]
38/100: train_loss=[0.3353891 0.5540924], train_acc=[0.8946 0.7909]
40/100: train_loss=[0.2822868  0.51259416], train_acc=[0.9116 0.8078]
42/100: train_loss=[0.28519553 0.5236946 ], train_acc=[0.9081  0.81215]
44/100: train_loss=[0.2770697 0.5134286], train_acc=[0.91135 0.8156 ]
46/100: train_loss=[0.27091202 0.5007441 ], train_acc=[0.91415 0.8172 ]
48/100: train_loss=[0.3202475 0.505885 ], train_acc=[0.8991  0.81605]
50/100: train_loss=[0.277168  0.4925069], train_acc=[0.9113  0.82015]
52/100: train_loss=[0.26406643 0.4924532 ], train_acc=[0.91655 0.82125]
54/100: train_loss=[0.26813602 0.48743516], train_acc=[0.91605 0.82235]
56/100: train_loss=[0.26289344 0.49387756], train_acc=[0.91825 0.8193 ]
58/100: train_loss=[0.26776183 0.4899073 ], train_acc=[0.91675 0.82085]
60/100: train_loss=[0.26072583 0.4857358 ], train_acc=[0.9199 0.8243]
62/100: train_loss=[0.26316774 0.5127115 ], train_acc=[0.9179  0.81555]
64/100: train_loss=[0.25200295 0.4826059 ], train_acc=[0.9207 0.8246]
66/100: train_loss=[0.2573614  0.50445855], train_acc=[0.9186 0.8114]
68/100: train_loss=[0.24935928 0.4825029 ], train_acc=[0.92165 0.82585]
70/100: train_loss=[0.2558779 0.4968988], train_acc=[0.9196 0.8184]
72/100: train_loss=[0.25268692 0.48550376], train_acc=[0.92145 0.8246 ]
74/100: train_loss=[0.25378808 0.48350537], train_acc=[0.921  0.8257]
76/100: train_loss=[0.24963358 0.4812882 ], train_acc=[0.9225 0.8248]
78/100: train_loss=[0.24662149 0.47887865], train_acc=[0.92385 0.82535]
80/100: train_loss=[0.25065178 0.47925696], train_acc=[0.92165 0.82735]
82/100: train_loss=[0.24539813 0.49129453], train_acc=[0.9235  0.81785]
84/100: train_loss=[0.24953343 0.4733136 ], train_acc=[0.92195 0.82965]
86/100: train_loss=[0.24869365 0.47878215], train_acc=[0.9236 0.8285]
88/100: train_loss=[0.25074214 0.4682954 ], train_acc=[0.92135 0.83295]
90/100: train_loss=[0.24562792 0.47002587], train_acc=[0.92155 0.82855]
92/100: train_loss=[0.24101004 0.46840003], train_acc=[0.92565 0.8314 ]
94/100: train_loss=[0.2512393  0.48363912], train_acc=[0.9209  0.82575]
96/100: train_loss=[0.24488308 0.48175186], train_acc=[0.9241 0.824 ]
98/100: train_loss=[0.24525486 0.46634656], train_acc=[0.924  0.8315]
100/100: train_loss=[0.23656718 0.46833667], train_acc=[0.9285  0.83325]
**** Time taken for fashion_and_mnist_2 = 2309.01695227623
loading dataset fashion_and_mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.0967927  0.97951615], train_acc=[0.63415 0.6366 ]
2/100: train_loss=[0.9156832 0.8523656], train_acc=[0.69225 0.6821 ]
4/100: train_loss=[0.7520235  0.94800234], train_acc=[0.7493  0.64965]
6/100: train_loss=[0.568014  0.7484173], train_acc=[0.81365 0.7246 ]
8/100: train_loss=[0.50521773 0.657622  ], train_acc=[0.8367  0.75855]
10/100: train_loss=[0.5237225  0.64493203], train_acc=[0.8301 0.7639]
12/100: train_loss=[0.43019345 0.6451841 ], train_acc=[0.8623 0.7613]
14/100: train_loss=[0.3923097 0.6041684], train_acc=[0.87515 0.77505]
16/100: train_loss=[0.40037236 0.5994709 ], train_acc=[0.87095 0.7769 ]
18/100: train_loss=[0.3679625  0.58964056], train_acc=[0.8821 0.78  ]
20/100: train_loss=[0.3430527  0.59108794], train_acc=[0.89135 0.7777 ]
22/100: train_loss=[0.34466055 0.5684505 ], train_acc=[0.88945 0.7853 ]
24/100: train_loss=[0.63283366 0.7664206 ], train_acc=[0.7964 0.7249]
26/100: train_loss=[0.30975077 0.5527231 ], train_acc=[0.90295 0.79395]
28/100: train_loss=[0.37602776 0.5755875 ], train_acc=[0.87675 0.786  ]
30/100: train_loss=[0.30154324 0.543726  ], train_acc=[0.90435 0.799  ]
32/100: train_loss=[0.29487467 0.5769395 ], train_acc=[0.90565 0.7774 ]
34/100: train_loss=[0.28943148 0.5386055 ], train_acc=[0.90825 0.80085]
36/100: train_loss=[0.27801523 0.53371197], train_acc=[0.91095 0.80195]
38/100: train_loss=[0.2778766 0.5305961], train_acc=[0.9135 0.8039]
40/100: train_loss=[0.2807388 0.5248466], train_acc=[0.9109 0.8046]
42/100: train_loss=[0.27389395 0.52329   ], train_acc=[0.9149 0.8069]
44/100: train_loss=[0.26683033 0.5122279 ], train_acc=[0.91575 0.81095]
46/100: train_loss=[0.2764355 0.5156601], train_acc=[0.91325 0.80865]
48/100: train_loss=[0.25666833 0.52708495], train_acc=[0.91955 0.8025 ]
50/100: train_loss=[0.26828384 0.56544083], train_acc=[0.91585 0.7804 ]
52/100: train_loss=[0.2627009 0.5195569], train_acc=[0.919   0.80675]
54/100: train_loss=[0.26315016 0.50475883], train_acc=[0.91715 0.81375]
56/100: train_loss=[0.26016077 0.5339924 ], train_acc=[0.91815 0.79535]
58/100: train_loss=[0.24715427 0.5078916 ], train_acc=[0.92345 0.81315]
60/100: train_loss=[0.28725713 0.49675694], train_acc=[0.90885 0.81625]
62/100: train_loss=[0.24295448 0.49085256], train_acc=[0.92485 0.8169 ]
64/100: train_loss=[0.25050876 0.49098718], train_acc=[0.92255 0.81865]
66/100: train_loss=[0.2458875  0.53150207], train_acc=[0.92345 0.80115]
68/100: train_loss=[0.25464717 0.52901673], train_acc=[0.92155 0.8043 ]
70/100: train_loss=[0.2514694 0.4859834], train_acc=[0.9218 0.821 ]
72/100: train_loss=[0.24731681 0.48823413], train_acc=[0.92505 0.81795]
74/100: train_loss=[0.25124422 0.48899022], train_acc=[0.92265 0.81925]
76/100: train_loss=[0.24431838 0.4990977 ], train_acc=[0.92395 0.815  ]
78/100: train_loss=[0.2563047  0.50039035], train_acc=[0.91985 0.81355]
80/100: train_loss=[0.25549042 0.5026504 ], train_acc=[0.92115 0.8149 ]
82/100: train_loss=[0.25638214 0.48827034], train_acc=[0.91895 0.82075]
84/100: train_loss=[0.2748446 0.5192976], train_acc=[0.9153 0.8087]
86/100: train_loss=[0.24264725 0.4898648 ], train_acc=[0.9246  0.81725]
88/100: train_loss=[0.2404849 0.4962249], train_acc=[0.9264 0.817 ]
90/100: train_loss=[0.23338775 0.472765  ], train_acc=[0.9279  0.82375]
92/100: train_loss=[0.23949292 0.4767771 ], train_acc=[0.92655 0.82385]
94/100: train_loss=[0.24266082 0.4759097 ], train_acc=[0.92525 0.82125]
96/100: train_loss=[0.23498745 0.48000738], train_acc=[0.92805 0.82195]
98/100: train_loss=[0.24229434 0.4705392 ], train_acc=[0.9238 0.8238]
100/100: train_loss=[0.2330298  0.46881366], train_acc=[0.92815 0.8259 ]
**** Time taken for fashion_and_mnist_3 = 2312.8420634269714
loading dataset fashion_and_mnist
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.1607323 1.0383288], train_acc=[0.60775 0.61735]
2/100: train_loss=[0.8550991  0.85251415], train_acc=[0.71315 0.68865]
4/100: train_loss=[0.6209991  0.76406765], train_acc=[0.794   0.71345]
6/100: train_loss=[0.5383576  0.72042286], train_acc=[0.8241 0.7346]
8/100: train_loss=[0.4646309 0.6617931], train_acc=[0.84985 0.75185]
10/100: train_loss=[0.4367041 0.6268196], train_acc=[0.8586  0.76565]
12/100: train_loss=[0.39500344 0.6009348 ], train_acc=[0.8718  0.77855]
14/100: train_loss=[0.39071566 0.59626335], train_acc=[0.87375 0.778  ]
16/100: train_loss=[0.37211713 0.57516056], train_acc=[0.8816  0.78745]
18/100: train_loss=[0.373472  0.5871169], train_acc=[0.8805  0.77635]
20/100: train_loss=[0.33651754 0.55488944], train_acc=[0.89465 0.79435]
22/100: train_loss=[0.32294908 0.5432341 ], train_acc=[0.8982  0.79945]
24/100: train_loss=[0.3230086 0.5580243], train_acc=[0.89755 0.7858 ]
26/100: train_loss=[0.3101785  0.52698106], train_acc=[0.9025  0.80465]
28/100: train_loss=[0.3190997 0.5344805], train_acc=[0.90105 0.80415]
30/100: train_loss=[0.2981714  0.52643067], train_acc=[0.9071 0.8048]
32/100: train_loss=[0.30386212 0.5150321 ], train_acc=[0.9045 0.8082]
34/100: train_loss=[0.2909087 0.5219469], train_acc=[0.9084 0.8082]
36/100: train_loss=[0.29826626 0.50363207], train_acc=[0.90605 0.81455]
38/100: train_loss=[0.28768438 0.5066666 ], train_acc=[0.91045 0.81065]
40/100: train_loss=[0.28366137 0.50329995], train_acc=[0.9104  0.81235]
42/100: train_loss=[0.28824344 0.50133914], train_acc=[0.9079 0.8109]
44/100: train_loss=[0.28007936 0.48766348], train_acc=[0.9126  0.81715]
46/100: train_loss=[0.320255 0.493651], train_acc=[0.898  0.8146]
48/100: train_loss=[0.2690411 0.4792948], train_acc=[0.9168  0.82245]
50/100: train_loss=[0.27317336 0.50657594], train_acc=[0.9138 0.8098]
52/100: train_loss=[0.28327808 0.51204956], train_acc=[0.9133 0.8091]
54/100: train_loss=[0.2736618  0.49192637], train_acc=[0.91415 0.8217 ]
56/100: train_loss=[0.2769396 0.4980425], train_acc=[0.91305 0.81685]
58/100: train_loss=[0.2781158 0.4792218], train_acc=[0.91375 0.82055]
60/100: train_loss=[0.28135604 0.47405794], train_acc=[0.91145 0.8247 ]
62/100: train_loss=[0.28539446 0.48979816], train_acc=[0.91105 0.81725]
64/100: train_loss=[0.2816028  0.48006687], train_acc=[0.91195 0.82   ]
66/100: train_loss=[0.25868976 0.47154656], train_acc=[0.9193 0.8254]
68/100: train_loss=[0.25864717 0.4742917 ], train_acc=[0.9184 0.8233]
70/100: train_loss=[0.27383572 0.48727292], train_acc=[0.9167 0.8211]
72/100: train_loss=[0.25467768 0.48610684], train_acc=[0.92215 0.81915]
74/100: train_loss=[0.25855416 0.46159875], train_acc=[0.91915 0.83045]
76/100: train_loss=[0.25532475 0.45668206], train_acc=[0.92345 0.83315]
78/100: train_loss=[0.250497   0.47044885], train_acc=[0.9234  0.82975]
80/100: train_loss=[0.25192988 0.46192393], train_acc=[0.92185 0.8318 ]
82/100: train_loss=[0.25422487 0.47053617], train_acc=[0.9219 0.8261]
84/100: train_loss=[0.2463911 0.4685928], train_acc=[0.92485 0.8275 ]
86/100: train_loss=[0.25552294 0.47353625], train_acc=[0.9208  0.82605]
88/100: train_loss=[0.25390872 0.45938507], train_acc=[0.92035 0.8312 ]
90/100: train_loss=[0.28102106 0.48083845], train_acc=[0.9138  0.82745]
92/100: train_loss=[0.26491633 0.46302414], train_acc=[0.91755 0.82845]
94/100: train_loss=[0.23999424 0.4573445 ], train_acc=[0.92655 0.833  ]
96/100: train_loss=[0.27339855 0.47463763], train_acc=[0.91625 0.82795]
98/100: train_loss=[0.288072   0.50045234], train_acc=[0.9124  0.82025]
100/100: train_loss=[0.24549446 0.45610604], train_acc=[0.92575 0.8342 ]
**** Time taken for fashion_and_mnist_4 = 2296.6198563575745
**** Time taken for fashion_and_mnist = 11542.030928611755
