==>>> total trainning batch number: 469
==>>> total testing batch number: 79
Preference Vector (4/5):
[0.382756   0.92384946]
fealsible solution is obtained.
1/100: weights=[ 0.99499184 -0.00500816], train_loss=[1.1220326 1.6082925], train_acc=[0.5906  0.42945]
2/100: weights=[0.7350951  0.26490492], train_loss=[0.9479102 1.35178  ], train_acc=[0.6474  0.51575]
4/100: weights=[0.7128935 0.2871065], train_loss=[0.8317111 1.167843 ], train_acc=[0.69365 0.58265]
6/100: weights=[0.48422647 0.51577353], train_loss=[0.76934105 1.0630548 ], train_acc=[0.71155 0.60865]
8/100: weights=[0.6175233 0.3824767], train_loss=[0.7528953 1.0082232], train_acc=[0.7177  0.62565]
10/100: weights=[0.38829258 0.6117074 ], train_loss=[0.70867    0.95907414], train_acc=[0.7398  0.64855]
12/100: weights=[0.73997456 0.26002547], train_loss=[0.68581694 0.909008  ], train_acc=[0.74875 0.66465]
14/100: weights=[0.68726134 0.31273857], train_loss=[0.6709517  0.87990224], train_acc=[0.75095 0.6763 ]
16/100: weights=[0.8091154  0.19088465], train_loss=[0.6598256 0.8584194], train_acc=[0.75765 0.6837 ]
18/100: weights=[0.7031417  0.29685834], train_loss=[0.6539096 0.8400999], train_acc=[0.75525 0.69125]
20/100: weights=[0.86496717 0.1350328 ], train_loss=[0.6421477  0.82603174], train_acc=[0.7594  0.69535]
22/100: weights=[0.66892564 0.33107442], train_loss=[0.63281214 0.80559206], train_acc=[0.762 0.703]
24/100: weights=[0.98611873 0.01388133], train_loss=[0.62822604 0.79355973], train_acc=[0.76805 0.7084 ]
26/100: weights=[0.83983743 0.1601625 ], train_loss=[0.6127711  0.78150433], train_acc=[0.77465 0.71425]
28/100: weights=[0.79923743 0.20076256], train_loss=[0.6311176 0.7751377], train_acc=[0.76815 0.71485]
30/100: weights=[0.5653979  0.43460208], train_loss=[0.6057797  0.76841223], train_acc=[0.77565 0.71505]
32/100: weights=[0.6333176  0.36668238], train_loss=[0.5888072 0.7491881], train_acc=[0.7809 0.7223]
34/100: weights=[0.658516   0.34148404], train_loss=[0.587849  0.7409108], train_acc=[0.7816  0.72465]
36/100: weights=[0.7603412 0.2396587], train_loss=[0.5848228 0.7327928], train_acc=[0.7844 0.7299]
38/100: weights=[0.79414135 0.20585866], train_loss=[0.5859962  0.73546785], train_acc=[0.785  0.7283]
40/100: weights=[0.68871987 0.31128016], train_loss=[0.5712253 0.7214036], train_acc=[0.7903 0.7348]
42/100: weights=[0.67366606 0.32633394], train_loss=[0.5770891 0.7167206], train_acc=[0.7862  0.73305]
44/100: weights=[0.98414034 0.01585971], train_loss=[0.5774824  0.71023256], train_acc=[0.7903  0.73605]
46/100: weights=[ 0.91264826 -0.08735175], train_loss=[0.5705717  0.70351815], train_acc=[0.79135 0.73925]
48/100: weights=[0.8488816  0.15111835], train_loss=[0.57362235 0.69817764], train_acc=[0.7858  0.74355]
50/100: weights=[0.8308     0.16919999], train_loss=[0.5793459  0.69334733], train_acc=[0.7857 0.742 ]
52/100: weights=[0.75858796 0.24141203], train_loss=[0.5672882 0.6880305], train_acc=[0.7955  0.74545]
54/100: weights=[0.48140863 0.51859134], train_loss=[0.56233644 0.68753177], train_acc=[0.79085 0.74515]
56/100: weights=[0.68712544 0.3128745 ], train_loss=[0.55448514 0.6803553 ], train_acc=[0.7965  0.74895]
58/100: weights=[0.5326424  0.46735758], train_loss=[0.5514907  0.67819405], train_acc=[0.80015 0.747  ]
60/100: weights=[0.46821478 0.53178525], train_loss=[0.5418744  0.67221624], train_acc=[0.8042  0.75435]
62/100: weights=[0.83278334 0.16721663], train_loss=[0.5512268  0.67001575], train_acc=[0.79945 0.7518 ]
64/100: weights=[0.94714904 0.05285095], train_loss=[0.54339397 0.6630775 ], train_acc=[0.80285 0.7561 ]
66/100: weights=[0.49373522 0.50626475], train_loss=[0.54510593 0.6679187 ], train_acc=[0.8013  0.75415]
68/100: weights=[0.7971129  0.20288709], train_loss=[0.5446299  0.65828365], train_acc=[0.80405 0.75805]
70/100: weights=[0.66722196 0.33277795], train_loss=[0.5333831 0.6592451], train_acc=[0.8065  0.75755]
72/100: weights=[0.75211614 0.24788386], train_loss=[0.54279613 0.6529316 ], train_acc=[0.8021 0.7607]
74/100: weights=[0.85008496 0.14991504], train_loss=[0.53867894 0.65106505], train_acc=[0.8071  0.76205]
76/100: weights=[0.739697  0.2603031], train_loss=[0.5304444  0.64710206], train_acc=[0.80855 0.76385]
78/100: weights=[0.8371737  0.16282628], train_loss=[0.53476477 0.6444753 ], train_acc=[0.8072  0.76395]
80/100: weights=[0.8684869  0.13151321], train_loss=[0.53164285 0.6443589 ], train_acc=[0.80615 0.7625 ]
82/100: weights=[0.71793544 0.28206453], train_loss=[0.537223   0.64360934], train_acc=[0.80355 0.7677 ]
84/100: weights=[0.5109768  0.48902318], train_loss=[0.52322507 0.64050186], train_acc=[0.8098 0.7631]
86/100: weights=[0.62753737 0.3724626 ], train_loss=[0.5333521  0.63333905], train_acc=[0.8082 0.7679]
88/100: weights=[0.7778299  0.22217005], train_loss=[0.5301612 0.6319627], train_acc=[0.806  0.7688]
90/100: weights=[0.52829844 0.47170156], train_loss=[0.52160794 0.64138764], train_acc=[0.8128 0.7704]
92/100: weights=[0.6291856  0.37081438], train_loss=[0.5222988  0.63140094], train_acc=[0.81015 0.7681 ]
94/100: weights=[0.56440926 0.4355907 ], train_loss=[0.5354123  0.62830263], train_acc=[0.80585 0.77   ]
96/100: weights=[0.44065955 0.5593404 ], train_loss=[0.5306873 0.6301915], train_acc=[0.807   0.77125]
98/100: weights=[0.71950084 0.2804992 ], train_loss=[0.52382445 0.6282905 ], train_acc=[0.81085 0.77085]
100/100: weights=[0.7635504  0.23644952], train_loss=[0.5176221 0.6233962], train_acc=[0.8131 0.7725]
**** Time taken for fashion_3 = 902.395101070404
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
Preference Vector (5/5):
[1.5707963e-04 1.0000000e+00]
fealsible solution is obtained.
1/100: weights=[0.869387   0.13061304], train_loss=[1.0603681 2.4872057], train_acc=[0.61725 0.20695]
2/100: weights=[ 0.8871491  -0.11285093], train_loss=[0.9580113 2.016388 ], train_acc=[0.6456  0.30275]
4/100: weights=[0.91084427 0.08915579], train_loss=[0.8276806 1.5845499], train_acc=[0.6922 0.4304]
6/100: weights=[0.61570686 0.3842932 ], train_loss=[0.76226586 1.3576171 ], train_acc=[0.71965 0.5128 ]
8/100: weights=[0.97744423 0.02255575], train_loss=[0.7325523 1.217013 ], train_acc=[0.728   0.55715]
10/100: weights=[0.7924102  0.20758976], train_loss=[0.7115943 1.1232815], train_acc=[0.73465 0.5922 ]
12/100: weights=[0.8618198  0.13818018], train_loss=[0.6922454 1.0719943], train_acc=[0.7419 0.6125]
14/100: weights=[0.69918525 0.30081475], train_loss=[0.6679885 1.0240588], train_acc=[0.7543 0.626 ]
16/100: weights=[0.8551151  0.14488488], train_loss=[0.6533998  0.97792673], train_acc=[0.7611 0.6454]
18/100: weights=[0.95857936 0.04142063], train_loss=[0.6409622 0.9451968], train_acc=[0.76515 0.65665]
20/100: weights=[0.76918125 0.23081875], train_loss=[0.63704413 0.9370304 ], train_acc=[0.7645 0.6586]
22/100: weights=[0.8400292  0.15997078], train_loss=[0.677983  0.9113361], train_acc=[0.7426  0.66855]
24/100: weights=[0.7670433  0.23295675], train_loss=[0.62130207 0.88479143], train_acc=[0.7718  0.67765]
26/100: weights=[0.94314873 0.05685122], train_loss=[0.62215674 0.87126297], train_acc=[0.77055 0.6767 ]
28/100: weights=[0.76186764 0.23813237], train_loss=[0.59728456 0.8522932 ], train_acc=[0.78075 0.68615]
30/100: weights=[0.78892624 0.21107379], train_loss=[0.6064833 0.8420953], train_acc=[0.77915 0.68885]
32/100: weights=[0.94183445 0.05816552], train_loss=[0.600386   0.83720803], train_acc=[0.77935 0.695  ]
34/100: weights=[0.84385055 0.15614948], train_loss=[0.59019214 0.8229744 ], train_acc=[0.78215 0.69795]
36/100: weights=[0.7600795 0.2399205], train_loss=[0.57722723 0.8113782 ], train_acc=[0.7901 0.7021]
38/100: weights=[0.9268289  0.07317111], train_loss=[0.5795817 0.8167608], train_acc=[0.79105 0.7026 ]
40/100: weights=[0.8565561  0.14344384], train_loss=[0.5901627  0.80263907], train_acc=[0.7825  0.70445]
42/100: weights=[0.8956403  0.10435966], train_loss=[0.5743774  0.78617495], train_acc=[0.7929 0.7111]
44/100: weights=[0.9947146  0.00528536], train_loss=[0.57248205 0.78007793], train_acc=[0.7908 0.7118]
46/100: weights=[0.8728976  0.12710238], train_loss=[0.5608095 0.7755644], train_acc=[0.797   0.71515]
48/100: weights=[0.7290233 0.2709767], train_loss=[0.5764793 0.7685992], train_acc=[0.7884 0.7175]
50/100: weights=[0.7261917  0.27380833], train_loss=[0.5580632 0.7624659], train_acc=[0.80015 0.71995]
52/100: weights=[0.74464643 0.25535357], train_loss=[0.55203694 0.7571767 ], train_acc=[0.79945 0.7203 ]
54/100: weights=[0.65253353 0.3474665 ], train_loss=[0.5488069 0.7529591], train_acc=[0.80095 0.7251 ]
56/100: weights=[0.8151832  0.18481682], train_loss=[0.55024916 0.7560058 ], train_acc=[0.8002 0.7188]
58/100: weights=[0.7926859  0.20731404], train_loss=[0.5552639 0.74838  ], train_acc=[0.7966  0.72435]
60/100: weights=[0.92768884 0.0723112 ], train_loss=[0.545329  0.7491298], train_acc=[0.8049 0.7248]
62/100: weights=[0.95131433 0.0486857 ], train_loss=[0.538685   0.73614883], train_acc=[0.80595 0.7293 ]
64/100: weights=[0.9526502  0.04734983], train_loss=[0.5402265  0.73863405], train_acc=[0.80795 0.72745]
66/100: weights=[0.5630012  0.43699878], train_loss=[0.54300606 0.73918784], train_acc=[0.8027  0.72755]
68/100: weights=[0.691171   0.30882895], train_loss=[0.53833944 0.74210453], train_acc=[0.8045 0.7229]
70/100: weights=[0.6543243  0.34567574], train_loss=[0.5463305 0.7290368], train_acc=[0.8016 0.7304]
72/100: weights=[0.697358 0.302642], train_loss=[0.5450103 0.7226336], train_acc=[0.8053 0.7336]
74/100: weights=[0.7776364  0.22236359], train_loss=[0.53787273 0.7190548 ], train_acc=[0.80765 0.7359 ]
76/100: weights=[0.8648386  0.13516146], train_loss=[0.5356183  0.72480744], train_acc=[0.8076  0.73495]
78/100: weights=[0.81477064 0.18522936], train_loss=[0.55236953 0.71839094], train_acc=[0.80265 0.73715]
80/100: weights=[0.83788234 0.16211765], train_loss=[0.5416336  0.70888853], train_acc=[0.80685 0.7394 ]
82/100: weights=[0.8730113  0.12698875], train_loss=[0.5311157  0.70740396], train_acc=[0.81065 0.74185]
84/100: weights=[ 0.96500987 -0.03499015], train_loss=[0.5430785  0.71204966], train_acc=[0.8059 0.7392]
86/100: weights=[0.7772408  0.22275922], train_loss=[0.5264603  0.70693237], train_acc=[0.8104  0.73695]
88/100: weights=[0.8757065  0.12429353], train_loss=[0.5315173  0.70043224], train_acc=[0.8128  0.74125]
90/100: weights=[0.90279126 0.09720873], train_loss=[0.5254821  0.70485854], train_acc=[0.8123  0.73925]
92/100: weights=[0.8221464  0.17785355], train_loss=[0.54689276 0.70437294], train_acc=[0.8027  0.73615]
94/100: weights=[0.80377966 0.19622035], train_loss=[0.5360842  0.70344394], train_acc=[0.8095  0.74125]
96/100: weights=[0.7030596  0.29694036], train_loss=[0.5235402 0.6953313], train_acc=[0.8143 0.743 ]
98/100: weights=[0.8562962  0.14370385], train_loss=[0.5281244 0.6970599], train_acc=[0.8112 0.7419]
100/100: weights=[0.7774143 0.2225857], train_loss=[0.51439   0.6935498], train_acc=[0.81685 0.74535]
**** Time taken for fashion_4 = 935.6255657672882
**** Time taken for fashion = 1838.0352551937103
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
Preference Vector (1/5):
[1.0000000e+00 1.5707963e-04]
fealsible solution is obtained.
1/100: weights=[0.0906344  0.90936565], train_loss=[2.779328  0.8763088], train_acc=[0.1456 0.685 ]
2/100: weights=[-0.02968439  0.97031564], train_loss=[2.2037773  0.77452594], train_acc=[0.2501 0.714 ]
4/100: weights=[0.3848319  0.61516804], train_loss=[1.8035246 0.6777064], train_acc=[0.37205 0.7469 ]
6/100: weights=[-0.08460514  0.9153949 ], train_loss=[1.5708709  0.64241767], train_acc=[0.45365 0.7594 ]
8/100: weights=[0.03002108 0.9699789 ], train_loss=[1.3873307 0.6116884], train_acc=[0.5252 0.7733]
10/100: weights=[0.12955663 0.87044334], train_loss=[1.2668624 0.5923129], train_acc=[0.56895 0.781  ]
12/100: weights=[0.3625288  0.63747126], train_loss=[1.1793776 0.5838057], train_acc=[0.6016  0.78335]
14/100: weights=[0.16618018 0.83381975], train_loss=[1.1063256 0.5787271], train_acc=[0.63    0.78575]
16/100: weights=[0.01648961 0.98351043], train_loss=[1.0425823  0.56109285], train_acc=[0.65085 0.7892 ]
18/100: weights=[0.13044944 0.8695506 ], train_loss=[0.9935726 0.5399055], train_acc=[0.6689 0.7995]
20/100: weights=[0.14799052 0.8520095 ], train_loss=[0.9580266  0.56387806], train_acc=[0.6822  0.79095]
22/100: weights=[0.13198787 0.86801213], train_loss=[0.91059035 0.52505   ], train_acc=[0.70065 0.8059 ]
24/100: weights=[0.2183268 0.7816732], train_loss=[0.89633965 0.52169734], train_acc=[0.70385 0.8072 ]
26/100: weights=[0.29083678 0.70916325], train_loss=[0.8496465 0.5208802], train_acc=[0.72395 0.81045]
28/100: weights=[0.11617644 0.8838235 ], train_loss=[0.81966805 0.51922685], train_acc=[0.73085 0.81055]
30/100: weights=[0.03666486 0.96333516], train_loss=[0.78912556 0.50340146], train_acc=[0.74155 0.81615]
32/100: weights=[0.03389868 0.96610135], train_loss=[0.76221514 0.4995532 ], train_acc=[0.7504  0.81755]
34/100: weights=[0.18163183 0.8183682 ], train_loss=[0.75685054 0.51308256], train_acc=[0.74985 0.8124 ]
36/100: weights=[0.12892653 0.8710735 ], train_loss=[0.7342725 0.4871863], train_acc=[0.75995 0.8235 ]
38/100: weights=[0.14757834 0.8524216 ], train_loss=[0.726759   0.48975968], train_acc=[0.7627  0.82145]
40/100: weights=[0.16664705 0.83335286], train_loss=[0.70703864 0.49439183], train_acc=[0.76635 0.82055]
42/100: weights=[0.3136471  0.68635297], train_loss=[0.6940529  0.48297188], train_acc=[0.7703 0.8241]
44/100: weights=[0.31632692 0.6836731 ], train_loss=[0.7001906  0.47282788], train_acc=[0.7691  0.82655]
46/100: weights=[0.28876033 0.71123964], train_loss=[0.6687549  0.47353306], train_acc=[0.781   0.82455]
48/100: weights=[0.08147755 0.9185225 ], train_loss=[0.65383196 0.48448694], train_acc=[0.784   0.82315]
50/100: weights=[0.22503924 0.7749607 ], train_loss=[0.6660456 0.480173 ], train_acc=[0.78165 0.82375]
52/100: weights=[0.03284008 0.9671599 ], train_loss=[0.63726187 0.46009204], train_acc=[0.7906 0.8319]
54/100: weights=[0.12479252 0.8752075 ], train_loss=[0.63651204 0.45816255], train_acc=[0.7922 0.8332]
56/100: weights=[0.19522874 0.8047713 ], train_loss=[0.6256078 0.4513122], train_acc=[0.79375 0.83675]
58/100: weights=[0.07980018 0.9201998 ], train_loss=[0.6211743  0.45252368], train_acc=[0.7959  0.83665]
60/100: weights=[0.08432931 0.9156707 ], train_loss=[0.6127234  0.45112357], train_acc=[0.7996  0.83575]
62/100: weights=[0.05700948 0.94299054], train_loss=[0.6037877  0.44510767], train_acc=[0.801   0.83665]
64/100: weights=[0.09295661 0.90704334], train_loss=[0.60942316 0.4607061 ], train_acc=[0.79855 0.8357 ]
66/100: weights=[0.08804467 0.91195536], train_loss=[0.59545255 0.44930217], train_acc=[0.804  0.8359]
68/100: weights=[0.06253882 0.93746126], train_loss=[0.5998428  0.43853596], train_acc=[0.80245 0.8417 ]
70/100: weights=[0.13341095 0.86658907], train_loss=[0.5874837 0.4495421], train_acc=[0.8072  0.83965]
72/100: weights=[0.22802177 0.77197826], train_loss=[0.5813859  0.44462124], train_acc=[0.80965 0.83845]
74/100: weights=[0.2800542 0.7199458], train_loss=[0.5875567  0.44261596], train_acc=[0.8055 0.8381]
76/100: weights=[0.09987523 0.9001248 ], train_loss=[0.5768521  0.46639243], train_acc=[0.80975 0.83   ]
78/100: weights=[0.26675135 0.7332486 ], train_loss=[0.57438093 0.45280042], train_acc=[0.81135 0.83755]
80/100: weights=[0.1079352  0.89206487], train_loss=[0.5721897 0.4337735], train_acc=[0.8104  0.84355]
82/100: weights=[0.14437748 0.8556225 ], train_loss=[0.5616007 0.4295535], train_acc=[0.81485 0.84465]
84/100: weights=[0.10909801 0.89090204], train_loss=[0.58636814 0.4764049 ], train_acc=[0.8085 0.8248]
86/100: weights=[0.23742595 0.7625741 ], train_loss=[0.5623448 0.4258705], train_acc=[0.81615 0.8473 ]
88/100: weights=[0.09777816 0.90222174], train_loss=[0.5720472 0.4291209], train_acc=[0.812  0.8467]
90/100: weights=[0.30806896 0.6919311 ], train_loss=[0.5513666  0.42575857], train_acc=[0.81925 0.8487 ]
92/100: weights=[0.1828419  0.81715816], train_loss=[0.5591894  0.43039834], train_acc=[0.81465 0.84645]
94/100: weights=[0.14429997 0.85569996], train_loss=[0.5458863 0.4415041], train_acc=[0.82005 0.8433 ]
96/100: weights=[0.16112745 0.83887255], train_loss=[0.54844385 0.42204016], train_acc=[0.8198 0.8494]
98/100: weights=[0.02070344 0.97929657], train_loss=[0.5530548  0.42752835], train_acc=[0.81795 0.84785]
100/100: weights=[0.07873455 0.9212655 ], train_loss=[0.53366286 0.42887145], train_acc=[0.82545 0.84785]
**** Time taken for fashion_and_mnist_0 = 929.8935925960541
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
Preference Vector (2/5):
[0.92384946 0.382756  ]
fealsible solution is obtained.
1/100: weights=[0.19577776 0.8042222 ], train_loss=[1.7822886 1.134306 ], train_acc=[0.3863 0.5756]
2/100: weights=[0.46913126 0.53086877], train_loss=[1.3625597  0.91203076], train_acc=[0.53865 0.66125]
4/100: weights=[0.63432556 0.36567444], train_loss=[1.1291025 0.7870699], train_acc=[0.6197 0.7124]
6/100: weights=[0.5554598  0.44454023], train_loss=[0.9751747  0.73412603], train_acc=[0.6684 0.732 ]
8/100: weights=[0.1095674  0.89043254], train_loss=[0.9111057  0.71311545], train_acc=[0.6921 0.7368]
10/100: weights=[0.24362633 0.7563736 ], train_loss=[0.86234903 0.67233986], train_acc=[0.71125 0.75385]
12/100: weights=[0.08857049 0.9114295 ], train_loss=[0.83676237 0.64873934], train_acc=[0.7203  0.75805]
14/100: weights=[-0.02257549  0.9774245 ], train_loss=[0.8012982  0.65043086], train_acc=[0.73155 0.7584 ]
16/100: weights=[0.13834247 0.8616576 ], train_loss=[0.7698522 0.6243638], train_acc=[0.7433 0.7694]
18/100: weights=[0.4806722  0.51932776], train_loss=[0.7816888 0.607428 ], train_acc=[0.73695 0.7722 ]
20/100: weights=[0.0961427  0.90385735], train_loss=[0.73238045 0.6112822 ], train_acc=[0.75515 0.7716 ]
22/100: weights=[0.19101234 0.8089876 ], train_loss=[0.7211756  0.62148315], train_acc=[0.7617  0.76915]
24/100: weights=[-0.01999574  0.9800042 ], train_loss=[0.7168138 0.5745826], train_acc=[0.76155 0.7899 ]
26/100: weights=[0.13384072 0.86615926], train_loss=[0.69876826 0.5917345 ], train_acc=[0.7691  0.77675]
28/100: weights=[0.24955879 0.75044125], train_loss=[0.69120735 0.56040686], train_acc=[0.77165 0.7942 ]
30/100: weights=[0.05846219 0.94153786], train_loss=[0.68542856 0.5798437 ], train_acc=[0.7731 0.7875]
32/100: weights=[0.14078087 0.85921913], train_loss=[0.6718546 0.5478156], train_acc=[0.77885 0.7996 ]
34/100: weights=[0.17840858 0.82159144], train_loss=[0.67352974 0.55104494], train_acc=[0.78   0.7975]
36/100: weights=[0.01982096 0.9801791 ], train_loss=[0.66143966 0.64668506], train_acc=[0.78265 0.75315]
38/100: weights=[0.12053517 0.87946475], train_loss=[0.6457997 0.5292911], train_acc=[0.78655 0.8067 ]
40/100: weights=[0.19625722 0.8037429 ], train_loss=[0.6433247  0.53097314], train_acc=[0.78845 0.8046 ]
42/100: weights=[0.42613193 0.57386804], train_loss=[0.64680743 0.5309068 ], train_acc=[0.78685 0.8037 ]
44/100: weights=[0.14891528 0.8510847 ], train_loss=[0.64491767 0.53101134], train_acc=[0.78825 0.8036 ]
46/100: weights=[0.09903862 0.9009614 ], train_loss=[0.6223196  0.52954906], train_acc=[0.7951  0.80385]
48/100: weights=[0.27347466 0.7265253 ], train_loss=[0.623078   0.51199347], train_acc=[0.7939 0.8141]
50/100: weights=[-0.08561422  0.9143858 ], train_loss=[0.6193216  0.51793945], train_acc=[0.7938  0.80865]
52/100: weights=[0.01701468 0.98298526], train_loss=[0.60531616 0.5248152 ], train_acc=[0.79695 0.8043 ]
54/100: weights=[0.1872256 0.8127744], train_loss=[0.6031275  0.51426995], train_acc=[0.8     0.81355]
56/100: weights=[0.2696     0.73039997], train_loss=[0.60866565 0.5022637 ], train_acc=[0.79745 0.81735]
58/100: weights=[0.06440657 0.9355935 ], train_loss=[0.5976135 0.5104268], train_acc=[0.8017  0.81685]
60/100: weights=[0.2021804 0.7978196], train_loss=[0.59017557 0.50637454], train_acc=[0.8035  0.81385]
62/100: weights=[0.2754187 0.7245813], train_loss=[0.58853334 0.49822575], train_acc=[0.80585 0.8183 ]
64/100: weights=[0.15676713 0.8432329 ], train_loss=[0.5725318  0.49321535], train_acc=[0.8102  0.81975]
66/100: weights=[0.04519339 0.9548066 ], train_loss=[0.5716988 0.4950429], train_acc=[0.8101  0.82035]
68/100: weights=[0.09721032 0.90278965], train_loss=[0.5678561 0.4879139], train_acc=[0.81135 0.8237 ]
70/100: weights=[0.17884018 0.8211598 ], train_loss=[0.56971484 0.510429  ], train_acc=[0.8115 0.8132]
72/100: weights=[0.05102078 0.9489792 ], train_loss=[0.560099  0.5025293], train_acc=[0.8149 0.8193]
74/100: weights=[0.12216033 0.8778397 ], train_loss=[0.557475   0.50807977], train_acc=[0.81425 0.81845]
76/100: weights=[0.4617109  0.53828907], train_loss=[0.6085006 0.4852326], train_acc=[0.7987  0.82425]
78/100: weights=[0.08543068 0.91456926], train_loss=[0.5468557  0.50100213], train_acc=[0.82045 0.81505]
80/100: weights=[0.25874102 0.741259  ], train_loss=[0.5487786  0.49408978], train_acc=[0.8175 0.822 ]
82/100: weights=[0.19908139 0.80091864], train_loss=[0.54953283 0.4749349 ], train_acc=[0.81795 0.82925]
84/100: weights=[0.43965265 0.5603473 ], train_loss=[0.5424438  0.46771723], train_acc=[0.8192 0.8298]
86/100: weights=[0.12293422 0.8770658 ], train_loss=[0.5399772  0.48047167], train_acc=[0.82185 0.82545]
88/100: weights=[0.19408016 0.8059198 ], train_loss=[0.54358625 0.48814568], train_acc=[0.81995 0.824  ]
90/100: weights=[0.2937695  0.70623046], train_loss=[0.54779845 0.4773746 ], train_acc=[0.81905 0.8262 ]
92/100: weights=[0.08653433 0.9134656 ], train_loss=[0.5266273  0.46727204], train_acc=[0.82625 0.83085]
94/100: weights=[0.08937196 0.910628  ], train_loss=[0.5212567  0.46842137], train_acc=[0.8288 0.8315]
96/100: weights=[0.11131272 0.88868725], train_loss=[0.5189778 0.4861224], train_acc=[0.8285 0.8259]
98/100: weights=[0.18734607 0.8126539 ], train_loss=[0.5226962  0.47749507], train_acc=[0.82645 0.82405]
100/100: weights=[0.00879754 0.9912024 ], train_loss=[0.5187006  0.46218693], train_acc=[0.8302 0.8326]
**** Time taken for fashion_and_mnist_1 = 893.2700793743134
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
Preference Vector (3/5):
[0.70710677 0.70710677]
fealsible solution is obtained.
1/100: weights=[0.5041339 0.4958661], train_loss=[1.4568089 1.383739 ], train_acc=[0.5087 0.5069]
2/100: weights=[0.59619886 0.40380114], train_loss=[1.1543221 1.126717 ], train_acc=[0.612   0.60065]
4/100: weights=[0.6767521 0.3232479], train_loss=[0.82433033 0.8493045 ], train_acc=[0.7301 0.6923]
6/100: weights=[0.3405128 0.6594872], train_loss=[0.70163244 0.78099895], train_acc=[0.76925 0.71495]
8/100: weights=[0.5376825 0.4623175], train_loss=[0.6326926  0.74106824], train_acc=[0.79375 0.7293 ]
10/100: weights=[0.5300644 0.4699356], train_loss=[0.5901282  0.70603347], train_acc=[0.8061  0.74375]
12/100: weights=[0.37619004 0.62380993], train_loss=[0.5598164  0.68373483], train_acc=[0.8167  0.75035]
14/100: weights=[0.5939074 0.4060926], train_loss=[0.5418635 0.6679228], train_acc=[0.82115 0.7546 ]
16/100: weights=[0.16591062 0.8340894 ], train_loss=[0.5122015  0.64645994], train_acc=[0.83275 0.76165]
18/100: weights=[0.42952588 0.5704741 ], train_loss=[0.480563 0.625132], train_acc=[0.8437  0.77365]
20/100: weights=[0.21946652 0.7805335 ], train_loss=[0.4633143 0.6245293], train_acc=[0.8488  0.77315]
22/100: weights=[0.26996186 0.7300381 ], train_loss=[0.44656557 0.61410415], train_acc=[0.8551 0.776 ]
24/100: weights=[-2.4867587e-04  9.9975127e-01], train_loss=[0.44354245 0.63976884], train_acc=[0.85425 0.75835]
26/100: weights=[0.37026182 0.62973815], train_loss=[0.43528295 0.59398365], train_acc=[0.8587  0.78185]
28/100: weights=[0.26903835 0.7309617 ], train_loss=[0.4244804 0.6102917], train_acc=[0.86275 0.7703 ]
30/100: weights=[0.13952912 0.8604709 ], train_loss=[0.42089728 0.57161766], train_acc=[0.8638 0.7911]
32/100: weights=[0.06280173 0.9371983 ], train_loss=[0.40865287 0.5678123 ], train_acc=[0.86795 0.7911 ]
34/100: weights=[0.18657668 0.81342334], train_loss=[0.39581284 0.55760926], train_acc=[0.87315 0.79535]
36/100: weights=[0.1056141  0.89438593], train_loss=[0.3984871  0.58985287], train_acc=[0.87115 0.7874 ]
38/100: weights=[0.5567704  0.44322962], train_loss=[0.38987145 0.5492052 ], train_acc=[0.87375 0.8005 ]
40/100: weights=[0.27643657 0.72356343], train_loss=[0.38222566 0.5485416 ], train_acc=[0.8774  0.79775]
42/100: weights=[0.41459012 0.5854099 ], train_loss=[0.3769738 0.5510053], train_acc=[0.87805 0.79885]
44/100: weights=[0.2880277 0.7119723], train_loss=[0.3739517 0.5303348], train_acc=[0.8781 0.805 ]
46/100: weights=[0.4951965  0.50480354], train_loss=[0.39801243 0.5397568 ], train_acc=[0.86905 0.79845]
48/100: weights=[0.36413118 0.6358688 ], train_loss=[0.3653764  0.53317684], train_acc=[0.881 0.802]
50/100: weights=[0.2029623 0.7970377], train_loss=[0.3585066  0.53529423], train_acc=[0.88415 0.80425]
52/100: weights=[0.5017911 0.4982089], train_loss=[0.3579779  0.52014333], train_acc=[0.8832  0.81005]
54/100: weights=[0.1307814 0.8692186], train_loss=[0.35327476 0.5510721 ], train_acc=[0.8857 0.8004]
56/100: weights=[0.43818986 0.56181014], train_loss=[0.37709904 0.52812004], train_acc=[0.87805 0.80575]
58/100: weights=[0.471055   0.52894497], train_loss=[0.35567987 0.5154763 ], train_acc=[0.88615 0.8124 ]
60/100: weights=[0.3401355 0.6598645], train_loss=[0.3540181  0.51806724], train_acc=[0.8843  0.81135]
62/100: weights=[0.11577865 0.8842214 ], train_loss=[0.33974752 0.51361406], train_acc=[0.8895  0.80925]
64/100: weights=[0.19621491 0.8037851 ], train_loss=[0.34512812 0.5025176 ], train_acc=[0.88875 0.8162 ]
66/100: weights=[0.768705 0.231295], train_loss=[0.35788128 0.50881857], train_acc=[0.8826 0.8114]
68/100: weights=[0.05061513 0.94938487], train_loss=[0.34289822 0.53574085], train_acc=[0.88725 0.8046 ]
70/100: weights=[0.5763371  0.42366287], train_loss=[0.33589315 0.49589336], train_acc=[0.8899 0.8197]
72/100: weights=[0.18533231 0.8146677 ], train_loss=[0.3406613  0.50209856], train_acc=[0.88895 0.81825]
74/100: weights=[0.23559238 0.76440763], train_loss=[0.32715794 0.50464475], train_acc=[0.89375 0.81355]
76/100: weights=[0.6878787  0.31212127], train_loss=[0.32978284 0.49720815], train_acc=[0.8924 0.8171]
78/100: weights=[0.65955675 0.34044328], train_loss=[0.33116704 0.49018064], train_acc=[0.89405 0.82125]
80/100: weights=[0.5835493  0.41645068], train_loss=[0.32876885 0.4967554 ], train_acc=[0.8916 0.8169]
82/100: weights=[0.4163075  0.58369243], train_loss=[0.3236263  0.49154854], train_acc=[0.8947 0.8205]
84/100: weights=[0.42928076 0.57071924], train_loss=[0.31778684 0.48715675], train_acc=[0.8972 0.8213]
86/100: weights=[0.2929529 0.7070471], train_loss=[0.31578258 0.48434415], train_acc=[0.89715 0.82205]
88/100: weights=[0.46080303 0.53919697], train_loss=[0.31291708 0.49086776], train_acc=[0.89825 0.82175]
90/100: weights=[0.10273783 0.8972622 ], train_loss=[0.3147564 0.498008 ], train_acc=[0.8977 0.815 ]
92/100: weights=[0.23789538 0.7621046 ], train_loss=[0.30971798 0.48645696], train_acc=[0.8989 0.8239]
94/100: weights=[0.3144725 0.6855275], train_loss=[0.31058478 0.4882067 ], train_acc=[0.8969  0.81945]
96/100: weights=[0.29741502 0.70258504], train_loss=[0.30744332 0.4868547 ], train_acc=[0.90065 0.82075]
98/100: weights=[0.41439962 0.5856004 ], train_loss=[0.30830038 0.47487187], train_acc=[0.89985 0.8264 ]
100/100: weights=[0.18211094 0.8178891 ], train_loss=[0.3070878  0.47603637], train_acc=[0.8995  0.82465]
**** Time taken for fashion_and_mnist_2 = 855.638060092926
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
Preference Vector (4/5):
[0.382756   0.92384946]
fealsible solution is obtained.
1/100: weights=[0.92266643 0.07733354], train_loss=[1.1573632 1.7080473], train_acc=[0.6154 0.3937]
2/100: weights=[ 0.78779465 -0.21220534], train_loss=[0.87966967 1.308376  ], train_acc=[0.71225 0.53195]
4/100: weights=[0.8544732  0.14552686], train_loss=[0.6773882  0.99376565], train_acc=[0.778   0.63035]
6/100: weights=[0.7680958  0.23190424], train_loss=[0.6460464 0.9116783], train_acc=[0.7874 0.6563]
8/100: weights=[0.514714   0.48528597], train_loss=[0.5105481 0.823631 ], train_acc=[0.83635 0.6973 ]
10/100: weights=[0.3866084  0.61339164], train_loss=[0.4664034 0.7839958], train_acc=[0.84895 0.71485]
12/100: weights=[0.55104935 0.44895062], train_loss=[0.43784624 0.74849355], train_acc=[0.85785 0.7148 ]
14/100: weights=[0.14756209 0.8524379 ], train_loss=[0.4333799  0.72786635], train_acc=[0.8604  0.72595]
16/100: weights=[0.98681    0.01318998], train_loss=[0.4573474 0.701479 ], train_acc=[0.8455  0.73605]
18/100: weights=[0.21870267 0.7812973 ], train_loss=[0.3832617 0.6779732], train_acc=[0.877  0.7426]
20/100: weights=[0.35359466 0.64640534], train_loss=[0.3751148 0.6898542], train_acc=[0.8802  0.74305]
22/100: weights=[0.54211813 0.45788187], train_loss=[0.4150059 0.6680606], train_acc=[0.8647  0.74705]
24/100: weights=[0.7777059 0.2222941], train_loss=[0.37237522 0.63343287], train_acc=[0.87845 0.7655 ]
26/100: weights=[0.5201981 0.4798019], train_loss=[0.34671226 0.6225667 ], train_acc=[0.88985 0.76895]
28/100: weights=[0.2818212 0.7181788], train_loss=[0.32914397 0.63163865], train_acc=[0.89495 0.76355]
30/100: weights=[0.39020422 0.6097958 ], train_loss=[0.3279732 0.6270804], train_acc=[0.8956  0.76595]
32/100: weights=[0.11073469 0.8892653 ], train_loss=[0.3170867  0.60103333], train_acc=[0.89875 0.78025]
34/100: weights=[0.48650673 0.51349324], train_loss=[0.32183075 0.5923746 ], train_acc=[0.8973 0.7796]
36/100: weights=[0.5239036  0.47609636], train_loss=[0.35853526 0.61137843], train_acc=[0.8828  0.77555]
38/100: weights=[0.44613785 0.55386215], train_loss=[0.30796057 0.5834752 ], train_acc=[0.9004 0.7819]
40/100: weights=[0.5855569  0.41444308], train_loss=[0.30313435 0.5875283 ], train_acc=[0.90285 0.77915]
42/100: weights=[0.55848116 0.44151887], train_loss=[0.3115489  0.57575977], train_acc=[0.89955 0.7859 ]
44/100: weights=[0.5711916  0.42880842], train_loss=[0.2870832  0.57012016], train_acc=[0.9046 0.7911]
46/100: weights=[0.50066423 0.49933574], train_loss=[0.2907642 0.5601034], train_acc=[0.90415 0.79415]
48/100: weights=[0.93612903 0.06387101], train_loss=[0.2905042 0.5547414], train_acc=[0.9065 0.795 ]
50/100: weights=[0.40666083 0.59333915], train_loss=[0.28142628 0.5840396 ], train_acc=[0.9067 0.7789]
52/100: weights=[0.29455075 0.7054493 ], train_loss=[0.29627416 0.54727626], train_acc=[0.902   0.80045]
54/100: weights=[0.43675303 0.56324697], train_loss=[0.28653932 0.57785785], train_acc=[0.90725 0.7882 ]
56/100: weights=[0.16401085 0.8359892 ], train_loss=[0.2706458 0.5493481], train_acc=[0.91175 0.79715]
58/100: weights=[0.5179664  0.48203358], train_loss=[0.26952577 0.5434751 ], train_acc=[0.912   0.79945]
60/100: weights=[0.54300237 0.45699763], train_loss=[0.27878708 0.5377879 ], train_acc=[0.9091 0.8035]
62/100: weights=[0.7636885  0.23631153], train_loss=[0.27602282 0.5364966 ], train_acc=[0.91    0.80255]
64/100: weights=[0.54757816 0.45242184], train_loss=[0.27191105 0.53702587], train_acc=[0.91105 0.80525]
66/100: weights=[0.8292169  0.17078312], train_loss=[0.27678806 0.5382277 ], train_acc=[0.90935 0.80075]
68/100: weights=[0.5210291 0.4789709], train_loss=[0.2796054  0.53562546], train_acc=[0.90945 0.80515]
70/100: weights=[0.43933597 0.560664  ], train_loss=[0.2675086 0.5271015], train_acc=[0.9124 0.8056]
72/100: weights=[0.27283913 0.7271609 ], train_loss=[0.26080337 0.5332036 ], train_acc=[0.91465 0.8052 ]
74/100: weights=[0.80544865 0.19455132], train_loss=[0.26995644 0.52924687], train_acc=[0.91145 0.80785]
76/100: weights=[0.3978128  0.60218716], train_loss=[0.25970465 0.52264905], train_acc=[0.91535 0.80945]
78/100: weights=[0.65957874 0.34042126], train_loss=[0.2612844  0.53880477], train_acc=[0.9143  0.80335]
80/100: weights=[0.73430943 0.26569054], train_loss=[0.2581212 0.5186463], train_acc=[0.9161 0.8114]
82/100: weights=[0.32264557 0.67735445], train_loss=[0.25494328 0.5237679 ], train_acc=[0.91635 0.80775]
84/100: weights=[0.5513702 0.4486298], train_loss=[0.25197312 0.51904315], train_acc=[0.9182  0.80905]
86/100: weights=[0.5994873  0.40051273], train_loss=[0.25418603 0.51674056], train_acc=[0.9167  0.81165]
88/100: weights=[0.62857056 0.37142944], train_loss=[0.2510295  0.51772654], train_acc=[0.91845 0.80925]
90/100: weights=[0.36609522 0.6339048 ], train_loss=[0.25473413 0.5112046 ], train_acc=[0.91735 0.8137 ]
92/100: weights=[0.5519325 0.4480675], train_loss=[0.25122693 0.51231045], train_acc=[0.91885 0.81325]
94/100: weights=[0.47739702 0.522603  ], train_loss=[0.25735828 0.5189801 ], train_acc=[0.9153  0.81185]
96/100: weights=[0.5264893  0.47351068], train_loss=[0.2581141  0.51213056], train_acc=[0.9169  0.81245]
98/100: weights=[0.42227393 0.57772607], train_loss=[0.26704538 0.50912297], train_acc=[0.91345 0.8145 ]
100/100: weights=[0.44608963 0.5539104 ], train_loss=[0.24333216 0.5050976 ], train_acc=[0.92105 0.81585]
**** Time taken for fashion_and_mnist_3 = 856.8629956245422
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
Preference Vector (5/5):
[1.5707963e-04 1.0000000e+00]
fealsible solution is obtained.
1/100: weights=[0.9371237  0.06287624], train_loss=[1.0783807 2.8078763], train_acc=[0.6421  0.18555]
2/100: weights=[0.7893082  0.21069184], train_loss=[0.8620314 1.9324347], train_acc=[0.71755 0.3454 ]
4/100: weights=[0.6117963 0.3882037], train_loss=[0.69106257 1.3622086 ], train_acc=[0.7727  0.50435]
6/100: weights=[0.78456116 0.21543877], train_loss=[0.5877712 1.0977197], train_acc=[0.8068  0.60025]
8/100: weights=[0.5913113  0.40868875], train_loss=[0.5151214 0.9758006], train_acc=[0.832   0.65055]
10/100: weights=[0.92803144 0.07196856], train_loss=[0.48392895 0.8993432 ], train_acc=[0.84285 0.67595]
12/100: weights=[0.8661202  0.13387978], train_loss=[0.47091547 0.85627896], train_acc=[0.84535 0.68555]
14/100: weights=[0.8968912  0.10310884], train_loss=[0.42221847 0.82748365], train_acc=[0.866  0.6931]
16/100: weights=[0.7245987 0.2754013], train_loss=[0.39619827 0.78834355], train_acc=[0.87195 0.7122 ]
18/100: weights=[0.71616024 0.28383982], train_loss=[0.39473918 0.7732055 ], train_acc=[0.87385 0.71775]
20/100: weights=[0.8828864  0.11711355], train_loss=[0.406404  0.7545005], train_acc=[0.8684 0.7202]
22/100: weights=[0.3001634  0.69983655], train_loss=[0.34097174 0.7583632 ], train_acc=[0.89445 0.71755]
24/100: weights=[0.8960786  0.10392142], train_loss=[0.35981515 0.73028445], train_acc=[0.88345 0.7321 ]
26/100: weights=[0.7146191 0.2853809], train_loss=[0.33970648 0.73905134], train_acc=[0.89145 0.724  ]
28/100: weights=[0.72317016 0.27682984], train_loss=[0.34793076 0.7079304 ], train_acc=[0.8876  0.73955]
30/100: weights=[0.6454781  0.35452187], train_loss=[0.30695388 0.69962156], train_acc=[0.90205 0.7432 ]
32/100: weights=[0.84747845 0.15252152], train_loss=[0.31826895 0.6911515 ], train_acc=[0.89735 0.7459 ]
34/100: weights=[0.7397118 0.2602882], train_loss=[0.2969032 0.6892356], train_acc=[0.90595 0.7446 ]
36/100: weights=[0.58462214 0.41537786], train_loss=[0.294264  0.6836505], train_acc=[0.90675 0.7489 ]
38/100: weights=[0.6422564  0.35774362], train_loss=[0.31316182 0.6866254 ], train_acc=[0.90155 0.74265]
40/100: weights=[0.92070633 0.07929368], train_loss=[0.28348827 0.67602456], train_acc=[0.9106  0.74635]
42/100: weights=[0.8881814  0.11181857], train_loss=[0.29002708 0.6656999 ], train_acc=[0.90725 0.75415]
44/100: weights=[0.58813125 0.41186866], train_loss=[0.28053376 0.6630188 ], train_acc=[0.91105 0.7533 ]
46/100: weights=[0.55615294 0.4438471 ], train_loss=[0.27632022 0.6608566 ], train_acc=[0.9116  0.75255]
48/100: weights=[0.82835346 0.17164648], train_loss=[0.2657984 0.6579615], train_acc=[0.9155 0.7563]
50/100: weights=[0.84923387 0.15076615], train_loss=[0.270894   0.64964336], train_acc=[0.91305 0.7569 ]
52/100: weights=[0.72585547 0.27414447], train_loss=[0.257741  0.6497824], train_acc=[0.9186  0.75715]
54/100: weights=[0.6103474 0.3896526], train_loss=[0.2616568  0.64799947], train_acc=[0.91775 0.7573 ]
56/100: weights=[0.9589892  0.04101076], train_loss=[0.24777628 0.63765746], train_acc=[0.9225 0.7611]
58/100: weights=[0.748137   0.25186303], train_loss=[0.25589177 0.63862014], train_acc=[0.9191  0.75915]
60/100: weights=[0.9391383  0.06086167], train_loss=[0.2503946 0.6331674], train_acc=[0.92165 0.76325]
62/100: weights=[0.64243454 0.35756555], train_loss=[0.2461229 0.6395555], train_acc=[0.9205  0.75905]
64/100: weights=[0.7508378  0.24916226], train_loss=[0.24055068 0.6367815 ], train_acc=[0.92485 0.76015]
66/100: weights=[0.668845   0.33115497], train_loss=[0.24249001 0.6334007 ], train_acc=[0.92295 0.763  ]
68/100: weights=[0.9474844  0.05251562], train_loss=[0.23967323 0.62591755], train_acc=[0.92345 0.7663 ]
70/100: weights=[0.9145929  0.08540718], train_loss=[0.23805638 0.62340516], train_acc=[0.9229  0.76725]
72/100: weights=[0.84648377 0.15351625], train_loss=[0.23323876 0.6214857 ], train_acc=[0.9265 0.7664]
74/100: weights=[0.4890587  0.51094127], train_loss=[0.23493123 0.6214497 ], train_acc=[0.92555 0.7675 ]
76/100: weights=[0.90305626 0.09694376], train_loss=[0.23722407 0.61831295], train_acc=[0.92475 0.76905]
78/100: weights=[0.77080035 0.22919962], train_loss=[0.22858024 0.6164765 ], train_acc=[0.9293 0.7693]
80/100: weights=[0.95328283 0.04671721], train_loss=[0.22427316 0.6127675 ], train_acc=[0.92845 0.77125]
82/100: weights=[0.77426773 0.22573225], train_loss=[0.22335862 0.61344796], train_acc=[0.92965 0.77065]
84/100: weights=[0.6492696  0.35073042], train_loss=[0.22657397 0.6191628 ], train_acc=[0.92815 0.7685 ]
86/100: weights=[0.71046436 0.28953564], train_loss=[0.22380614 0.61039466], train_acc=[0.92875 0.7729 ]
88/100: weights=[0.7203417  0.27965832], train_loss=[0.225179  0.6158907], train_acc=[0.93    0.76825]
90/100: weights=[0.62270683 0.37729326], train_loss=[0.23533887 0.60424966], train_acc=[0.9266 0.7747]
92/100: weights=[0.6132317  0.38676828], train_loss=[0.2225831  0.61059785], train_acc=[0.9302 0.7737]
94/100: weights=[0.7867012  0.21329878], train_loss=[0.21743922 0.6038454 ], train_acc=[0.9315  0.77495]
96/100: weights=[0.8308224  0.16917765], train_loss=[0.21815331 0.60338247], train_acc=[0.93235 0.7752 ]
98/100: weights=[0.7855195  0.21448052], train_loss=[0.21839772 0.60205543], train_acc=[0.93155 0.7766 ]
100/100: weights=[0.81813455 0.18186544], train_loss=[0.23508759 0.602834  ], train_acc=[0.9247 0.7747]
**** Time taken for fashion_and_mnist_4 = 918.6319608688354
**** Time taken for fashion_and_mnist = 4454.337676048279
