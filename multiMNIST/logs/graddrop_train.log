==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.5123261 1.7083005], train_acc=[0.50055 0.41355]
2/100: train_loss=[1.0697052 1.2221342], train_acc=[0.6532  0.58425]
4/100: train_loss=[0.7911429 0.9180008], train_acc=[0.74285 0.69105]
6/100: train_loss=[0.67914754 0.793131  ], train_acc=[0.7776 0.7308]
8/100: train_loss=[0.6135765 0.725679 ], train_acc=[0.801  0.7601]
10/100: train_loss=[0.5716495  0.66144186], train_acc=[0.8153  0.78045]
12/100: train_loss=[0.51924974 0.62900925], train_acc=[0.829  0.7874]
14/100: train_loss=[0.49232948 0.6188798 ], train_acc=[0.84005 0.79215]
16/100: train_loss=[0.47786072 0.5639299 ], train_acc=[0.8428 0.8103]
18/100: train_loss=[0.45566347 0.56661415], train_acc=[0.84795 0.8085 ]
20/100: train_loss=[0.43956476 0.5467483 ], train_acc=[0.85605 0.81595]
22/100: train_loss=[0.42377788 0.5194254 ], train_acc=[0.86   0.8277]
24/100: train_loss=[0.41619816 0.52363473], train_acc=[0.86415 0.8254 ]
26/100: train_loss=[0.41181827 0.51096594], train_acc=[0.86325 0.8291 ]
28/100: train_loss=[0.40719044 0.5020637 ], train_acc=[0.8667  0.83405]
30/100: train_loss=[0.40416288 0.50448066], train_acc=[0.86775 0.831  ]
32/100: train_loss=[0.40631062 0.5047637 ], train_acc=[0.8667 0.8283]
34/100: train_loss=[0.3788312  0.48988807], train_acc=[0.87435 0.83805]
36/100: train_loss=[0.37424216 0.4716658 ], train_acc=[0.8746  0.84245]
38/100: train_loss=[0.3702125  0.45774677], train_acc=[0.87565 0.8465 ]
40/100: train_loss=[0.37767765 0.51687795], train_acc=[0.87435 0.8271 ]
42/100: train_loss=[0.3773223 0.4433627], train_acc=[0.87385 0.85065]
44/100: train_loss=[0.37481096 0.43801552], train_acc=[0.87395 0.8536 ]
46/100: train_loss=[0.36208493 0.45159662], train_acc=[0.8806 0.8483]
48/100: train_loss=[0.35852805 0.42797953], train_acc=[0.8814  0.85595]
50/100: train_loss=[0.34929058 0.42185077], train_acc=[0.8847 0.861 ]
52/100: train_loss=[0.33817565 0.4376599 ], train_acc=[0.888  0.8553]
54/100: train_loss=[0.3354411 0.4199235], train_acc=[0.8901  0.85955]
56/100: train_loss=[0.3447164  0.41920424], train_acc=[0.88865 0.85885]
58/100: train_loss=[0.33703306 0.4186206 ], train_acc=[0.88975 0.8594 ]
60/100: train_loss=[0.33524245 0.41809446], train_acc=[0.88985 0.86065]
62/100: train_loss=[0.32823646 0.40569904], train_acc=[0.89195 0.8662 ]
64/100: train_loss=[0.3313769  0.40366006], train_acc=[0.89315 0.8645 ]
66/100: train_loss=[0.33728522 0.40571514], train_acc=[0.889   0.86135]
68/100: train_loss=[0.3366343  0.40663746], train_acc=[0.88905 0.86225]
70/100: train_loss=[0.32924584 0.39397636], train_acc=[0.894 0.865]
72/100: train_loss=[0.32447955 0.4235941 ], train_acc=[0.89425 0.8573 ]
74/100: train_loss=[0.3246532  0.40448162], train_acc=[0.89395 0.86355]
76/100: train_loss=[0.3228839 0.3827507], train_acc=[0.8955  0.87085]
78/100: train_loss=[0.318543   0.38870493], train_acc=[0.8965  0.86855]
80/100: train_loss=[0.32589558 0.38547164], train_acc=[0.89365 0.87235]
82/100: train_loss=[0.31801823 0.37828007], train_acc=[0.89705 0.87115]
84/100: train_loss=[0.31619072 0.38194478], train_acc=[0.89655 0.8716 ]
86/100: train_loss=[0.3179315  0.37800074], train_acc=[0.89645 0.87565]
88/100: train_loss=[0.31222653 0.37287045], train_acc=[0.89655 0.87635]
90/100: train_loss=[0.31585968 0.37720153], train_acc=[0.8973  0.87555]
92/100: train_loss=[0.3081742  0.38555396], train_acc=[0.8985  0.87315]
94/100: train_loss=[0.30752444 0.37658766], train_acc=[0.8995  0.87415]
96/100: train_loss=[0.32768548 0.3773703 ], train_acc=[0.8924  0.87445]
98/100: train_loss=[0.30670354 0.37035087], train_acc=[0.9006  0.87635]
100/100: train_loss=[0.30589053 0.38668188], train_acc=[0.89945 0.87225]
**** Time taken for mnist_0 = 2612.893307685852
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.7759215 2.1478615], train_acc=[0.38855 0.22195]
2/100: train_loss=[1.1457505 1.3411824], train_acc=[0.6122 0.5444]
4/100: train_loss=[0.8625813  0.92672473], train_acc=[0.7135 0.6859]
6/100: train_loss=[0.72363514 0.7622384 ], train_acc=[0.7605  0.74105]
8/100: train_loss=[0.6427481  0.69524395], train_acc=[0.7857  0.76665]
10/100: train_loss=[0.5995188  0.63865936], train_acc=[0.8035  0.78315]
12/100: train_loss=[0.5736284 0.6119128], train_acc=[0.8114  0.79335]
14/100: train_loss=[0.54589623 0.57180923], train_acc=[0.8189  0.80715]
16/100: train_loss=[0.5033669  0.55799145], train_acc=[0.83295 0.8092 ]
18/100: train_loss=[0.4859402 0.5402267], train_acc=[0.8386  0.81845]
20/100: train_loss=[0.46422476 0.50897586], train_acc=[0.84635 0.8301 ]
22/100: train_loss=[0.45716718 0.5024178 ], train_acc=[0.849   0.83175]
24/100: train_loss=[0.4618442  0.48711285], train_acc=[0.8484 0.8357]
26/100: train_loss=[0.44134194 0.47712356], train_acc=[0.85545 0.841  ]
28/100: train_loss=[0.42290232 0.46922767], train_acc=[0.862  0.8392]
30/100: train_loss=[0.41091245 0.46470702], train_acc=[0.8655  0.84405]
32/100: train_loss=[0.41178676 0.44957927], train_acc=[0.8652  0.84975]
34/100: train_loss=[0.40052325 0.48553824], train_acc=[0.86695 0.83925]
36/100: train_loss=[0.44252738 0.44861352], train_acc=[0.85545 0.84545]
38/100: train_loss=[0.38760883 0.42526448], train_acc=[0.87185 0.8574 ]
40/100: train_loss=[0.38592806 0.42596987], train_acc=[0.87345 0.85675]
42/100: train_loss=[0.4082985 0.4349463], train_acc=[0.86685 0.8545 ]
44/100: train_loss=[0.37258008 0.42789963], train_acc=[0.87765 0.8568 ]
46/100: train_loss=[0.37429655 0.43047738], train_acc=[0.8783  0.85555]
48/100: train_loss=[0.36567235 0.41942927], train_acc=[0.88085 0.86045]
50/100: train_loss=[0.37024024 0.4074245 ], train_acc=[0.88015 0.8642 ]
52/100: train_loss=[0.35744187 0.4161757 ], train_acc=[0.8825  0.86035]
54/100: train_loss=[0.35606623 0.41868746], train_acc=[0.8834  0.86045]
56/100: train_loss=[0.35641155 0.4047044 ], train_acc=[0.8819  0.86335]
58/100: train_loss=[0.39168343 0.41516548], train_acc=[0.8697  0.86005]
60/100: train_loss=[0.35785228 0.3957386 ], train_acc=[0.8831  0.86675]
62/100: train_loss=[0.36424732 0.4280653 ], train_acc=[0.8831  0.85765]
64/100: train_loss=[0.35130715 0.39189795], train_acc=[0.8863 0.8681]
66/100: train_loss=[0.36243612 0.4011917 ], train_acc=[0.8801 0.8652]
68/100: train_loss=[0.33797717 0.3848948 ], train_acc=[0.88805 0.87145]
70/100: train_loss=[0.3397269 0.3870458], train_acc=[0.8899  0.86875]
72/100: train_loss=[0.33606344 0.38767776], train_acc=[0.8885 0.8696]
74/100: train_loss=[0.33612067 0.37761655], train_acc=[0.88915 0.874  ]
76/100: train_loss=[0.3482397  0.38738483], train_acc=[0.8847 0.8695]
78/100: train_loss=[0.33387393 0.39748624], train_acc=[0.8892  0.86815]
80/100: train_loss=[0.33148342 0.3759779 ], train_acc=[0.89235 0.87395]
82/100: train_loss=[0.33961535 0.3724147 ], train_acc=[0.88805 0.8755 ]
84/100: train_loss=[0.33664244 0.38181257], train_acc=[0.88845 0.8708 ]
86/100: train_loss=[0.32793206 0.3755896 ], train_acc=[0.8923  0.87275]
88/100: train_loss=[0.33250555 0.37379047], train_acc=[0.89045 0.87515]
90/100: train_loss=[0.3309149 0.3695368], train_acc=[0.8909  0.87615]
92/100: train_loss=[0.32678655 0.38064212], train_acc=[0.8929 0.8731]
94/100: train_loss=[0.3359057  0.37730765], train_acc=[0.89    0.87505]
96/100: train_loss=[0.3242125 0.3621641], train_acc=[0.89435 0.8786 ]
98/100: train_loss=[0.32825303 0.37349796], train_acc=[0.89215 0.875  ]
100/100: train_loss=[0.32376277 0.3658688 ], train_acc=[0.89515 0.87785]
**** Time taken for mnist_1 = 2598.829444885254
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.3262618 1.7341598], train_acc=[0.5478 0.4013]
2/100: train_loss=[0.95709133 1.1887181 ], train_acc=[0.6824 0.5875]
4/100: train_loss=[0.68305796 0.8295263 ], train_acc=[0.77335 0.71735]
6/100: train_loss=[0.61581594 0.72718126], train_acc=[0.7957 0.7536]
8/100: train_loss=[0.5598937  0.67906845], train_acc=[0.81755 0.7738 ]
10/100: train_loss=[0.5195949  0.63579404], train_acc=[0.8304  0.78415]
12/100: train_loss=[0.48358768 0.57036686], train_acc=[0.84165 0.80705]
14/100: train_loss=[0.46435156 0.54489416], train_acc=[0.85065 0.8158 ]
16/100: train_loss=[0.45426127 0.5300515 ], train_acc=[0.85495 0.8233 ]
18/100: train_loss=[0.4369958 0.5104985], train_acc=[0.8602 0.8285]
20/100: train_loss=[0.45270193 0.5019175 ], train_acc=[0.85495 0.8339 ]
22/100: train_loss=[0.42733192 0.47802392], train_acc=[0.86195 0.84105]
24/100: train_loss=[0.41311723 0.494818  ], train_acc=[0.86625 0.83655]
26/100: train_loss=[0.39685804 0.46128413], train_acc=[0.8724 0.8449]
28/100: train_loss=[0.3957504  0.45851025], train_acc=[0.87365 0.8455 ]
30/100: train_loss=[0.39453444 0.44391382], train_acc=[0.8728  0.85135]
32/100: train_loss=[0.37873942 0.44792747], train_acc=[0.87925 0.8529 ]
34/100: train_loss=[0.3690138  0.44165993], train_acc=[0.8823  0.85275]
36/100: train_loss=[0.36538455 0.42519066], train_acc=[0.88255 0.8572 ]
38/100: train_loss=[0.3661822  0.43164164], train_acc=[0.88245 0.85545]
40/100: train_loss=[0.37899894 0.42694417], train_acc=[0.8787 0.8575]
42/100: train_loss=[0.35691997 0.43296468], train_acc=[0.8847 0.856 ]
44/100: train_loss=[0.35688266 0.4095276 ], train_acc=[0.88435 0.86325]
46/100: train_loss=[0.34153405 0.40142214], train_acc=[0.8909 0.8661]
48/100: train_loss=[0.342587   0.40130588], train_acc=[0.89065 0.86645]
50/100: train_loss=[0.34332836 0.3933935 ], train_acc=[0.8884  0.86865]
52/100: train_loss=[0.3345954  0.40950173], train_acc=[0.89205 0.86235]
54/100: train_loss=[0.35928625 0.40515468], train_acc=[0.8842 0.8678]
56/100: train_loss=[0.3305348  0.44914642], train_acc=[0.89485 0.8504 ]
58/100: train_loss=[0.32328865 0.3985313 ], train_acc=[0.8966 0.8652]
60/100: train_loss=[0.3277105 0.3867206], train_acc=[0.89415 0.86985]
62/100: train_loss=[0.31723934 0.39152965], train_acc=[0.89765 0.86925]
64/100: train_loss=[0.32389358 0.37857816], train_acc=[0.89485 0.8753 ]
66/100: train_loss=[0.3213939  0.38413593], train_acc=[0.89555 0.87325]
68/100: train_loss=[0.31744695 0.37411678], train_acc=[0.8988  0.87385]
70/100: train_loss=[0.31173027 0.3911673 ], train_acc=[0.89945 0.87085]
72/100: train_loss=[0.3104273  0.37319443], train_acc=[0.90035 0.8755 ]
74/100: train_loss=[0.30910224 0.3646272 ], train_acc=[0.89875 0.8783 ]
76/100: train_loss=[0.30212873 0.36980438], train_acc=[0.9008 0.8758]
78/100: train_loss=[0.29634246 0.37705177], train_acc=[0.9038 0.8744]
80/100: train_loss=[0.3032508  0.36874852], train_acc=[0.9009  0.87765]
82/100: train_loss=[0.30035937 0.35298863], train_acc=[0.90215 0.8817 ]
84/100: train_loss=[0.29558998 0.34920487], train_acc=[0.9047 0.8831]
86/100: train_loss=[0.29201975 0.36214936], train_acc=[0.9038  0.88195]
88/100: train_loss=[0.29754853 0.34536266], train_acc=[0.90255 0.88665]
90/100: train_loss=[0.2966232 0.3498114], train_acc=[0.9024  0.88275]
92/100: train_loss=[0.29465443 0.3637431 ], train_acc=[0.9045  0.87915]
94/100: train_loss=[0.28705305 0.34803116], train_acc=[0.90595 0.88355]
96/100: train_loss=[0.2908499 0.3562667], train_acc=[0.9062  0.88105]
98/100: train_loss=[0.28371155 0.34696525], train_acc=[0.90795 0.88585]
100/100: train_loss=[0.29100955 0.33942813], train_acc=[0.905   0.88645]
**** Time taken for mnist_2 = 2639.1744968891144
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.3647139 1.4930065], train_acc=[0.53445 0.4876 ]
2/100: train_loss=[0.9432292 1.0790734], train_acc=[0.68745 0.63175]
4/100: train_loss=[0.7142274 0.7810672], train_acc=[0.76585 0.73635]
6/100: train_loss=[0.59729093 0.6734247 ], train_acc=[0.80265 0.7758 ]
8/100: train_loss=[0.5788677 0.6285389], train_acc=[0.8108 0.7939]
10/100: train_loss=[0.53068846 0.58891314], train_acc=[0.82635 0.8043 ]
12/100: train_loss=[0.5273162  0.56834465], train_acc=[0.8259  0.80895]
14/100: train_loss=[0.4702065  0.53390944], train_acc=[0.8451 0.8236]
16/100: train_loss=[0.44949517 0.5260508 ], train_acc=[0.8525  0.82215]
18/100: train_loss=[0.4255164 0.5242464], train_acc=[0.8594 0.8283]
20/100: train_loss=[0.41064236 0.47302783], train_acc=[0.8662 0.8451]
22/100: train_loss=[0.40273213 0.4762827 ], train_acc=[0.86915 0.843  ]
24/100: train_loss=[0.39668593 0.480505  ], train_acc=[0.86975 0.83995]
26/100: train_loss=[0.38391683 0.44106305], train_acc=[0.87365 0.8564 ]
28/100: train_loss=[0.38459876 0.44586754], train_acc=[0.875   0.85345]
30/100: train_loss=[0.38020742 0.4357805 ], train_acc=[0.87565 0.8556 ]
32/100: train_loss=[0.37454605 0.44466797], train_acc=[0.8763  0.85225]
34/100: train_loss=[0.36468795 0.41783243], train_acc=[0.88005 0.86265]
36/100: train_loss=[0.35629836 0.4140737 ], train_acc=[0.8832 0.8631]
38/100: train_loss=[0.36006677 0.41284636], train_acc=[0.88075 0.8636 ]
40/100: train_loss=[0.3586427  0.40995774], train_acc=[0.88235 0.8647 ]
42/100: train_loss=[0.34750053 0.4104333 ], train_acc=[0.88615 0.8645 ]
44/100: train_loss=[0.34713563 0.40436748], train_acc=[0.88505 0.8676 ]
46/100: train_loss=[0.3444953  0.40177497], train_acc=[0.88745 0.8663 ]
48/100: train_loss=[0.3417165 0.4077856], train_acc=[0.88705 0.86335]
50/100: train_loss=[0.33968103 0.3965106 ], train_acc=[0.88825 0.86865]
52/100: train_loss=[0.33762196 0.4044952 ], train_acc=[0.88945 0.86525]
54/100: train_loss=[0.3357894  0.38157257], train_acc=[0.8902  0.87345]
56/100: train_loss=[0.32987553 0.39085487], train_acc=[0.89285 0.8706 ]
58/100: train_loss=[0.3299114 0.3869925], train_acc=[0.89175 0.872  ]
60/100: train_loss=[0.3224934  0.37805212], train_acc=[0.89445 0.87315]
62/100: train_loss=[0.32297787 0.377802  ], train_acc=[0.8935 0.8749]
64/100: train_loss=[0.3351694  0.38320002], train_acc=[0.89305 0.87185]
66/100: train_loss=[0.3154912  0.37217504], train_acc=[0.8944 0.8754]
68/100: train_loss=[0.3200898  0.40579852], train_acc=[0.8947  0.86475]
70/100: train_loss=[0.31282714 0.37935254], train_acc=[0.8992 0.874 ]
72/100: train_loss=[0.31618312 0.36982796], train_acc=[0.8961  0.87585]
74/100: train_loss=[0.3141175  0.35960713], train_acc=[0.8974  0.88005]
76/100: train_loss=[0.30758247 0.372627  ], train_acc=[0.9018  0.87755]
78/100: train_loss=[0.3055899  0.36888763], train_acc=[0.90145 0.87935]
80/100: train_loss=[0.33209074 0.36765313], train_acc=[0.8906  0.87805]
82/100: train_loss=[0.29978842 0.36304066], train_acc=[0.90195 0.88055]
84/100: train_loss=[0.30401573 0.36845955], train_acc=[0.9006  0.87705]
86/100: train_loss=[0.30158168 0.3780295 ], train_acc=[0.90165 0.87575]
88/100: train_loss=[0.30164018 0.3672136 ], train_acc=[0.9021 0.8779]
90/100: train_loss=[0.2994666  0.35795492], train_acc=[0.9018  0.88205]
92/100: train_loss=[0.3000185  0.35278675], train_acc=[0.90235 0.88295]
94/100: train_loss=[0.29482418 0.3569392 ], train_acc=[0.9043 0.8806]
96/100: train_loss=[0.30488133 0.34917584], train_acc=[0.8993 0.8848]
98/100: train_loss=[0.28920254 0.3516591 ], train_acc=[0.90615 0.8829 ]
100/100: train_loss=[0.28978908 0.3620342 ], train_acc=[0.90425 0.87995]
**** Time taken for mnist_3 = 2639.1526992321014
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.2143836 1.376144 ], train_acc=[0.5964 0.5373]
2/100: train_loss=[0.8302777 0.960175 ], train_acc=[0.7306 0.6754]
4/100: train_loss=[0.6319238  0.74648434], train_acc=[0.7955 0.7536]
6/100: train_loss=[0.55990237 0.6332379 ], train_acc=[0.8163 0.7902]
8/100: train_loss=[0.4847021 0.5840224], train_acc=[0.84115 0.8045 ]
10/100: train_loss=[0.47293103 0.5525914 ], train_acc=[0.84435 0.81855]
12/100: train_loss=[0.4405108 0.5210273], train_acc=[0.85695 0.82615]
14/100: train_loss=[0.42351422 0.5048531 ], train_acc=[0.8616 0.8328]
16/100: train_loss=[0.44073448 0.5246063 ], train_acc=[0.85635 0.82665]
18/100: train_loss=[0.39517143 0.47727334], train_acc=[0.87225 0.8412 ]
20/100: train_loss=[0.3833851  0.45301569], train_acc=[0.87755 0.84995]
22/100: train_loss=[0.38351673 0.4609532 ], train_acc=[0.87675 0.84725]
24/100: train_loss=[0.37921545 0.44897577], train_acc=[0.8784  0.85305]
26/100: train_loss=[0.39548856 0.45149508], train_acc=[0.8707  0.85195]
28/100: train_loss=[0.35970306 0.44467115], train_acc=[0.88315 0.855  ]
30/100: train_loss=[0.369129   0.45154455], train_acc=[0.87885 0.8515 ]
32/100: train_loss=[0.35453588 0.4163594 ], train_acc=[0.88475 0.86455]
34/100: train_loss=[0.35151485 0.45718944], train_acc=[0.88465 0.8514 ]
36/100: train_loss=[0.33909792 0.4126076 ], train_acc=[0.8898 0.8659]
38/100: train_loss=[0.34251148 0.4038341 ], train_acc=[0.889   0.86825]
40/100: train_loss=[0.37901717 0.429523  ], train_acc=[0.8734 0.8572]
42/100: train_loss=[0.33137992 0.40388396], train_acc=[0.8907  0.86815]
44/100: train_loss=[0.32338992 0.4151973 ], train_acc=[0.89195 0.86335]
46/100: train_loss=[0.3454421  0.40497434], train_acc=[0.88365 0.86905]
48/100: train_loss=[0.33612034 0.40505195], train_acc=[0.8895  0.86725]
50/100: train_loss=[0.31703463 0.42208505], train_acc=[0.8952 0.8634]
52/100: train_loss=[0.3187245  0.42797214], train_acc=[0.8946 0.8605]
54/100: train_loss=[0.31865567 0.38664737], train_acc=[0.89555 0.8754 ]
56/100: train_loss=[0.3298985 0.385669 ], train_acc=[0.889   0.87295]
58/100: train_loss=[0.31128654 0.38149974], train_acc=[0.89605 0.87565]
60/100: train_loss=[0.30772343 0.37995327], train_acc=[0.8973  0.87775]
62/100: train_loss=[0.31702265 0.37772384], train_acc=[0.893  0.8766]
64/100: train_loss=[0.30939636 0.38172123], train_acc=[0.8963 0.8751]
66/100: train_loss=[0.3026789 0.3776405], train_acc=[0.89915 0.87775]
68/100: train_loss=[0.30434    0.37797672], train_acc=[0.9006 0.8769]
70/100: train_loss=[0.30978727 0.42761797], train_acc=[0.89525 0.8628 ]
72/100: train_loss=[0.30314246 0.3744039 ], train_acc=[0.90005 0.8771 ]
74/100: train_loss=[0.30023372 0.37223077], train_acc=[0.89915 0.87885]
76/100: train_loss=[0.30890554 0.3781263 ], train_acc=[0.8988  0.87635]
78/100: train_loss=[0.30326188 0.38848284], train_acc=[0.8979 0.875 ]
80/100: train_loss=[0.29059887 0.36682546], train_acc=[0.9027  0.88175]
82/100: train_loss=[0.30296323 0.3836336 ], train_acc=[0.8973  0.87425]
84/100: train_loss=[0.29241505 0.36760765], train_acc=[0.90165 0.87795]
86/100: train_loss=[0.29574415 0.35798314], train_acc=[0.90055 0.88215]
88/100: train_loss=[0.29945195 0.36853394], train_acc=[0.8997  0.87925]
90/100: train_loss=[0.2849717  0.36030108], train_acc=[0.90365 0.88185]
92/100: train_loss=[0.30057913 0.36316434], train_acc=[0.9002  0.87885]
94/100: train_loss=[0.29095706 0.36505762], train_acc=[0.9015 0.8796]
96/100: train_loss=[0.2977056  0.37344944], train_acc=[0.90115 0.87875]
98/100: train_loss=[0.2874978 0.3609239], train_acc=[0.9032  0.87985]
100/100: train_loss=[0.29333976 0.3733128 ], train_acc=[0.90245 0.87745]
**** Time taken for mnist_4 = 2664.8516097068787
**** Time taken for mnist = 13154.95042347908
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.4585776 1.4021511], train_acc=[0.45275 0.483  ]
2/100: train_loss=[1.1675485 1.189023 ], train_acc=[0.5656 0.5745]
4/100: train_loss=[0.97028226 1.0395274 ], train_acc=[0.6338 0.6091]
6/100: train_loss=[0.8955623  0.91270894], train_acc=[0.6629  0.66415]
8/100: train_loss=[0.8511846  0.86141557], train_acc=[0.68465 0.6785 ]
10/100: train_loss=[0.80836976 0.80638725], train_acc=[0.6969 0.7008]
12/100: train_loss=[0.7896721  0.79074246], train_acc=[0.7093 0.7068]
14/100: train_loss=[0.7622606  0.76892227], train_acc=[0.71755 0.7172 ]
16/100: train_loss=[0.7519657  0.76986694], train_acc=[0.72135 0.71   ]
18/100: train_loss=[0.7357373  0.73650455], train_acc=[0.72605 0.7272 ]
20/100: train_loss=[0.71876496 0.7215661 ], train_acc=[0.73385 0.73405]
22/100: train_loss=[0.743382   0.72377235], train_acc=[0.7191 0.7317]
24/100: train_loss=[0.7309286  0.77737397], train_acc=[0.7299 0.7193]
26/100: train_loss=[0.70954794 0.6933914 ], train_acc=[0.74105 0.7461 ]
28/100: train_loss=[0.69680357 0.6831245 ], train_acc=[0.7433 0.749 ]
30/100: train_loss=[0.7010368 0.679116 ], train_acc=[0.7414  0.74995]
32/100: train_loss=[0.68327814 0.6794631 ], train_acc=[0.74915 0.75165]
34/100: train_loss=[0.6834061  0.69684875], train_acc=[0.744  0.7379]
36/100: train_loss=[0.67747027 0.65383613], train_acc=[0.7499  0.76035]
38/100: train_loss=[0.6765296  0.65733105], train_acc=[0.7521  0.76105]
40/100: train_loss=[0.6551244  0.68015593], train_acc=[0.7598 0.7468]
42/100: train_loss=[0.6611156 0.6589623], train_acc=[0.7553 0.7543]
44/100: train_loss=[0.65268755 0.6467042 ], train_acc=[0.7618  0.76545]
46/100: train_loss=[0.6560099 0.6491666], train_acc=[0.75795 0.7599 ]
48/100: train_loss=[0.64336205 0.62592953], train_acc=[0.7656  0.77065]
50/100: train_loss=[0.62676847 0.6508639 ], train_acc=[0.76905 0.75825]
52/100: train_loss=[0.62303686 0.621744  ], train_acc=[0.7739 0.7718]
54/100: train_loss=[0.64365304 0.6342829 ], train_acc=[0.7626  0.76855]
56/100: train_loss=[0.6165522 0.620116 ], train_acc=[0.772   0.77265]
58/100: train_loss=[0.62549186 0.6226077 ], train_acc=[0.7672 0.7723]
60/100: train_loss=[0.61365086 0.6192873 ], train_acc=[0.773   0.77715]
62/100: train_loss=[0.62543094 0.6131455 ], train_acc=[0.76745 0.7713 ]
64/100: train_loss=[0.6120803  0.60589844], train_acc=[0.7721 0.7777]
66/100: train_loss=[0.6069675 0.6083612], train_acc=[0.77505 0.77385]
68/100: train_loss=[0.61055195 0.60526   ], train_acc=[0.7763 0.7839]
70/100: train_loss=[0.6049565 0.6037882], train_acc=[0.7781  0.77805]
72/100: train_loss=[0.6092229 0.6149998], train_acc=[0.77545 0.76675]
74/100: train_loss=[0.5952755  0.60323924], train_acc=[0.7821 0.7812]
76/100: train_loss=[0.5940903 0.6029165], train_acc=[0.7834  0.77755]
78/100: train_loss=[0.59943235 0.5897218 ], train_acc=[0.7773  0.78335]
80/100: train_loss=[0.58173037 0.6043632 ], train_acc=[0.78505 0.77715]
82/100: train_loss=[0.59445983 0.6016075 ], train_acc=[0.77985 0.78105]
84/100: train_loss=[0.60256904 0.58658695], train_acc=[0.7786 0.7841]
86/100: train_loss=[0.5831325 0.5977213], train_acc=[0.7852 0.7825]
88/100: train_loss=[0.5954053  0.58523095], train_acc=[0.77925 0.7828 ]
90/100: train_loss=[0.57861626 0.5811409 ], train_acc=[0.78545 0.7879 ]
92/100: train_loss=[0.58608645 0.58187604], train_acc=[0.78375 0.7855 ]
94/100: train_loss=[0.57557374 0.57740176], train_acc=[0.7866 0.7904]
96/100: train_loss=[0.5779991  0.57906973], train_acc=[0.78745 0.7891 ]
98/100: train_loss=[0.5822164  0.58667564], train_acc=[0.7863  0.78515]
100/100: train_loss=[0.5737469  0.57860684], train_acc=[0.78805 0.78855]
**** Time taken for fashion_0 = 2705.950398683548
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.2946625 1.4297293], train_acc=[0.53   0.4813]
2/100: train_loss=[1.0669521 1.1358461], train_acc=[0.6069  0.58255]
4/100: train_loss=[0.94095886 0.9437749 ], train_acc=[0.65105 0.6617 ]
6/100: train_loss=[0.8401489  0.88031954], train_acc=[0.69365 0.67675]
8/100: train_loss=[0.78036207 0.81560653], train_acc=[0.7111  0.69915]
10/100: train_loss=[0.75040185 0.79107684], train_acc=[0.7195 0.7104]
12/100: train_loss=[0.73547995 0.76379377], train_acc=[0.72715 0.71505]
14/100: train_loss=[0.71900034 0.7450032 ], train_acc=[0.7325 0.7249]
16/100: train_loss=[0.7151592 0.7249191], train_acc=[0.73285 0.73235]
18/100: train_loss=[0.6922073  0.71169406], train_acc=[0.74445 0.7401 ]
20/100: train_loss=[0.67865115 0.6982569 ], train_acc=[0.75105 0.74205]
22/100: train_loss=[0.67102337 0.7001763 ], train_acc=[0.75175 0.74155]
24/100: train_loss=[0.6702432 0.7059256], train_acc=[0.75245 0.73645]
26/100: train_loss=[0.6588474  0.67901224], train_acc=[0.75805 0.7479 ]
28/100: train_loss=[0.64556015 0.66634136], train_acc=[0.7605  0.75145]
30/100: train_loss=[0.6383181  0.66495305], train_acc=[0.76535 0.75825]
32/100: train_loss=[0.64043754 0.66940355], train_acc=[0.7647 0.7546]
34/100: train_loss=[0.6257884 0.6554754], train_acc=[0.76915 0.758  ]
36/100: train_loss=[0.63982487 0.66409576], train_acc=[0.7632  0.75355]
38/100: train_loss=[0.61322516 0.653817  ], train_acc=[0.7758  0.75745]
40/100: train_loss=[0.6079974  0.64793926], train_acc=[0.77735 0.758  ]
42/100: train_loss=[0.6117716 0.6392257], train_acc=[0.7761 0.7611]
44/100: train_loss=[0.61862814 0.64423543], train_acc=[0.77435 0.7634 ]
46/100: train_loss=[0.6110959 0.6442452], train_acc=[0.77535 0.76605]
48/100: train_loss=[0.60117    0.62643313], train_acc=[0.77965 0.76665]
50/100: train_loss=[0.59821874 0.627539  ], train_acc=[0.78095 0.7717 ]
52/100: train_loss=[0.5886782 0.6175312], train_acc=[0.7836 0.7726]
54/100: train_loss=[0.6031883 0.622326 ], train_acc=[0.77395 0.7735 ]
56/100: train_loss=[0.5853647 0.6208177], train_acc=[0.78565 0.773  ]
58/100: train_loss=[0.5997756 0.6306069], train_acc=[0.77875 0.7681 ]
60/100: train_loss=[0.587064  0.6110889], train_acc=[0.785   0.77775]
62/100: train_loss=[0.5832616 0.6215378], train_acc=[0.7882  0.77525]
64/100: train_loss=[0.5889857  0.62547195], train_acc=[0.78235 0.7705 ]
66/100: train_loss=[0.5944521 0.6246984], train_acc=[0.7847  0.77125]
68/100: train_loss=[0.5861817 0.6119355], train_acc=[0.7849  0.77595]
70/100: train_loss=[0.5678096  0.59917283], train_acc=[0.7919  0.77755]
72/100: train_loss=[0.57051647 0.6068652 ], train_acc=[0.7917  0.77715]
74/100: train_loss=[0.5865105 0.6228345], train_acc=[0.7844 0.7725]
76/100: train_loss=[0.5818202  0.60463125], train_acc=[0.78455 0.77885]
78/100: train_loss=[0.5624496 0.6178451], train_acc=[0.79475 0.77585]
80/100: train_loss=[0.5629227 0.6008123], train_acc=[0.7942  0.78205]
82/100: train_loss=[0.5777195  0.59184843], train_acc=[0.7872  0.78345]
84/100: train_loss=[0.56305015 0.58710426], train_acc=[0.79465 0.78495]
86/100: train_loss=[0.5618263 0.5899599], train_acc=[0.79675 0.78145]
88/100: train_loss=[0.5686859 0.6029065], train_acc=[0.79015 0.77985]
90/100: train_loss=[0.55688393 0.58661836], train_acc=[0.7964  0.78575]
92/100: train_loss=[0.56130046 0.5892506 ], train_acc=[0.79425 0.78525]
94/100: train_loss=[0.56111544 0.5911792 ], train_acc=[0.79495 0.7853 ]
96/100: train_loss=[0.55442035 0.58891594], train_acc=[0.7956 0.7844]
98/100: train_loss=[0.5584093  0.58208114], train_acc=[0.79415 0.7868 ]
100/100: train_loss=[0.55280906 0.58542776], train_acc=[0.79675 0.7869 ]
**** Time taken for fashion_1 = 2704.586405277252
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.2383823 1.356074 ], train_acc=[0.53595 0.5003 ]
2/100: train_loss=[1.0311363 1.1107918], train_acc=[0.61565 0.59085]
4/100: train_loss=[0.86767447 0.9283438 ], train_acc=[0.675  0.6602]
6/100: train_loss=[0.8070765  0.84651244], train_acc=[0.69975 0.68805]
8/100: train_loss=[0.77021176 0.83005583], train_acc=[0.7136  0.68805]
10/100: train_loss=[0.7486659 0.7686071], train_acc=[0.7155  0.71745]
12/100: train_loss=[0.7123632 0.7409169], train_acc=[0.73545 0.7274 ]
14/100: train_loss=[0.6840882 0.726005 ], train_acc=[0.74765 0.7317 ]
16/100: train_loss=[0.679606  0.7072606], train_acc=[0.74945 0.7369 ]
18/100: train_loss=[0.6655662  0.70508826], train_acc=[0.7561  0.73555]
20/100: train_loss=[0.66057557 0.6870634 ], train_acc=[0.75345 0.7439 ]
22/100: train_loss=[0.666495  0.6808057], train_acc=[0.75195 0.74995]
24/100: train_loss=[0.64629585 0.6793706 ], train_acc=[0.76035 0.7485 ]
26/100: train_loss=[0.62586105 0.6567491 ], train_acc=[0.7701 0.7556]
28/100: train_loss=[0.637758  0.6603812], train_acc=[0.7612 0.7521]
30/100: train_loss=[0.61839694 0.64081603], train_acc=[0.7706  0.76355]
32/100: train_loss=[0.6193786 0.638689 ], train_acc=[0.77165 0.76155]
34/100: train_loss=[0.60769826 0.63959116], train_acc=[0.77465 0.7634 ]
36/100: train_loss=[0.6121435 0.6301373], train_acc=[0.77465 0.76115]
38/100: train_loss=[0.62115806 0.63203216], train_acc=[0.77265 0.76185]
40/100: train_loss=[0.6049657  0.61696136], train_acc=[0.77325 0.774  ]
42/100: train_loss=[0.59424406 0.6133184 ], train_acc=[0.7814  0.77225]
44/100: train_loss=[0.5901583 0.6073301], train_acc=[0.78375 0.77625]
46/100: train_loss=[0.5906312 0.605621 ], train_acc=[0.78525 0.77475]
48/100: train_loss=[0.583772  0.6011986], train_acc=[0.7861 0.7759]
50/100: train_loss=[0.5842344 0.6095385], train_acc=[0.7836  0.77315]
52/100: train_loss=[0.5808192 0.5970323], train_acc=[0.78505 0.7788 ]
54/100: train_loss=[0.58543926 0.6130276 ], train_acc=[0.7851 0.7789]
56/100: train_loss=[0.5742404 0.5968256], train_acc=[0.7893 0.785 ]
58/100: train_loss=[0.5810363 0.5914809], train_acc=[0.78455 0.78645]
60/100: train_loss=[0.5679396 0.5920397], train_acc=[0.792  0.7829]
62/100: train_loss=[0.5670647  0.58139336], train_acc=[0.79025 0.78805]
64/100: train_loss=[0.5641118 0.5885128], train_acc=[0.7938  0.78255]
66/100: train_loss=[0.57980967 0.5807711 ], train_acc=[0.7853 0.7878]
68/100: train_loss=[0.5724532 0.5838666], train_acc=[0.78625 0.7889 ]
70/100: train_loss=[0.57257885 0.5947382 ], train_acc=[0.7927 0.7795]
72/100: train_loss=[0.5702166 0.5757194], train_acc=[0.78995 0.79095]
74/100: train_loss=[0.56127566 0.58034056], train_acc=[0.79235 0.7873 ]
76/100: train_loss=[0.58725375 0.57110494], train_acc=[0.7828 0.7909]
78/100: train_loss=[0.5550174 0.5704213], train_acc=[0.79475 0.79185]
80/100: train_loss=[0.56618994 0.5680181 ], train_acc=[0.7918  0.79245]
82/100: train_loss=[0.5559554  0.58075833], train_acc=[0.80015 0.7856 ]
84/100: train_loss=[0.5527995 0.5743216], train_acc=[0.79745 0.7905 ]
86/100: train_loss=[0.5554369  0.56249666], train_acc=[0.7984 0.794 ]
88/100: train_loss=[0.5531341  0.56349176], train_acc=[0.79925 0.79585]
90/100: train_loss=[0.54515994 0.565129  ], train_acc=[0.8016 0.7903]
92/100: train_loss=[0.55857265 0.57180256], train_acc=[0.79555 0.78885]
94/100: train_loss=[0.55277234 0.5632421 ], train_acc=[0.79805 0.79465]
96/100: train_loss=[0.5432835 0.5593244], train_acc=[0.80025 0.7957 ]
98/100: train_loss=[0.55100906 0.57775265], train_acc=[0.80025 0.78515]
100/100: train_loss=[0.540433  0.5537046], train_acc=[0.8019  0.79915]
**** Time taken for fashion_2 = 2678.3014748096466
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.2002476 1.4196392], train_acc=[0.5459 0.4682]
2/100: train_loss=[1.0611308 1.2635808], train_acc=[0.59675 0.54845]
4/100: train_loss=[0.910467  0.9463511], train_acc=[0.6554 0.6512]
6/100: train_loss=[0.87233937 0.9002688 ], train_acc=[0.67485 0.66325]
8/100: train_loss=[0.8086622  0.82872975], train_acc=[0.69915 0.68915]
10/100: train_loss=[0.7721044 0.7711398], train_acc=[0.7147  0.71505]
12/100: train_loss=[0.74965477 0.74719304], train_acc=[0.7199  0.72695]
14/100: train_loss=[0.7326028 0.7393169], train_acc=[0.73135 0.72485]
16/100: train_loss=[0.7404181  0.73933864], train_acc=[0.72355 0.72665]
18/100: train_loss=[0.7330325  0.72252965], train_acc=[0.72935 0.7302 ]
20/100: train_loss=[0.6877744 0.6846049], train_acc=[0.7427 0.7509]
22/100: train_loss=[0.68053025 0.6916591 ], train_acc=[0.74665 0.74445]
24/100: train_loss=[0.6577989 0.6847043], train_acc=[0.75755 0.7495 ]
26/100: train_loss=[0.68964803 0.6715088 ], train_acc=[0.7412 0.7547]
28/100: train_loss=[0.66177005 0.66815484], train_acc=[0.7536 0.7553]
30/100: train_loss=[0.65179753 0.6536776 ], train_acc=[0.758  0.7639]
32/100: train_loss=[0.63018847 0.6550976 ], train_acc=[0.7657 0.7593]
34/100: train_loss=[0.63904256 0.66614854], train_acc=[0.76625 0.7447 ]
36/100: train_loss=[0.623174  0.6404177], train_acc=[0.7704 0.7636]
38/100: train_loss=[0.6319155 0.6370214], train_acc=[0.76475 0.7647 ]
40/100: train_loss=[0.6145205  0.62013745], train_acc=[0.77405 0.7761 ]
42/100: train_loss=[0.6228663 0.6326914], train_acc=[0.76945 0.7677 ]
44/100: train_loss=[0.6039779 0.6497754], train_acc=[0.77785 0.7611 ]
46/100: train_loss=[0.6048732 0.6230546], train_acc=[0.7794 0.7689]
48/100: train_loss=[0.6017457  0.61885226], train_acc=[0.7767  0.77455]
50/100: train_loss=[0.6018976 0.6055562], train_acc=[0.77915 0.77975]
52/100: train_loss=[0.6130972 0.6090706], train_acc=[0.7765  0.77305]
54/100: train_loss=[0.60044193 0.6008613 ], train_acc=[0.7807  0.78385]
56/100: train_loss=[0.58131635 0.59728616], train_acc=[0.78705 0.78495]
58/100: train_loss=[0.5882121  0.60415274], train_acc=[0.78205 0.7824 ]
60/100: train_loss=[0.582525   0.59623843], train_acc=[0.78405 0.7832 ]
62/100: train_loss=[0.58028764 0.58982897], train_acc=[0.7862  0.78175]
64/100: train_loss=[0.59064555 0.58598435], train_acc=[0.784   0.78625]
66/100: train_loss=[0.5735178  0.59345657], train_acc=[0.78995 0.77895]
68/100: train_loss=[0.59327996 0.58862835], train_acc=[0.7804 0.7814]
70/100: train_loss=[0.58757216 0.5830137 ], train_acc=[0.78265 0.78915]
72/100: train_loss=[0.5762983 0.5799505], train_acc=[0.78595 0.7901 ]
74/100: train_loss=[0.5648716 0.5846072], train_acc=[0.79455 0.7847 ]
76/100: train_loss=[0.5715564 0.5895096], train_acc=[0.7925 0.7881]
78/100: train_loss=[0.5906819  0.58059025], train_acc=[0.78315 0.79165]
80/100: train_loss=[0.57029134 0.5737474 ], train_acc=[0.78755 0.79335]
82/100: train_loss=[0.55926687 0.5748925 ], train_acc=[0.794  0.7928]
84/100: train_loss=[0.5543502 0.5892735], train_acc=[0.796   0.78515]
86/100: train_loss=[0.5690536 0.5819943], train_acc=[0.788  0.7866]
88/100: train_loss=[0.565271   0.58026236], train_acc=[0.7943 0.7877]
90/100: train_loss=[0.5880707 0.57388  ], train_acc=[0.78605 0.79015]
92/100: train_loss=[0.564316  0.5670638], train_acc=[0.79465 0.79115]
94/100: train_loss=[0.55262524 0.56960636], train_acc=[0.7977 0.7946]
96/100: train_loss=[0.55828446 0.57071453], train_acc=[0.7971 0.7902]
98/100: train_loss=[0.5597654 0.5601136], train_acc=[0.7955 0.7955]
100/100: train_loss=[0.5690775 0.5777781], train_acc=[0.79415 0.7902 ]
**** Time taken for fashion_3 = 2489.4284393787384
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.1368464 1.2381016], train_acc=[0.5627 0.5456]
2/100: train_loss=[0.95247394 1.0264915 ], train_acc=[0.6455 0.6314]
4/100: train_loss=[0.89354753 0.87839365], train_acc=[0.6705  0.67645]
6/100: train_loss=[0.82513255 0.87144464], train_acc=[0.68995 0.6868 ]
8/100: train_loss=[0.75200945 0.7871913 ], train_acc=[0.71755 0.7048 ]
10/100: train_loss=[0.7444638  0.74356395], train_acc=[0.71995 0.72695]
12/100: train_loss=[0.7102044  0.72821677], train_acc=[0.73685 0.7252 ]
14/100: train_loss=[0.69879025 0.701373  ], train_acc=[0.73475 0.7421 ]
16/100: train_loss=[0.6910965 0.6949512], train_acc=[0.7409 0.7381]
18/100: train_loss=[0.680071   0.67783624], train_acc=[0.74495 0.74795]
20/100: train_loss=[0.6573781  0.66074663], train_acc=[0.7525 0.753 ]
22/100: train_loss=[0.6446098 0.6513728], train_acc=[0.7574  0.75575]
24/100: train_loss=[0.63189965 0.6481468 ], train_acc=[0.76285 0.7584 ]
26/100: train_loss=[0.6270893 0.627794 ], train_acc=[0.76295 0.76925]
28/100: train_loss=[0.6388753 0.6690355], train_acc=[0.76095 0.75395]
30/100: train_loss=[0.62208253 0.6273061 ], train_acc=[0.7682 0.7698]
32/100: train_loss=[0.6090784 0.6172722], train_acc=[0.7691  0.77175]
34/100: train_loss=[0.6040954 0.6084774], train_acc=[0.77445 0.7762 ]
36/100: train_loss=[0.60761124 0.6179474 ], train_acc=[0.77065 0.77425]
38/100: train_loss=[0.5885285 0.6023889], train_acc=[0.7798 0.779 ]
40/100: train_loss=[0.58619875 0.6022819 ], train_acc=[0.77835 0.7777 ]
42/100: train_loss=[0.589524  0.5942629], train_acc=[0.77705 0.7827 ]
44/100: train_loss=[0.58968323 0.5918197 ], train_acc=[0.77645 0.7813 ]
46/100: train_loss=[0.5830469 0.5903513], train_acc=[0.78395 0.7832 ]
48/100: train_loss=[0.57027805 0.58956474], train_acc=[0.78645 0.78715]
50/100: train_loss=[0.59000593 0.5878052 ], train_acc=[0.78195 0.77995]
52/100: train_loss=[0.58491266 0.59559804], train_acc=[0.78105 0.78305]
54/100: train_loss=[0.57140625 0.5996419 ], train_acc=[0.787   0.77695]
56/100: train_loss=[0.5814053 0.5696173], train_acc=[0.78335 0.78985]
58/100: train_loss=[0.5635276  0.56739473], train_acc=[0.78775 0.79145]
60/100: train_loss=[0.56254196 0.5667413 ], train_acc=[0.79    0.79105]
62/100: train_loss=[0.568222   0.58053726], train_acc=[0.7868  0.78735]
64/100: train_loss=[0.55781925 0.5663925 ], train_acc=[0.79265 0.7887 ]
66/100: train_loss=[0.5665938 0.5721312], train_acc=[0.789   0.79245]
68/100: train_loss=[0.54911363 0.56641304], train_acc=[0.79815 0.79075]
70/100: train_loss=[0.55738324 0.5560202 ], train_acc=[0.7928 0.7955]
72/100: train_loss=[0.55595696 0.5505624 ], train_acc=[0.7957 0.7985]
74/100: train_loss=[0.5526967 0.5668632], train_acc=[0.79455 0.7895 ]
76/100: train_loss=[0.5563318 0.5628727], train_acc=[0.7936  0.79235]
78/100: train_loss=[0.5604555 0.5484799], train_acc=[0.79335 0.79895]
80/100: train_loss=[0.5510403 0.5501763], train_acc=[0.7985  0.79655]
82/100: train_loss=[0.55775744 0.5900576 ], train_acc=[0.796   0.78455]
84/100: train_loss=[0.55132055 0.54720736], train_acc=[0.7959  0.79935]
86/100: train_loss=[0.54606795 0.56564325], train_acc=[0.7987  0.79405]
88/100: train_loss=[0.55375993 0.5426072 ], train_acc=[0.7957  0.80075]
90/100: train_loss=[0.54557794 0.54897183], train_acc=[0.7985 0.7985]
92/100: train_loss=[0.5712542 0.5517815], train_acc=[0.78635 0.79755]
94/100: train_loss=[0.5389049 0.5376815], train_acc=[0.8016  0.80325]
96/100: train_loss=[0.5439604 0.5455789], train_acc=[0.80085 0.80205]
98/100: train_loss=[0.54252005 0.54392195], train_acc=[0.80205 0.8004 ]
100/100: train_loss=[0.5449937 0.5405072], train_acc=[0.79765 0.80215]
**** Time taken for fashion_4 = 2397.6990303993225
**** Time taken for fashion = 12976.021229982376
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[2.2118797 1.801474 ], train_acc=[0.20485 0.34935]
2/100: train_loss=[1.4573352 1.1680843], train_acc=[0.50235 0.5734 ]
4/100: train_loss=[0.9201931 0.8935824], train_acc=[0.68835 0.6824 ]
6/100: train_loss=[0.7302803 0.7637002], train_acc=[0.7552 0.7198]
8/100: train_loss=[0.6474372  0.72545075], train_acc=[0.78375 0.7324 ]
10/100: train_loss=[0.5898653 0.6848891], train_acc=[0.8041  0.75065]
12/100: train_loss=[0.5391789 0.6533979], train_acc=[0.82065 0.76075]
14/100: train_loss=[0.5113325  0.63766605], train_acc=[0.82965 0.7661 ]
16/100: train_loss=[0.50700045 0.64170325], train_acc=[0.8339 0.7653]
18/100: train_loss=[0.47924796 0.63755745], train_acc=[0.8452 0.7617]
20/100: train_loss=[0.46312907 0.6075218 ], train_acc=[0.8494  0.77525]
22/100: train_loss=[0.43839565 0.6005485 ], train_acc=[0.85305 0.781  ]
24/100: train_loss=[0.42459035 0.5868655 ], train_acc=[0.8611  0.78325]
26/100: train_loss=[0.4101068  0.57816935], train_acc=[0.86635 0.7899 ]
28/100: train_loss=[0.39049986 0.57340777], train_acc=[0.87545 0.78865]
30/100: train_loss=[0.37614766 0.5608228 ], train_acc=[0.87795 0.79555]
32/100: train_loss=[0.37241226 0.55785745], train_acc=[0.88125 0.79635]
34/100: train_loss=[0.3745491 0.5592355], train_acc=[0.87845 0.7941 ]
36/100: train_loss=[0.3800037  0.55375487], train_acc=[0.87685 0.79715]
38/100: train_loss=[0.3659454 0.5692909], train_acc=[0.88305 0.7938 ]
40/100: train_loss=[0.36750165 0.543775  ], train_acc=[0.88    0.79975]
42/100: train_loss=[0.34119672 0.5475882 ], train_acc=[0.8909  0.79635]
44/100: train_loss=[0.33257818 0.55218756], train_acc=[0.8931 0.7982]
46/100: train_loss=[0.34983706 0.5441855 ], train_acc=[0.88815 0.79535]
48/100: train_loss=[0.32131365 0.5757645 ], train_acc=[0.89715 0.77865]
50/100: train_loss=[0.32980865 0.53829056], train_acc=[0.8923 0.8013]
52/100: train_loss=[0.3180615  0.52404904], train_acc=[0.8989  0.80935]
54/100: train_loss=[0.33584455 0.5213986 ], train_acc=[0.89235 0.8108 ]
56/100: train_loss=[0.30341357 0.5211264 ], train_acc=[0.9037  0.80795]
58/100: train_loss=[0.30941108 0.52466094], train_acc=[0.9018  0.80775]
60/100: train_loss=[0.30681863 0.5301232 ], train_acc=[0.9037 0.802 ]
62/100: train_loss=[0.29532036 0.51280534], train_acc=[0.9055  0.81155]
64/100: train_loss=[0.30073327 0.5311883 ], train_acc=[0.90485 0.80405]
66/100: train_loss=[0.29600662 0.5356995 ], train_acc=[0.9052  0.80085]
68/100: train_loss=[0.2992155 0.5457771], train_acc=[0.90445 0.799  ]
70/100: train_loss=[0.31511837 0.5018585 ], train_acc=[0.8998  0.81765]
72/100: train_loss=[0.2862915 0.5024034], train_acc=[0.90735 0.8159 ]
74/100: train_loss=[0.29379177 0.5039559 ], train_acc=[0.90495 0.81665]
76/100: train_loss=[0.28683585 0.50932765], train_acc=[0.90625 0.8142 ]
78/100: train_loss=[0.28780863 0.5172846 ], train_acc=[0.90635 0.8075 ]
80/100: train_loss=[0.2740996  0.51707536], train_acc=[0.91025 0.8135 ]
82/100: train_loss=[0.28257507 0.49946165], train_acc=[0.9089  0.81915]
84/100: train_loss=[0.2720648  0.51123834], train_acc=[0.9118 0.8104]
86/100: train_loss=[0.28028527 0.51458275], train_acc=[0.90895 0.81095]
88/100: train_loss=[0.28037992 0.48918393], train_acc=[0.9093  0.82225]
90/100: train_loss=[0.27481684 0.49712613], train_acc=[0.91045 0.8206 ]
92/100: train_loss=[0.27073768 0.4910021 ], train_acc=[0.91185 0.81935]
94/100: train_loss=[0.26461992 0.4873203 ], train_acc=[0.9152 0.8211]
96/100: train_loss=[0.27274427 0.48874202], train_acc=[0.91225 0.82   ]
98/100: train_loss=[0.27109712 0.50243187], train_acc=[0.91275 0.815  ]
100/100: train_loss=[0.27159798 0.48788738], train_acc=[0.91305 0.8227 ]
**** Time taken for fashion_and_mnist_0 = 2325.6647872924805
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.788931  1.2148091], train_acc=[0.37665 0.53875]
2/100: train_loss=[1.3009953 1.0132257], train_acc=[0.5585  0.63115]
4/100: train_loss=[0.9542991 0.883326 ], train_acc=[0.67795 0.6713 ]
6/100: train_loss=[0.81540555 0.80793196], train_acc=[0.72335 0.7067 ]
8/100: train_loss=[0.6984627 0.760593 ], train_acc=[0.7673 0.7166]
10/100: train_loss=[0.61483556 0.7268591 ], train_acc=[0.798  0.7301]
12/100: train_loss=[0.57577413 0.7186832 ], train_acc=[0.8126  0.73105]
14/100: train_loss=[0.5537773 0.6874141], train_acc=[0.82    0.74735]
16/100: train_loss=[0.525507  0.6907701], train_acc=[0.83055 0.7484 ]
18/100: train_loss=[0.5281211  0.66982234], train_acc=[0.8259 0.7563]
20/100: train_loss=[0.48031902 0.6824189 ], train_acc=[0.84505 0.7419 ]
22/100: train_loss=[0.4668178 0.654615 ], train_acc=[0.8461  0.75815]
24/100: train_loss=[0.47086036 0.6260588 ], train_acc=[0.84715 0.7707 ]
26/100: train_loss=[0.4331499 0.6264267], train_acc=[0.85865 0.76935]
28/100: train_loss=[0.45492092 0.6176369 ], train_acc=[0.85225 0.77355]
30/100: train_loss=[0.4695524 0.6032387], train_acc=[0.84315 0.78385]
32/100: train_loss=[0.40140805 0.6049758 ], train_acc=[0.8709 0.7804]
34/100: train_loss=[0.40061352 0.6066995 ], train_acc=[0.86875 0.77905]
36/100: train_loss=[0.3754762 0.604585 ], train_acc=[0.87735 0.77525]
38/100: train_loss=[0.37876472 0.5952305 ], train_acc=[0.87595 0.78225]
40/100: train_loss=[0.36933774 0.58526605], train_acc=[0.8817  0.78715]
42/100: train_loss=[0.3659769 0.5704919], train_acc=[0.88105 0.79175]
44/100: train_loss=[0.35232684 0.57769513], train_acc=[0.8857  0.79385]
46/100: train_loss=[0.34331086 0.5705633 ], train_acc=[0.8897 0.7897]
48/100: train_loss=[0.3835422 0.5644669], train_acc=[0.87465 0.7925 ]
50/100: train_loss=[0.32947335 0.6083296 ], train_acc=[0.89425 0.76415]
52/100: train_loss=[0.40260538 0.5953868 ], train_acc=[0.8731  0.78205]
54/100: train_loss=[0.33733714 0.5590211 ], train_acc=[0.89115 0.7958 ]
56/100: train_loss=[0.31303126 0.5610478 ], train_acc=[0.8989 0.7959]
58/100: train_loss=[0.31459993 0.55750674], train_acc=[0.8988 0.7943]
60/100: train_loss=[0.335955  0.5743764], train_acc=[0.8917 0.7938]
62/100: train_loss=[0.313835   0.55128497], train_acc=[0.89805 0.7972 ]
64/100: train_loss=[0.3059261  0.54595494], train_acc=[0.9012 0.8022]
66/100: train_loss=[0.30097437 0.5393605 ], train_acc=[0.9028  0.80305]
68/100: train_loss=[0.29892755 0.53107816], train_acc=[0.90265 0.80665]
70/100: train_loss=[0.31349844 0.53182596], train_acc=[0.8965  0.80555]
72/100: train_loss=[0.30106676 0.5276167 ], train_acc=[0.9039  0.80685]
74/100: train_loss=[0.30934393 0.55266863], train_acc=[0.9012 0.798 ]
76/100: train_loss=[0.29470834 0.5375129 ], train_acc=[0.90615 0.80415]
78/100: train_loss=[0.28498217 0.53378236], train_acc=[0.91015 0.8059 ]
80/100: train_loss=[0.29000688 0.5227908 ], train_acc=[0.90825 0.81265]
82/100: train_loss=[0.2932458  0.52330977], train_acc=[0.90555 0.8076 ]
84/100: train_loss=[0.3117775  0.53394544], train_acc=[0.9001  0.80595]
86/100: train_loss=[0.2936749 0.5176327], train_acc=[0.90705 0.81255]
88/100: train_loss=[0.27688402 0.5195644 ], train_acc=[0.91335 0.81205]
90/100: train_loss=[0.28046376 0.520578  ], train_acc=[0.9105 0.8125]
92/100: train_loss=[0.28450665 0.53010875], train_acc=[0.9087 0.807 ]
94/100: train_loss=[0.2771696 0.5145609], train_acc=[0.9122  0.81125]
96/100: train_loss=[0.28581595 0.5275062 ], train_acc=[0.90905 0.80915]
98/100: train_loss=[0.27361974 0.52388406], train_acc=[0.91255 0.8081 ]
100/100: train_loss=[0.27321318 0.5094261 ], train_acc=[0.9139 0.8141]
**** Time taken for fashion_and_mnist_1 = 2319.732335805893
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.4406539 1.1533933], train_acc=[0.49915 0.57585]
2/100: train_loss=[1.0446924  0.95311916], train_acc=[0.64855 0.64965]
4/100: train_loss=[0.75093025 0.82181454], train_acc=[0.7511  0.69705]
6/100: train_loss=[0.65586245 0.762569  ], train_acc=[0.7819 0.7215]
8/100: train_loss=[0.57528377 0.7374053 ], train_acc=[0.80755 0.72485]
10/100: train_loss=[0.5321636  0.73108137], train_acc=[0.82505 0.73325]
12/100: train_loss=[0.4881054 0.6756448], train_acc=[0.8376 0.7493]
14/100: train_loss=[0.47987887 0.6538424 ], train_acc=[0.8396 0.7586]
16/100: train_loss=[0.44029224 0.63728696], train_acc=[0.8559  0.76575]
18/100: train_loss=[0.42720872 0.6305905 ], train_acc=[0.85955 0.769  ]
20/100: train_loss=[0.44347987 0.6097532 ], train_acc=[0.8532  0.77195]
22/100: train_loss=[0.39258307 0.5953918 ], train_acc=[0.87275 0.7803 ]
24/100: train_loss=[0.38263714 0.59306616], train_acc=[0.8751  0.78295]
26/100: train_loss=[0.37093544 0.5965315 ], train_acc=[0.8785  0.77725]
28/100: train_loss=[0.3704202 0.5780106], train_acc=[0.8797 0.7857]
30/100: train_loss=[0.37498605 0.5719281 ], train_acc=[0.87765 0.79065]
32/100: train_loss=[0.3410017  0.56086534], train_acc=[0.8904  0.79085]
34/100: train_loss=[0.33391523 0.56084764], train_acc=[0.89325 0.7943 ]
36/100: train_loss=[0.33539513 0.55293065], train_acc=[0.8923 0.7977]
38/100: train_loss=[0.32328323 0.5491507 ], train_acc=[0.8968 0.7983]
40/100: train_loss=[0.3335222 0.5510266], train_acc=[0.8912 0.795 ]
42/100: train_loss=[0.3646728 0.5455749], train_acc=[0.881   0.79985]
44/100: train_loss=[0.32875007 0.5464403 ], train_acc=[0.8915 0.8004]
46/100: train_loss=[0.34131396 0.542243  ], train_acc=[0.8896 0.8019]
48/100: train_loss=[0.31684268 0.53155416], train_acc=[0.89815 0.80395]
50/100: train_loss=[0.3224313 0.5292238], train_acc=[0.8958 0.8063]
52/100: train_loss=[0.3160497  0.52565795], train_acc=[0.8964  0.80585]
54/100: train_loss=[0.2955668 0.5204231], train_acc=[0.90485 0.80945]
56/100: train_loss=[0.30842856 0.52428716], train_acc=[0.8999  0.80745]
58/100: train_loss=[0.2995837  0.51867646], train_acc=[0.9029  0.81035]
60/100: train_loss=[0.31469256 0.5285875 ], train_acc=[0.89785 0.80795]
62/100: train_loss=[0.2896205 0.514712 ], train_acc=[0.90725 0.8103 ]
64/100: train_loss=[0.27951285 0.50811434], train_acc=[0.9107 0.8145]
66/100: train_loss=[0.27692986 0.5161018 ], train_acc=[0.9111  0.81125]
68/100: train_loss=[0.290727  0.5105457], train_acc=[0.9049 0.8147]
70/100: train_loss=[0.27437282 0.5189717 ], train_acc=[0.91355 0.8085 ]
72/100: train_loss=[0.28212997 0.5038833 ], train_acc=[0.9098 0.8137]
74/100: train_loss=[0.2774299 0.514041 ], train_acc=[0.9112  0.80925]
76/100: train_loss=[0.28561842 0.5082763 ], train_acc=[0.9082 0.8166]
78/100: train_loss=[0.2718569 0.5110018], train_acc=[0.9127 0.8126]
80/100: train_loss=[0.27916834 0.49890167], train_acc=[0.91065 0.81895]
82/100: train_loss=[0.2688981 0.5050051], train_acc=[0.91265 0.8151 ]
84/100: train_loss=[0.2894642  0.49696013], train_acc=[0.90855 0.8185 ]
86/100: train_loss=[0.2724341  0.49867323], train_acc=[0.9121  0.81795]
88/100: train_loss=[0.2635216  0.50448245], train_acc=[0.91515 0.81505]
90/100: train_loss=[0.2709018  0.50097674], train_acc=[0.9138 0.8191]
92/100: train_loss=[0.2783236 0.5009413], train_acc=[0.91065 0.8181 ]
94/100: train_loss=[0.26340252 0.49863115], train_acc=[0.9157  0.82095]
96/100: train_loss=[0.25792086 0.49617845], train_acc=[0.9178  0.82005]
98/100: train_loss=[0.28205034 0.506273  ], train_acc=[0.9089 0.8161]
100/100: train_loss=[0.25296447 0.5008504 ], train_acc=[0.9195 0.8195]
**** Time taken for fashion_and_mnist_2 = 2323.8807718753815
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.6506978 1.2169636], train_acc=[0.4359 0.5498]
2/100: train_loss=[1.2235467 1.0661443], train_acc=[0.59515 0.60105]
4/100: train_loss=[0.8930218 0.8660848], train_acc=[0.70895 0.67955]
6/100: train_loss=[0.7880032  0.77628917], train_acc=[0.73695 0.7075 ]
8/100: train_loss=[0.6992938 0.7269841], train_acc=[0.76855 0.7355 ]
10/100: train_loss=[0.63055485 0.70513123], train_acc=[0.79045 0.73675]
12/100: train_loss=[0.5595457  0.66447383], train_acc=[0.8175  0.75585]
14/100: train_loss=[0.53063685 0.66781783], train_acc=[0.82785 0.753  ]
16/100: train_loss=[0.59790176 0.65597427], train_acc=[0.80175 0.7541 ]
18/100: train_loss=[0.51974905 0.66177446], train_acc=[0.82895 0.75455]
20/100: train_loss=[0.46698964 0.61563545], train_acc=[0.8467 0.7729]
22/100: train_loss=[0.45841256 0.6259497 ], train_acc=[0.84875 0.7673 ]
24/100: train_loss=[0.43720958 0.6088526 ], train_acc=[0.8568 0.7742]
26/100: train_loss=[0.40988252 0.5933044 ], train_acc=[0.8662 0.7794]
28/100: train_loss=[0.46820766 0.5940883 ], train_acc=[0.8458 0.7804]
30/100: train_loss=[0.38859463 0.590255  ], train_acc=[0.87375 0.78185]
32/100: train_loss=[0.43279296 0.58068144], train_acc=[0.85395 0.78385]
34/100: train_loss=[0.373914  0.5719313], train_acc=[0.87855 0.7873 ]
36/100: train_loss=[0.35432285 0.5687804 ], train_acc=[0.8869 0.7894]
38/100: train_loss=[0.365235   0.58053404], train_acc=[0.88185 0.7849 ]
40/100: train_loss=[0.37045127 0.5848412 ], train_acc=[0.8817 0.7805]
42/100: train_loss=[0.35192066 0.5820986 ], train_acc=[0.88785 0.7851 ]
44/100: train_loss=[0.33069226 0.5591194 ], train_acc=[0.8931  0.79375]
46/100: train_loss=[0.33708814 0.55813676], train_acc=[0.89255 0.79375]
48/100: train_loss=[0.34390512 0.5703138 ], train_acc=[0.88905 0.7874 ]
50/100: train_loss=[0.36792263 0.55321455], train_acc=[0.8789 0.7947]
52/100: train_loss=[0.3744208  0.55653936], train_acc=[0.87875 0.79565]
54/100: train_loss=[0.32199797 0.55564696], train_acc=[0.89745 0.79575]
56/100: train_loss=[0.3597412 0.5469586], train_acc=[0.87905 0.79845]
58/100: train_loss=[0.30748054 0.5362154 ], train_acc=[0.9023  0.80375]
60/100: train_loss=[0.32566118 0.53743726], train_acc=[0.89425 0.8032 ]
62/100: train_loss=[0.30598873 0.540967  ], train_acc=[0.90205 0.8001 ]
64/100: train_loss=[0.32801247 0.5398981 ], train_acc=[0.89345 0.8016 ]
66/100: train_loss=[0.31324604 0.53385806], train_acc=[0.8998 0.8048]
68/100: train_loss=[0.32825372 0.5322375 ], train_acc=[0.89375 0.8047 ]
70/100: train_loss=[0.2970323  0.53397894], train_acc=[0.90635 0.8027 ]
72/100: train_loss=[0.3207836 0.5396051], train_acc=[0.89705 0.8047 ]
74/100: train_loss=[0.2931277 0.5148275], train_acc=[0.90615 0.8123 ]
76/100: train_loss=[0.2989489 0.5341644], train_acc=[0.90425 0.8033 ]
78/100: train_loss=[0.29375902 0.5254633 ], train_acc=[0.9065 0.8048]
80/100: train_loss=[0.2907644  0.53356045], train_acc=[0.90645 0.80585]
82/100: train_loss=[0.32212424 0.51486605], train_acc=[0.89595 0.8113 ]
84/100: train_loss=[0.30575973 0.5137351 ], train_acc=[0.9025  0.81475]
86/100: train_loss=[0.28556168 0.51507723], train_acc=[0.9092  0.81095]
88/100: train_loss=[0.28495786 0.51765406], train_acc=[0.9084  0.81075]
90/100: train_loss=[0.30057362 0.5286977 ], train_acc=[0.90395 0.80485]
92/100: train_loss=[0.30360973 0.50782496], train_acc=[0.90295 0.8147 ]
94/100: train_loss=[0.30308452 0.5037583 ], train_acc=[0.90155 0.8162 ]
96/100: train_loss=[0.2917709 0.5245561], train_acc=[0.9071  0.80595]
98/100: train_loss=[0.28593817 0.51232886], train_acc=[0.9092 0.811 ]
100/100: train_loss=[0.27882522 0.5023292 ], train_acc=[0.911   0.81525]
**** Time taken for fashion_and_mnist_3 = 2327.1492669582367
==>>> total trainning batch number: 469
==>>> total testing batch number: 79
1/100: train_loss=[1.1051562 0.9662333], train_acc=[0.6305 0.6521]
2/100: train_loss=[0.83324414 0.83509463], train_acc=[0.72395 0.6883 ]
4/100: train_loss=[0.61996496 0.73084027], train_acc=[0.7992  0.72785]
6/100: train_loss=[0.58171403 0.71506953], train_acc=[0.8068  0.73775]
8/100: train_loss=[0.48317024 0.684307  ], train_acc=[0.8454  0.74265]
10/100: train_loss=[0.46256542 0.64111775], train_acc=[0.85065 0.76285]
12/100: train_loss=[0.43372202 0.6284638 ], train_acc=[0.8602  0.77235]
14/100: train_loss=[0.40991467 0.6080103 ], train_acc=[0.86925 0.77385]
16/100: train_loss=[0.40086904 0.5907734 ], train_acc=[0.8693 0.7815]
18/100: train_loss=[0.38203353 0.58742493], train_acc=[0.87765 0.78235]
20/100: train_loss=[0.385256  0.5811644], train_acc=[0.8748 0.7799]
22/100: train_loss=[0.3717039 0.581384 ], train_acc=[0.87985 0.78645]
24/100: train_loss=[0.36703986 0.56402606], train_acc=[0.88225 0.78985]
26/100: train_loss=[0.3450272 0.5552518], train_acc=[0.89   0.7929]
28/100: train_loss=[0.3397725 0.5511212], train_acc=[0.89435 0.7931 ]
30/100: train_loss=[0.33418864 0.5454368 ], train_acc=[0.89525 0.79805]
32/100: train_loss=[0.34211284 0.53578746], train_acc=[0.8907  0.80355]
34/100: train_loss=[0.33045706 0.5282994 ], train_acc=[0.89535 0.8044 ]
36/100: train_loss=[0.33331883 0.52713656], train_acc=[0.89475 0.8061 ]
38/100: train_loss=[0.319739   0.52112204], train_acc=[0.89945 0.8076 ]
40/100: train_loss=[0.30697858 0.51449597], train_acc=[0.90365 0.80955]
42/100: train_loss=[0.31602854 0.552235  ], train_acc=[0.9009 0.7949]
44/100: train_loss=[0.31608066 0.5074958 ], train_acc=[0.8981  0.81125]
46/100: train_loss=[0.29173818 0.5068569 ], train_acc=[0.91015 0.81375]
48/100: train_loss=[0.30009544 0.5028684 ], train_acc=[0.90555 0.8133 ]
50/100: train_loss=[0.29415524 0.49979952], train_acc=[0.9076  0.81585]
52/100: train_loss=[0.2832634 0.5251638], train_acc=[0.9115  0.79825]
54/100: train_loss=[0.28867263 0.4938373 ], train_acc=[0.9097  0.81785]
56/100: train_loss=[0.3017263  0.50422364], train_acc=[0.904  0.8127]
58/100: train_loss=[0.28703362 0.50133026], train_acc=[0.9098  0.81665]
60/100: train_loss=[0.33108416 0.49819335], train_acc=[0.8944 0.8153]
62/100: train_loss=[0.2780706 0.5009677], train_acc=[0.9138  0.81365]
64/100: train_loss=[0.28075767 0.48828495], train_acc=[0.9117  0.81945]
66/100: train_loss=[0.28440726 0.48566854], train_acc=[0.91255 0.8234 ]
68/100: train_loss=[0.2687056 0.4870451], train_acc=[0.91655 0.81995]
70/100: train_loss=[0.2714571  0.50046426], train_acc=[0.9129  0.81405]
72/100: train_loss=[0.26959288 0.47877958], train_acc=[0.9155 0.8236]
74/100: train_loss=[0.27812663 0.48707733], train_acc=[0.9129 0.8231]
76/100: train_loss=[0.2642594 0.4745034], train_acc=[0.9185 0.8262]
78/100: train_loss=[0.27019522 0.4803519 ], train_acc=[0.91685 0.8234 ]
80/100: train_loss=[0.26464522 0.48493564], train_acc=[0.919  0.8233]
82/100: train_loss=[0.27045012 0.5057511 ], train_acc=[0.91435 0.81635]
84/100: train_loss=[0.26138815 0.47325957], train_acc=[0.9205  0.82505]
86/100: train_loss=[0.25889227 0.4861797 ], train_acc=[0.91935 0.82035]
88/100: train_loss=[0.28065503 0.4916818 ], train_acc=[0.91245 0.8211 ]
90/100: train_loss=[0.2817488  0.46855977], train_acc=[0.9102 0.8284]
92/100: train_loss=[0.267304   0.47021872], train_acc=[0.91615 0.8256 ]
94/100: train_loss=[0.26367572 0.4678367 ], train_acc=[0.91795 0.82785]
96/100: train_loss=[0.27574334 0.47753617], train_acc=[0.9145 0.8244]
98/100: train_loss=[0.25689274 0.4725147 ], train_acc=[0.9203  0.82665]
100/100: train_loss=[0.26318917 0.4738539 ], train_acc=[0.9182 0.8265]
**** Time taken for fashion_and_mnist_4 = 2273.0699167251587
**** Time taken for fashion_and_mnist = 11569.546944141388
